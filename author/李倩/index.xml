<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>李倩 | 云原生社区（中国）</title>
    <link>https://cloudnative.to/author/%E6%9D%8E%E5%80%A9/</link>
      <atom:link href="https://cloudnative.to/author/%E6%9D%8E%E5%80%A9/index.xml" rel="self" type="application/rss+xml" />
    <description>李倩</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Fri, 21 Apr 2023 12:00:00 +0800</lastBuildDate>
    <image>
      <url>https://cloudnative.to/author/%E6%9D%8E%E5%80%A9/avatar_hue38add62c87b7486d80c9f3fda25dfc1_12220_270x270_fill_q75_lanczos_center.jpg</url>
      <title>李倩</title>
      <link>https://cloudnative.to/author/%E6%9D%8E%E5%80%A9/</link>
    </image>
    
    <item>
      <title>使用全景拓扑持续跟踪云原生应用的压测性能瓶颈</title>
      <link>https://cloudnative.to/blog/service-map-observation-performance-test/</link>
      <pubDate>Fri, 21 Apr 2023 12:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/service-map-observation-performance-test/</guid>
      <description>&lt;p&gt;测试小姐姐正在对云原生的电商应用进行压测，但是如何对压测结果进行持续的观测呢？这一直是比较头痛的事情，本文将介绍如何利用 DeepFlow 的全景拓扑帮助小姐姐快速找到瓶颈点。DeepFlow 全景拓扑无需业务修改代码、配置或者重启服务，利用 BPF/eBPF 技术通过对业务零侵扰的方式构建而来，这是一种很便捷且低成本的方式来观测全链路压测的结果。&lt;/p&gt;
&lt;h2 id=&#34;背景介绍&#34;&gt;背景介绍&lt;/h2&gt;
&lt;p&gt;DeepFlow 在线的 Sandbox 环境中部署了一个云原生的电商应用，此电商应用来源于 &lt;a href=&#34;https://github.com/open-telemetry/opentelemetry-demo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;，此应用覆盖 Go/Java/.NET/PHP/Python 等多种语言，且涵盖 Redis/Kafka/PostgreSQL 等中间件，所有的这些服务都部署在 K8s 环境中。在做全链路压测时，当前通常的方式需要对应用进行代码级别的改造，这对于仅负责测试的小姐姐来说又很难推动，接下来将详细介绍 DeepFlow 的全景拓扑如何轻松解决小姐姐的苦恼。&lt;/p&gt;
&lt;p&gt;以下是电商应用微服务的调用关系图，提供了外网和内网访问的两种方式。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-调用关系&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e3b447b.png&#34; alt=&#34;调用关系&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      调用关系
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;DeepFlow 的 Sandbox 考虑到安全性的问题，仅支持了内网的访问方式，以下是 DeepFlow 的全景拓扑自动绘制的调用关系，接下来的整个过程都将基于此拓扑进行。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-全景拓扑&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/20230420644151ebabb38.jpg&#34; alt=&#34;全景拓扑&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      全景拓扑
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;DeepFlow 的全景拓扑可以与多指标进行结合，当指标量超过阈值时，则将通过标红的形式可视化出来。在开始接下来压测及调优过程之前，需要对本次过程中使用到的指标有一个了解。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;指标&lt;/th&gt;
&lt;th&gt;说明&lt;/th&gt;
&lt;th&gt;观测目标&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;流量速率&lt;/td&gt;
&lt;td&gt;作为主指标，构建全景拓扑&lt;/td&gt;
&lt;td&gt;&amp;ndash;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;应用请求速率&lt;/td&gt;
&lt;td&gt;统计服务的请求速率，主要用于观测压测过程中请求量是否符合压测预期&lt;/td&gt;
&lt;td&gt;符合测试压测的速率&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;应用异常个数&lt;/td&gt;
&lt;td&gt;统计服务的异常个数，主要用于观测压测过程中是否存在服务异常的情况&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;应用响应时延&lt;/td&gt;
&lt;td&gt;统计服务的响应时延，主要用于观测压测过程中响应时延是否超过预期&lt;/td&gt;
&lt;td&gt;1s 以内&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCP 建连时延&lt;/td&gt;
&lt;td&gt;统计 TCP 建连时延，主要用于观测压测过程中网络是否存在波动&lt;/td&gt;
&lt;td&gt;10ms 以内&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;TCP 建连失败&lt;/td&gt;
&lt;td&gt;统计 TCP 建连失败，主要用于观测压测过程中系统性能是否稳定&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;逐个击破性能瓶颈&#34;&gt;逐个击破性能瓶颈&lt;/h2&gt;
&lt;p&gt;在 loadgenerator 所在的 node，通过脚本模拟 1.5k 的并发访问量，观测全景拓扑，一片红（在当前并发量的情况下，观测的指标量都超过阈值了），说明了目前这个系统在当前资源分配情况下，是扛不住 1.5k 的并发访问量的。查看指标量，应用响应时延、异常数和网络建连时延、建连失败的指标量都远超阈值了。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-压测开始&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e7c5190.jpg&#34; alt=&#34;压测开始&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      压测开始
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;找一个指标量（服务响应时延）来层层追踪拓扑图，loadgenerator 访问 frontend 响应时延达到 15s，而 frontend 访问后端服务中，其中访问 productcatalog 及 recommendation 分别都消耗了大概 11s、5s 的时延，其中 productcatalog 没有继续往后的访问了，可继续追踪 recommendation 访问的后端，其中也是访问 productcatalog 消耗了大概 4s 的时延，&lt;strong&gt;到此基本能确定当前应用的性能瓶颈就在 productcatalog 上。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-分析拓扑&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e553e4e.jpg&#34; alt=&#34;分析拓扑&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      分析拓扑
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;接下来先直接对 productcatalog 扩容，增加 pod 数量到之前的 1 倍，然后再观测下全景拓扑，可以看出来拓扑图上的红变少一些了，同时观测下指标量，发现应用的响应时延及异常数都降下来了。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-扩容_01&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e46c14c.jpg&#34; alt=&#34;扩容_01&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      扩容_01
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;沿着前面的思路，依然使用服务响应时延来层层追踪拓扑图，&lt;strong&gt;发现通过扩容 1 倍的 POD 数，虽然缓解了 productcatalog 性能压力，但是还是没彻底解决。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-分析拓扑_01&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e906a89.jpg&#34; alt=&#34;分析拓扑_01&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      分析拓扑_01
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;接下来继续对 productcatalog 扩容，这次扩容到 2 倍，再观测全景拓扑，红色部分更少了，指标量也更接近预期了。不过&lt;strong&gt;这次发现解决了 productcatalog 的性能问题后，cart 的性能问题冒出来了&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-扩容_02&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e64d271.jpg&#34; alt=&#34;扩容_02&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      扩容_02
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;继续对 cart 服务的 POD 的数量扩容 1 倍，观测全景拓扑，发现红色部分都没了。到此基本上可以出压测结果了，&lt;strong&gt;针对当前电商应用在 1.5k 的并发访问量的情况下，productcatalog 需要是比其他服务（除 cart 外） 2 倍的资源分配，cart 需要比其他服务（除 productcatalog 外） 多 1 倍的资源分配才能应对。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-扩容_03&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e6c4bf9.jpg&#34; alt=&#34;扩容_03&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      扩容_03
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;指标量分析&#34;&gt;指标量分析&lt;/h2&gt;
&lt;p&gt;再结合历史曲线图，来详细分析下指标量的变化，让大家能更好的理解为什么应用的性能问题除了带来应用指标量的波动，为什么还会带了网络指标量的变化。明显在 17:48 分后端服务是整个处理不过来的，这时多个指标都能反应此情况。&lt;code&gt;建连失败&lt;/code&gt;很多，在失败的过程中还不停的&lt;code&gt;重传 SYN 报文&lt;/code&gt;，同时建连失败的都是因为&lt;code&gt;服务端直接回 RST &lt;/code&gt;导致，此时仅看这部分指标量已经能清楚是&lt;strong&gt;后端系统对连接处理不过来了&lt;/strong&gt;。再继续结合应用指标量分析，在建连失败多的情况下，&lt;code&gt;请求量&lt;/code&gt;反而下降了，这是因为建连都没成功，根本发不了请求，此时后端的&lt;code&gt;异常数&lt;/code&gt;及&lt;code&gt;响应时延&lt;/code&gt;也都是挺高，也是直接反应了&lt;strong&gt;后端对请求处理不过来了。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-历史曲线&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/pub/pic/202304206440e8e89523c.jpg&#34; alt=&#34;历史曲线&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      历史曲线
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;什么是-deepflow&#34;&gt;什么是 DeepFlow&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepflowio/deepflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow&lt;/a&gt; 是一款开源的高度自动化的可观测性平台，是为云原生应用开发者建设可观测性能力而量身打造的全栈、全链路、高性能数据引擎。DeepFlow 使用 eBPF、WASM、OpenTelemetry 等新技术，创新的实现了 AutoTracing、AutoMetrics、AutoTagging、SmartEncoding 等核心机制，帮助开发者提升埋点插码的自动化水平，降低可观测性平台的运维复杂度。利用 DeepFlow 的可编程能力和开放接口，开发者可以快速将其融入到自己的可观测性技术栈中。&lt;/p&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&#34;https://github.com/deepflowio/deepflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/deepflowio/deepflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;访问 &lt;a href=&#34;https://deepflow.io/docs/zh/install/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow Demo&lt;/a&gt;，体验高度自动化的可观测性新时代。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>应用响应时延背后深藏的网络时延</title>
      <link>https://cloudnative.to/blog/analysis-of-delay-with-deepflow/</link>
      <pubDate>Wed, 08 Mar 2023 12:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/analysis-of-delay-with-deepflow/</guid>
      <description>&lt;p&gt;应用异常时，基本可以分为&lt;strong&gt;服务访问不通&lt;/strong&gt;和&lt;strong&gt;服务响应慢&lt;/strong&gt;两个大类。其中服务响应慢的问题定位非常棘手，很多无头案。应用团队有日志和追踪，对于自认为的不可能不合理的事情都会甩给基础设施团队，又由于基础设施团队现有的监控数据缺乏应用的观测视角，通常成为一切「不是我的问题」超自然现象的终极背锅侠，其中以网络团队尤为严重。&lt;/p&gt;
&lt;h2 id=&#34;响应时延&#34;&gt;响应时延&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;服务为什么响应慢&lt;/strong&gt;？？？首先，我们需要一种方式来度量何为响应慢，参考 Google 在 SRE Handbook 中提到过&lt;code&gt;4 个黄金信号&lt;/code&gt;及 Weave Cloud 提出来的 &lt;code&gt;RED 方法&lt;/code&gt;，都存在度量的指标（Latency/Duration），后文统称为&lt;code&gt;响应时延&lt;/code&gt;。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Latency 表达的是&lt;strong&gt;服务处理某个请求所需要的时间&lt;/strong&gt;，站在的是服务端视角&lt;/li&gt;
&lt;li&gt;Duration 表达的是&lt;strong&gt;每个请求所花费的时间&lt;/strong&gt;，站在的是客户端视角&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结下来，不论站在什么视角，&lt;code&gt;响应时延&lt;/code&gt;表达的都是处理一个请求所花费的时间，可以用来&lt;strong&gt;表征服务响应慢的度量指标&lt;/strong&gt;，但若要定位为什么响应慢还需要进一步剖解响应时延：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;系统时延：系统转发请求/响应的时延消耗&lt;/li&gt;
&lt;li&gt;网络时延：包含查询 DNS 时延及网络处理的时延&lt;/li&gt;
&lt;li&gt;应用时延：从不同视角来看，包含客户端应用处理时延 + 服务端应用处理时延&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-响应时延拆解&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./2.jpg&#34; alt=&#34;响应时延拆解&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      响应时延拆解
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;确定度量指标后，接下就可以分析服务响应慢的原因，此时可以利用&lt;strong&gt;分布式链路追踪&lt;/strong&gt;能力来快速来&lt;strong&gt;定界瓶颈点&lt;/strong&gt;，例如可利用 DeepFlow 的分布式追踪能力来快速定界瓶颈点在应用、系统还是网络。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-分布式链路追踪-火焰图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./1.png&#34; alt=&#34;分布式链路追踪-火焰图&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      分布式链路追踪-火焰图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;完成瓶颈点定界后，则需要去查找根因。对于应用或者系统的问题，可以利用&lt;strong&gt;性能剖析（profile）继续追查根因&lt;/strong&gt;，而对应网络时延的分析，其中 DNS 时延分析是相对简单的，只需要关注请求的响应时延即可，但网络处理时延瓶颈的定位却缺少了分析的工具，接下来将主要聚焦讨论网络传输时延如何分析。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-性能剖析-火焰图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./3.jpg&#34; alt=&#34;性能剖析-火焰图&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      性能剖析-火焰图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;网络时延&#34;&gt;网络时延&lt;/h2&gt;
&lt;p&gt;参考 AWS 中的定义网络时延是指网络通信中的延时，网络时延显示了数据通过网络传输所需的时间。&lt;strong&gt;讨论网络时延如何，也是需要可度量的指标&lt;/strong&gt;，AWS 也指定了使用“首字节时间”和“往返时间”等指标来衡量网络时延，这两个指标是可以适用于所有网络协议的传输时延的度量，但实际应用 80% 都使用的 TCP 协议，对于 TCP 协议是需要更细粒度的度量指标，下文通过图文的形式，详细的介绍目前可用的度量指标及用法。&lt;/p&gt;
&lt;p&gt;TCP 协议是面向连接的传输层通信协议，对其详细的通信过程分析，时延可分为三大类：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建连时产生的时延
&lt;ul&gt;
&lt;li&gt;[1] 完整的&lt;code&gt;建连时延&lt;/code&gt;包含客户端发出 SYN 包到收到服务端回复的 SYN+ACK 包，并再次回复 ACK 包的整个时间。建连时延拆解开又可分为&lt;code&gt;客户端建连时延&lt;/code&gt;与&lt;code&gt;服务端建连时延&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;[2] &lt;code&gt;客户端建连时延&lt;/code&gt;为客户端收到 SYN+ACK 包后，回复 ACK 包的时间&lt;/li&gt;
&lt;li&gt;[3] &lt;code&gt;服务端建连时延&lt;/code&gt;为服务端收到 SYN 包后，回复 SYN+ACK 包的时间&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据通信时产生的时延，可拆解为&lt;code&gt;客户端等待时延&lt;/code&gt;+&lt;code&gt;数据传输时延&lt;/code&gt;
&lt;ul&gt;
&lt;li&gt;[4] &lt;code&gt;客户端等待时延&lt;/code&gt;为建连成功后，客户端首次发送请求的时间；为收到服务端的数据包后，客户端再发起数据包的时间&lt;/li&gt;
&lt;li&gt;[5] &lt;code&gt;数据传输时延&lt;/code&gt;为客户端发送数据包到收到服务端回复数据包的时间&lt;/li&gt;
&lt;li&gt;[6] 在数据传输时延中还会产生系统协议栈的处理时延，称为&lt;code&gt;系统时延&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;断连时产生的时延：因为断连的时延并不影响到应用的响应时延，因此并不会单独统计此部分使用&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-tcp-网络时延解剖&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./4.jpg&#34; alt=&#34;TCP 网络时延解剖&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      TCP 网络时延解剖
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;度量的网络时延的指标已经拆解好了，接下来讨论在哪里采集指标&lt;/strong&gt;，网络的报文将在客户端，各种虚拟和物理网络与服务端之间穿梭，因此可报文穿梭的位置点来采集，后续统称为&lt;code&gt;统计位置&lt;/code&gt;。当然统计位置越多，定位网络的瓶颈路径越快，但是统计位置多则随之带来的计算量也是成倍增加，企业在有成本压力时，建议在重要节点进行采集即可，比如 K8s Pod 虚拟网卡、K8s Node 网卡、云服务器网卡、网关（如 LVS/Nginx 等）网卡、硬件防火墙/负载均衡器前后&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-统计位置&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./5.jpg&#34; alt=&#34;统计位置&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      统计位置
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;分析到这，&lt;strong&gt;基本已经清晰网络时延的详细的度量指标了，回过头结合响应时延再讨论下如何查看网络时延对其的影响&lt;/strong&gt;，基本可以分两种情况讨论&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;应用发起请求为短连接&lt;/strong&gt;：此时分析网络时延需要查看 &lt;strong&gt;DNS 时延 + 建连时延 + 客户端等待时延 + 数据传输时延 + 系统时延&lt;/strong&gt;，则可快速定位时延发生的具体原因了。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DNS 时延高&lt;/strong&gt;，结合统计位置，则可回答是&lt;strong&gt;网络传输时延高&lt;/strong&gt;还是&lt;strong&gt;DNS 服务响应慢&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;建连时延高&lt;/strong&gt;，结合客户端建连时延 + 服务端建连时延 + 统计位置，则可回答是&lt;strong&gt;网络传输时延高&lt;/strong&gt;还是&lt;strong&gt;客户端系统回复慢&lt;/strong&gt;还是&lt;strong&gt;服务端处理建连响应慢&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;客户端等待时延高&lt;/strong&gt;，结合统计位置，则可回答是&lt;strong&gt;网络传输时延高&lt;/strong&gt;还是&lt;strong&gt;客户端请求发送延迟&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据传输时延高&lt;/strong&gt;，结合统计位置，则可回答是&lt;strong&gt;网络传输时延高&lt;/strong&gt;还是&lt;strong&gt;服务端响应慢&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;系统时延高&lt;/strong&gt;，结合统计位置，则可回答&lt;strong&gt;网络传输时延高&lt;/strong&gt;还是&lt;strong&gt;服务端协议栈处理慢&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;应用发起请求为长连接&lt;/strong&gt;：因为长连接是保持长期活动的 HTTP 连接，不需要考虑 DNS 查询与建连的时延消耗，只需要关注&lt;strong&gt;客户端等待时延 + 数据传输时延 + 系统时延&lt;/strong&gt;即可&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;案例分析&#34;&gt;案例分析&lt;/h2&gt;
&lt;p&gt;限于笔者时间限制又想早点将&lt;strong&gt;应用响应时延背后深藏的网络时延&lt;/strong&gt;剖解分享给大家，本文不继续补充实际案例，将在一周后分享在某xx智能终端公司的如何结合 DeepFlow 在&lt;strong&gt;服务响应慢时，网络团队在存在可观测性的时延数据时，如何硬气回怼。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;什么是-deepflow&#34;&gt;什么是 DeepFlow&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepflowys/deepflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow&lt;/a&gt; 是一款开源的高度自动化的可观测性平台，是为云原生应用开发者建设可观测性能力而量身打造的全栈、全链路、高性能数据引擎。DeepFlow 使用 eBPF、WASM、OpenTelemetry 等新技术，创新的实现了 AutoTracing、AutoMetrics、AutoTagging、SmartEncoding 等核心机制，帮助开发者提升埋点插码的自动化水平，降低可观测性平台的运维复杂度。利用 DeepFlow 的可编程能力和开放接口，开发者可以快速将其融入到自己的可观测性技术栈中。&lt;/p&gt;
&lt;p&gt;GitHub 地址：https://github.com/deepflowys/deepflow&lt;/p&gt;
&lt;p&gt;访问 &lt;a href=&#34;https://deepflow.yunshan.net/docs/zh/install/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow Demo&lt;/a&gt;，体验高度自动化的可观测性新时代。&lt;/p&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/cn/what-is/latency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://aws.amazon.com/cn/what-is/latency/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://baike.baidu.com/item/%E7%B3%BB%E7%BB%9F%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4/22026261&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://baike.baidu.com/item/%E7%B3%BB%E7%BB%9F%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4/22026261&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://dev.to/aws/why-are-services-slow-sometimes-mn3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://dev.to/aws/why-are-services-slow-sometimes-mn3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/promql/prometheus-promql-best-praticase&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://yunlzheng.gitbook.io/prometheus-book/parti-prometheus-ji-chu/promql/prometheus-promql-best-praticase&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.weave.works/blog/the-red-method-key-metrics-for-microservices-architecture/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://aws.amazon.com/what-is/latency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://aws.amazon.com/what-is/latency/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>可观测性实战：快速定位 K8s 应用故障</title>
      <link>https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/</link>
      <pubDate>Tue, 14 Feb 2023 12:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/</guid>
      <description>&lt;p&gt;故障发生在2023春节前两天，DeepFlow 团队内部访问工单系统出现问题，影响了所有北京区的同事，这篇文章将详细记录如何利用 DeepFlow 定位到对这次问题根因（网关 MSS 误变更导致报文大于 MTU，大数据报文被丢弃）。&lt;/p&gt;
&lt;h2 id=&#34;背景介绍&#34;&gt;背景介绍&lt;/h2&gt;
&lt;p&gt;工单系统是 DeepFlow 团队自主研发的一个跟踪工单的内部工具，部署在阿里公有云的容器服务（ACK）中，工单系统通过 Ingress 的方式对外提供服务，办公区与阿里云通过 VPN 连接，因此办公区可以直接使用域名访问工单系统。在&lt;a href=&#34;https://deepflow.yunshan.net/blog/020-k8s-service-exception-troubleshooting/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《K8s 服务异常排障过程全解密》&lt;/a&gt;文中对 K8s 访问方式做过总结，工单系统是比较典型的&lt;code&gt;方式三&lt;/code&gt;的访问形式&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-集群外客户端通过-ingress-访问集群内服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;集群外客户端通过 Ingress 访问集群内服务&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/1_hu7a56a8a76a4e59d2ef5e3d5206f4c8f1_169694_df912b6c9554e89f339c6059ffc4eb36.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/1_hu7a56a8a76a4e59d2ef5e3d5206f4c8f1_169694_119908934db590460517b25c00976e48.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/1_hu7a56a8a76a4e59d2ef5e3d5206f4c8f1_169694_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/1_hu7a56a8a76a4e59d2ef5e3d5206f4c8f1_169694_df912b6c9554e89f339c6059ffc4eb36.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      集群外客户端通过 Ingress 访问集群内服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;下图是通过 DeepFlow 自动绘制的访问拓扑图，可以看出北京和广州办公区都是通过 Ingress 的形式来访问工单的入口服务 (ticket_web)。工单系统部署在基础服务的容器集群上，此容器集群所有的 Node 上都已经部署了 deepflow-agent，因此可以自动采集所有 POD 及 Node 的网络/系统/应用相关的数据，其中就包括阿里云 nginx-ingress-controller 服务对应的 POD 以及应用的 POD&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-工单系统访问拓扑&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;工单系统访问拓扑&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/3_hu76c44d080188ab7444b81fc7e0c1d551_69522_5aa5d9c14a22e7e7ed78db50bf0485af.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/3_hu76c44d080188ab7444b81fc7e0c1d551_69522_a6e79f0305752aaa1d6cda57faaf0e3d.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/3_hu76c44d080188ab7444b81fc7e0c1d551_69522_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/3_hu76c44d080188ab7444b81fc7e0c1d551_69522_5aa5d9c14a22e7e7ed78db50bf0485af.webp&#34;
               width=&#34;760&#34;
               height=&#34;358&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      工单系统访问拓扑
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;排障过程&#34;&gt;排障过程&lt;/h2&gt;
&lt;p&gt;下午 3:00 左右，陆续收到同事反馈，工单系统加载不出来，首先和工单系统研发明确，并未做过任何变更&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-故障现场&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;故障现场&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/4_hue501760cd9478bf27a2e4fb66f2e2f59_228755_4bbf74b4054d42e0a2799b66da2dffeb.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/4_hue501760cd9478bf27a2e4fb66f2e2f59_228755_9c4c1b140a4c6f91e14934611086e36f.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/4_hue501760cd9478bf27a2e4fb66f2e2f59_228755_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/4_hue501760cd9478bf27a2e4fb66f2e2f59_228755_4bbf74b4054d42e0a2799b66da2dffeb.webp&#34;
               width=&#34;760&#34;
               height=&#34;254&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      故障现场
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;依据&lt;a href=&#34;https://deepflow.yunshan.net/blog/020-k8s-service-exception-troubleshooting/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《K8s 服务异常排障过程全解密》&lt;/a&gt;总结的思路&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-k8s-服务异常排障思路&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;K8s 服务异常排障思路&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/5_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_41ad4f6b73a394975f6b882dd220b05d.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/5_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_444a56990f10fabdea0337c9ba0397fb.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/5_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/5_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_41ad4f6b73a394975f6b882dd220b05d.webp&#34;
               width=&#34;760&#34;
               height=&#34;470&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      K8s 服务异常排障思路
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;查看了对应的 &lt;code&gt;Node/POD&lt;/code&gt; 负载、状态等都正常；登录到 DeepFlow 平台，调出了工单系统的访问拓扑（拓扑上标红部分表明有异常），从访问拓扑可知&lt;code&gt;后端服务&lt;/code&gt;黄金指标也都一切正常；又通过图可看出来广州办公室对工单系统的访问也并没有异常（也同步与广州同事确认，访问一切正常），可推测 &lt;code&gt;DNS/SVC&lt;/code&gt; 也应该都正常；进一步结合拓扑图，&lt;strong&gt;可看出异常仅出现在北京办公室与 nginx-ingress-controller 之间&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-访问拓扑&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;访问拓扑&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/6_hud1bef9a4cabd2475621b84ea33036c69_76286_4f6bd1fc2dbbd7b0083cf8a8a7189c3d.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/6_hud1bef9a4cabd2475621b84ea33036c69_76286_efb9adcb79cd3c25995c9b171ff5563b.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/6_hud1bef9a4cabd2475621b84ea33036c69_76286_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/6_hud1bef9a4cabd2475621b84ea33036c69_76286_4f6bd1fc2dbbd7b0083cf8a8a7189c3d.webp&#34;
               width=&#34;760&#34;
               height=&#34;330&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      访问拓扑
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;继续分析标红的路径&lt;/strong&gt;，查看对应的流日志，因为云下未部署采集器，因此仅支持查看的是 nginx-ingress-controller POD 以及 Node 的数据，发现了几个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务端异常都是因为&lt;code&gt;传输-连接超时&lt;/code&gt;导致的&lt;/li&gt;
&lt;li&gt;服务端异常时，服务端 (nginx-ingress-controller) 回复的数据包，都是大包&lt;/li&gt;
&lt;li&gt;服务端异常时，服务端 (nginx-ingress-controller) 都未收到任何客户端发送的数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-流日志&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;流日志&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/7_huc5e3e591dc9829a04d00b62d7e502daa_117928_396e3bd3fe970f97b1b584ac4bc6904d.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/7_huc5e3e591dc9829a04d00b62d7e502daa_117928_05d0159c179d6fb2ce7b5e349e2b17e5.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/7_huc5e3e591dc9829a04d00b62d7e502daa_117928_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/7_huc5e3e591dc9829a04d00b62d7e502daa_117928_396e3bd3fe970f97b1b584ac4bc6904d.webp&#34;
               width=&#34;760&#34;
               height=&#34;91&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      流日志
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;结合以上几点发现，怀疑方向转移到 MSS/MTU 上&lt;/strong&gt;，立马咨询了 IT 同事，是不是变动过网关的 MSS/MTU 值，IT 同事否认了&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-聊天记录-01&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;聊天记录-01&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/8_huf1e4fc4b5eeed03130a01527d1c8241b_43181_89bedbdb7559a9b54b2ed96bc2ea3539.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/8_huf1e4fc4b5eeed03130a01527d1c8241b_43181_56b7b99bf17fdbc83721701304b56827.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/8_huf1e4fc4b5eeed03130a01527d1c8241b_43181_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/8_huf1e4fc4b5eeed03130a01527d1c8241b_43181_89bedbdb7559a9b54b2ed96bc2ea3539.webp&#34;
               width=&#34;760&#34;
               height=&#34;125&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      聊天记录-01
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;既然云下部分未变动过，转而怀疑是不是 nginx-ingress-controller 动过 MSS/MTU&lt;/strong&gt;，通过&lt;code&gt;时序图&lt;/code&gt;查看 MSS 是否有变化，通过故障前后对比可知：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;客户端在故障前后发送的 MSS 确实发生过变动，从 1280 变为了 1380&lt;/li&gt;
&lt;li&gt;服务端 (nginx-ingress-controller) MSS 值一直未变动过&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-时序图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;时序图&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/9_hu4ab3e11ffe6fca733ba5c6a764f2580f_156671_28279327edfcf58ef9f66fb1a7e7022f.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/9_hu4ab3e11ffe6fca733ba5c6a764f2580f_156671_5fab15834c833b9e54b2585a5901a64f.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/9_hu4ab3e11ffe6fca733ba5c6a764f2580f_156671_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/9_hu4ab3e11ffe6fca733ba5c6a764f2580f_156671_28279327edfcf58ef9f66fb1a7e7022f.webp&#34;
               width=&#34;760&#34;
               height=&#34;218&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      时序图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;通过数据可明确，云下一定变动过 MSS 值，拿着数据截图又去找 IT 同事，最后 IT 同事一顿找，&lt;strong&gt;明确改了 MSS 值&lt;/strong&gt;，将 MSS 值恢复后，工单系统恢复正常&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-聊天记录-02&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;聊天记录-02&#34; srcset=&#34;
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/10_hu240492604497ad2fc5a1af42bdc910e2_65738_ea2488498b2acad3032160b55d84c351.webp 400w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/10_hu240492604497ad2fc5a1af42bdc910e2_65738_2636d2f9ca8a44421014af84d3509e40.webp 760w,
               /blog/troubleshooting-of-the-k8s-application-with-deepflow/10_hu240492604497ad2fc5a1af42bdc910e2_65738_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/troubleshooting-of-the-k8s-application-with-deepflow/10_hu240492604497ad2fc5a1af42bdc910e2_65738_ea2488498b2acad3032160b55d84c351.webp&#34;
               width=&#34;760&#34;
               height=&#34;271&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      聊天记录-02
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;问题总结&#34;&gt;问题总结&lt;/h2&gt;
&lt;p&gt;问：&lt;strong&gt;MSS 值变动了，为什么影响了工单系统&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因为云下的 MTU 值设置的是 1420，如果 MSS 值为 1380 + 报文头则会大于 MTU 值，因此大数据报文无法通过云下的网关，这就导致了 nginx-ingress-controller 收不到任何客户端的回应（客户端也未收到服务端的包）出现&lt;code&gt;传输-连接超时&lt;/code&gt;的情况&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;问：&lt;strong&gt;MSS 值变动了，为什么其他内部系统未受到影响&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;工单描述中包含了大量的图片和文件，因此存在传输大数据的情况，而其他系统大部分都是文字传输，所以未受到明显影响&lt;/li&gt;
&lt;li&gt;其他如 gitlab 存在图片和大文字传输的服务，并未使用 Ingress 的方式对外提供访问形式，而是利用阿里云 Terway 提供的内网直接访问 Headless 服务后端 POD 的方式，在协商 MSS 值时，后端 POD 的值为 1360（nginx-ingress-controller 的值为 1460），因此最终协商的取 1360 + 报文头则小于 MTU 值，所以也未受到明显影响&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;什么是-deepflow&#34;&gt;什么是 DeepFlow&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepflowys/deepflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow&lt;/a&gt; 是一款开源的高度自动化的可观测性平台，是为云原生应用开发者建设可观测性能力而量身打造的全栈、全链路、高性能数据引擎。DeepFlow 使用 eBPF、WASM、OpenTelemetry 等新技术，创新的实现了 AutoTracing、AutoMetrics、AutoTagging、SmartEncoding 等核心机制，帮助开发者提升埋点插码的自动化水平，降低可观测性平台的运维复杂度。利用 DeepFlow 的可编程能力和开放接口，开发者可以快速将其融入到自己的可观测性技术栈中。&lt;/p&gt;
&lt;p&gt;GitHub 地址：https://github.com/deepflowys/deepflow&lt;/p&gt;
&lt;p&gt;访问 &lt;a href=&#34;https://deepflow.yunshan.net/docs/zh/install/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow Demo&lt;/a&gt;，体验高度自动化的可观测性新时代。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes 服务异常排障过程全解密</title>
      <link>https://cloudnative.to/blog/k8s-service-exception-troubleshooting/</link>
      <pubDate>Mon, 26 Dec 2022 12:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/k8s-service-exception-troubleshooting/</guid>
      <description>&lt;p&gt;Kubernetes（K8s）是一个用于大规模运行分布式应用和服务的开源容器编排平台。K8s 让应用发布更加快速安全，让应用部署也更加灵活，但在带来这些便利性的同时，也给应用排障增加了 K8s 平台层面的复杂度，本篇文章将以常见的服务异常入手，来详细拆解 K8s 服务访问方式，以及如何利用现有的可观测体系来对 k8s 平台和应用服务进行快速排障。&lt;/p&gt;
&lt;h2 id=&#34;服务的访问方式&#34;&gt;服务的访问方式&lt;/h2&gt;
&lt;p&gt;开启 K8s 服务异常排障过程前，须对 K8s 服务的访问路径有一个全面的了解，下面我们先介绍目前常用的 K8s 服务访问方式（不同云原生平台实现方式可能基于部署方案、性能优化等情况会存在一些差异，但是如要运维 K8s 服务，则需要在一开始就对访问方式有一个了解）。&lt;/p&gt;
&lt;p&gt;方式一：&lt;strong&gt;集群内客户端通过 ClusterIP 访问集群内服务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-集群内客户端通过-clusterip-访问集群内服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;集群内客户端通过 ClusterIP 访问集群内服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_74752a4738cb903161b3e9059f811845.webp 400w,
               /blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_20ab70f82fa40d7f8ed546bd45c88ff7.webp 760w,
               /blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_74752a4738cb903161b3e9059f811845.webp&#34;
               width=&#34;760&#34;
               height=&#34;571&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      集群内客户端通过 ClusterIP 访问集群内服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;从&lt;code&gt;访问逻辑拓扑&lt;/code&gt;来分析，集群内客户端 POD 访问的是集群内服务的 svc_name，然后在 svc 层进行 DNAT，将请求转发到对应的后端 POD。这个过程对应的&lt;code&gt;访问实现拓扑&lt;/code&gt;则要复杂不少：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step 1: client_pod 根据 DNS 配置，请求 DNS 服务器解析 svc_name，DNS 服务器会返回 svc_name 对应的 ClusterIP&lt;/li&gt;
&lt;li&gt;step 2: client_pod 请求 ClusterIP，Node 根据 kube-proxy 配置的 IPVS/IPTABLES 完成 DNAT&lt;/li&gt;
&lt;li&gt;step 3: 根据 DNAT 的结果，Node 将请求转发给对应的 server_pod，server_pod 可能与 client_pod 在同一个 Node，也可能在不同 Node，此差异主要体现在网络转发层面&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;方式二：&lt;strong&gt;集群外客户端通过 NodePort 访问集群内服务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-集群外客户端通过-nodeport-访问集群内服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;集群外客户端通过 NodePort 访问集群内服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_ada1825890c0d6a3d993f602f2d8b1dc.webp 400w,
               /blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_63f54ca416490ad2058e2c1a75722f73.webp 760w,
               /blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_ada1825890c0d6a3d993f602f2d8b1dc.webp&#34;
               width=&#34;760&#34;
               height=&#34;573&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      集群外客户端通过 NodePort 访问集群内服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;相比&lt;code&gt;方式一&lt;/code&gt;，&lt;code&gt;访问逻辑拓扑&lt;/code&gt;上 client 访问的区别是从 svc_name 替换为 nodeip:port。&lt;code&gt;访问实现拓扑&lt;/code&gt;主要过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step 1: client 直接请求 svc 对外暴露的 nodeip:port，如果是 LoadBalance 类型的服务，在此之前还会访问 LB（因为并不是 K8s 服务的中的特别能力，所以此处并无特别说明），请求转发到对应的 Node 上，Node 也会根据kube-proxy 配置的 IPVS/IPTABLES 完成 DNAT&lt;/li&gt;
&lt;li&gt;step 2: 与&lt;code&gt;方式一&lt;/code&gt;的 step 3 一致&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;方式三：&lt;strong&gt;集群外客户端通过 Ingress 访问集群内服务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-集群外客户端通过-ingress-访问集群内服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;集群外客户端通过 Ingress 访问集群内服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_5c683e9e1e1718050d385c2ea7332915.webp 400w,
               /blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_a863ea2c1db8840377006529a194fcb9.webp 760w,
               /blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_5c683e9e1e1718050d385c2ea7332915.webp&#34;
               width=&#34;760&#34;
               height=&#34;642&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      集群外客户端通过 Ingress 访问集群内服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;方式三&lt;/code&gt;相比前两种方式，引入了 Ingress 的概念，因此复杂度增加了非常多。&lt;code&gt;访问逻辑拓扑&lt;/code&gt;中外部 client 可以直接请求 url 而不是 ip 了，请求 url 会先到达 Ingress，由 Ingress 完成反向代理，转发给后端的 svc，svc 再完成 DNAT 转发给后端 POD。&lt;code&gt;访问实现拓扑&lt;/code&gt;会根据 ingress-controller 部署形式不同而有差异，ingress-controller &lt;strong&gt;非&lt;/strong&gt; hostnetwork 部署下文简称&lt;code&gt;部署模式一&lt;/code&gt;，ingress-controller hostnetwork 部署下文简称&lt;code&gt;部署模式二&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step 1：外部 client 请求 DNS 服务器解析 url 的域名，DNS 服务会返回 ingress-controller 服务所在的 IP (如果前面有挂 LB，则访问的是 LB 的 IP，否则返回的是 ingress-controller 服务的 nodeip:port)&lt;/li&gt;
&lt;li&gt;step 2：此时&lt;code&gt;部署模式一&lt;/code&gt;，则需要按&lt;code&gt;方式二&lt;/code&gt;访问 ingress-controller（为避免画图过于复杂，未画出 ingress_controller_pod 分布在不同 Node 场景）；&lt;code&gt;部署模式二&lt;/code&gt;，请求到达 Node 后，则直接转给 ingress_controller_pod&lt;/li&gt;
&lt;li&gt;step 3：此为&lt;code&gt;部署模式一&lt;/code&gt;特有的步骤，参考&lt;code&gt;方式二&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;step 4/5/6：请求经过 ingress_controller_pod 后，已经确定需要转发的后端 svc，则按&lt;code&gt;方式一&lt;/code&gt;进行转发即可（为避免画图过于复杂，未画出 server_pod 分布在同 Node 场景）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;服务异常的排障思路&#34;&gt;服务异常的排障思路&lt;/h2&gt;
&lt;p&gt;了解服务的访问方式后，在遇到服务异常时，基于一套整体的排障思路来开展工作，更能事半功倍，接下来详细聊聊如何进行排障。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-服务异常的排障思路&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;服务异常的排障思路&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_bf5388560f752cb1b71d829d571b650e.webp 400w,
               /blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_becdd3b11f3af151d1def1c3428dcd17.webp 760w,
               /blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_bf5388560f752cb1b71d829d571b650e.webp&#34;
               width=&#34;760&#34;
               height=&#34;470&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      服务异常的排障思路
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 1：&lt;strong&gt;确定 Node/POD 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首先我们可以通过 Prometheus 提供的 Dashboard 来检查部署的实例的基础状态，通过直接查看应用的 CPU/内存/带宽等指标，确认目前部署实例的负载都在正常范围内。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-node&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;node&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_054fa3090e8dcfb2c67198fba2f27887.webp 400w,
               /blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_4fba6a1ef4df6d454806077a9b2dae72.webp 760w,
               /blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_054fa3090e8dcfb2c67198fba2f27887.webp&#34;
               width=&#34;760&#34;
               height=&#34;375&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      node
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pod&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pod&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_c233bb5e096030d0498fd6ced0416705.webp 400w,
               /blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_fe5388917eb11cc9d44f8229577808df.webp 760w,
               /blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_c233bb5e096030d0498fd6ced0416705.webp&#34;
               width=&#34;760&#34;
               height=&#34;334&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      pod
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 2：&lt;strong&gt;确定后端服务是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;然后我们可以通过 DeepFlow 提供的 Dashboard 查看后端服务的黄金指标：请求、异常、时延，以快速判断目前的服务是否在正常运行。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-后端服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;后端服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_16ffa651fcfa699abd1ec8da06be592c.webp 400w,
               /blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_96b25080b78279f7b10c2ed91efbf4fe.webp 760w,
               /blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_16ffa651fcfa699abd1ec8da06be592c.webp&#34;
               width=&#34;760&#34;
               height=&#34;375&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      后端服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 3：&lt;strong&gt;确定 DNS 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;从前面&lt;strong&gt;服务的访问方式&lt;/strong&gt;一节可知，仅方式一/方式三的访问过程经过 DNS 服务，因此只有这两种场景才需要检查 DNS 服务是否异常，方式一和三都需要检查集群内的 DNS 服务是否异常，方式三相比方式一还需要检查 client 访问的集群外的 DNS 服务是否异常。对于 CoreDNS 本身，我们可以使用 Prometheus 提供的 Dashboard 来进行排障，对于排查应用服务访问 DNS 异常，我们可以使用 DeepFlow 提供的 Dashboard 查看 DNS 服务的请求、异常、时延指标。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-coredns&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;coredns&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_5296faef1387c2836bf8555e5a9adf1d.webp 400w,
               /blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_41ff233d1182b87668c74f92ec5ce803.webp 760w,
               /blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_5296faef1387c2836bf8555e5a9adf1d.webp&#34;
               width=&#34;760&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      coredns
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-dns&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;DNS&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_ae9a164663d73a94ba6a966bc16dd812.webp 400w,
               /blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_266cb7c6de06e5ee1d69e40e0f580231.webp 760w,
               /blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_ae9a164663d73a94ba6a966bc16dd812.webp&#34;
               width=&#34;760&#34;
               height=&#34;416&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      DNS
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;如 DNS 服务无异常，则可直接用 ClusterIP 访问服务，如果能正常访问，那可以确定是 DNS 的问题，并且这个问题很大可能就是配置错误。&lt;/p&gt;
&lt;p&gt;step 4：&lt;strong&gt;确定 SVC 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因为 SVC 的功能实际是 kube-proxy 同步配置到 IPVS/IPTABLES 来实现的，所以我们可以参考 &lt;strong&gt;step 1&lt;/strong&gt; 的排查步骤，把 kube-proxy 视作应用 POD，通过 Prometheus 提供的 Dashboard 查看 kube-proxy 是否正常。&lt;/p&gt;
&lt;p&gt;如果能确定应用服务运行正常，可以尝试直接访问后端 POD，如果能正常访问，则可以继续分析 SVC 问题，很大可能是 IPVS/IPTABLES 配置错误。&lt;/p&gt;
&lt;p&gt;step 5：&lt;strong&gt;确定 Ingress 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;服务访问方式&lt;/strong&gt;中方式三的场景下，如果需要检查 Ingress 的状态，可以查看基于 ingress-controller 服务的状态/负载/请求日志等构建的 Dashboard。这一类 Dashboard 除了 Prometheus/DeepFlow 有提供之外，各个 API 网关的厂商也有提供，可以用 DeepFlow + 厂商提供的 Dashboard 结合进行分析，厂商会更关注网关本身的分析，DeepFlow 则更关注全链路分析，快速定位问题点。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ingress&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ingress&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_07b820b9533958df09528a1c5dca577d.webp 400w,
               /blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_d917b9a51ee64f68baa20caf40dc8a38.webp 760w,
               /blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_07b820b9533958df09528a1c5dca577d.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ingress
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 6：&lt;strong&gt;追踪访问路径异常点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上述排障过程，都是独立的一个个点检查，如果都没问题，则可以去追踪报障的某一次访问路径是否有异常。如果能直接定位访问路径，确认问题点就会变得更简单。比如我们发现访问路径如果存在断路，则分析断路位置即可；如果追踪的是时延高的问题，则分析追踪到的每一段路径的时延即可。访问路径需要能覆盖从应用-&amp;gt;系统-&amp;gt;网络各个层面，目前提供这样全链路追踪能力的组件不多，可以使用 DeepFlow 自动化的分布式追踪能力来进行排查。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-追踪访问路径&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;追踪访问路径&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_3c8ae998802f92c7fd3484880512762c.webp 400w,
               /blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_8bf0f8fdd120c1f9476a466ec255a8f4.webp 760w,
               /blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_3c8ae998802f92c7fd3484880512762c.webp&#34;
               width=&#34;760&#34;
               height=&#34;130&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      追踪访问路径
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;什么是-deepflow&#34;&gt;什么是 DeepFlow&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepflowys/deepflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow&lt;/a&gt; 是一款开源的高度自动化的可观测性平台，是为云原生应用开发者建设可观测性能力而量身打造的全栈、全链路、高性能数据引擎。DeepFlow 使用 eBPF、WASM、OpenTelemetry 等新技术，创新的实现了 AutoTracing、AutoMetrics、AutoTagging、SmartEncoding 等核心机制，帮助开发者提升埋点插码的自动化水平，降低可观测性平台的运维复杂度。利用 DeepFlow 的可编程能力和开放接口，开发者可以快速将其融入到自己的可观测性技术栈中。&lt;/p&gt;
&lt;p&gt;GitHub 地址：&lt;a href=&#34;https://github.com/deepflowys/deepflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/deepflowys/deepflow&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;访问 &lt;a href=&#34;https://deepflow.yunshan.net/docs/zh/install/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow Demo&lt;/a&gt;，体验高度自动化的可观测性新时代。&lt;/p&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://deepflow.yunshan.net/docs/zh/about/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://deepflow.yunshan.net/docs/zh/about/overview/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/1860-node-exporter-full/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/1860-node-exporter-full/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/15661-1-k8s-for-prometheus-dashboard-20211010/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/15661-1-k8s-for-prometheus-dashboard-20211010/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/9614-nginx-ingress-controller/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/9614-nginx-ingress-controller/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/14981-coredns/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/14981-coredns/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/service/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nginx.com/products/nginx-ingress-controller/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nginx.com/products/nginx-ingress-controller/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/haproxytech/kubernetes-ingress#readme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/haproxytech/kubernetes-ingress#readme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service-topology/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/service-topology/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/mp5coRHPAdx5nIfcCnPFhw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mp.weixin.qq.com/s/mp5coRHPAdx5nIfcCnPFhw&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>DeepFlow AutoLogging 介绍：自动采集应用调用日志和流日志</title>
      <link>https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/</link>
      <pubDate>Fri, 14 Oct 2022 12:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/</guid>
      <description>&lt;p&gt;本文为云杉网络&lt;code&gt;原力释放 - 云原生可观测性分享会&lt;/code&gt;第九期直播实录。&lt;a href=&#34;https://www.bilibili.com/video/BV1Z14y147XM?spm_id_from=333.337.search-card.all.click&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;回看链接&lt;/a&gt;，&lt;a href=&#34;http://yunshan-guangzhou.oss-cn-beijing.aliyuncs.com/yunshan-ticket/pdf/f9d3931bd360757b30b91d93d1aa9621_20220823171717.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPT下载&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;DeepFlow 是一款开源的高度自动化的可观测性平台，是为云原生应用开发者建设可观测性能力而量身打造的全栈、全链路、高性能数据引擎。DeepFlow 使用 eBPF、WASM、OpenTelemetry 等新技术，创新的实现了 AutoTracing、AutoMetrics、AutoTagging、SmartEncoding 等核心机制，帮助开发者提升埋点插码的自动化水平，降低可观测性平台的运维复杂度。利用 DeepFlow 的可编程能力和开放接口，开发者可以快速将其融入到自己的可观测性技术栈中。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-deepflow---架构图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;DeepFlow - 架构图&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630472cfdd6cc_huc2aad606b97861a70cfe5cda2c879b2d_243567_1a77528a697b143b907e8033374cb74d.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630472cfdd6cc_huc2aad606b97861a70cfe5cda2c879b2d_243567_1e08ffab7b50e60627cf94ca356034ed.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630472cfdd6cc_huc2aad606b97861a70cfe5cda2c879b2d_243567_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630472cfdd6cc_huc2aad606b97861a70cfe5cda2c879b2d_243567_1a77528a697b143b907e8033374cb74d.webp&#34;
               width=&#34;760&#34;
               height=&#34;431&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      DeepFlow - 架构图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;今天的内容是云杉网络“云原生可观测性分享会”的直播里面第八期&lt;a href=&#34;https://deepflow.yunshan.net/blog/001-a-new-era-of-automated-observability/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow首个开源版本&lt;/a&gt;的分享的延续，上篇主要和大家详细聊了 AutoMetrics 和 AutoTracing 的能力，对于可观测领域三大支柱的的 Logging，在这次博客及直播中给大家带来详细讲解。&lt;/p&gt;
&lt;p&gt;今天从三个方面给大家进行分享：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一：分享应用调用日志，从数据来源、数据抽象到数据使用三个角度和大家谈谈，如何自动采集的 HTTP/MySQL 等多协议调用日志；&lt;/li&gt;
&lt;li&gt;二：分享网络流日志，主要对比公有云的流日志及流日志的应用场景；&lt;/li&gt;
&lt;li&gt;三：讲解 AutoLogging 的实现，基于 BPF/eBPF 的自动日志采集能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;0x0-应用调用日志-数据来源&#34;&gt;0x0: 应用调用日志-数据来源&lt;/h2&gt;
&lt;p&gt;首先强调，应用调用日志与应用在代码层面打的日志不同，例如 Nginx 的 AccessLog，MySQL 的 General Log/Error Log 这些都是调用日志的范畴。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-数据来源&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-数据来源&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/202208236304743fc4697_hu2792be96ffaa8c3369e2e0e31aa9872f_342596_e4a69a3adf5a51a1a0970a53a3dfbd16.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/202208236304743fc4697_hu2792be96ffaa8c3369e2e0e31aa9872f_342596_d518bd38a3c8a345c6d93d9b431f0a02.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/202208236304743fc4697_hu2792be96ffaa8c3369e2e0e31aa9872f_342596_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/202208236304743fc4697_hu2792be96ffaa8c3369e2e0e31aa9872f_342596_e4a69a3adf5a51a1a0970a53a3dfbd16.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-数据来源
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;但是这些日志都是单个组件的日志，并不是应用的调用日志，对于应用问题的排查，需要挨个去找组件的负责人看日志，但组件负责人不懂业务，不知道如何快速搜索日志，导致了问题的排查过程中协作成本巨高。&lt;/p&gt;
&lt;p&gt;应用的调用日志是给 Dev 团队建设的，一个站在应用视角快速查看所有的调用详情信息的能力，其实这个能力获取可以将目前现有的组件日志都集中起来查看也是一种思路，但是如何以应用无感知/自动化的形式低成本的接入，以及更符合云原生的这个理念来实现的话，这是目前市面上没有的，这是 DeepFlow 的 AutoLogging 的价值点所在。&lt;/p&gt;
&lt;p&gt;DeepFlow 的调用日志，其实由各种各样的应用协议组成的，目前 DeepFlow 平台上已经包含了例如网络应用的 HTTP 的访问日志、DNS 的查询日志、SQL/NoSQL 的操作日志、RPC 的调用日志、MQ 的 API 调用日志，也会包含可观测领域中 Tracing 的数据，例如 OpenTelmetry 协议的 Span 调用，还会陆续支持一些物联网的协议，例如 MQTT 的日志。&lt;/p&gt;
&lt;h2 id=&#34;0x1-应用调用日志-数据抽象&#34;&gt;0x1: 应用调用日志-数据抽象&lt;/h2&gt;
&lt;p&gt;可以看到数据的来源非常的丰富，随着社区的需求和版本的迭代，将会有更多协议的数据接入。如果需更好的使用这些‘五花八门’的数据，需要对数据进行治理，治理的第一步，对数据进行统一的抽象，数据抽象将从公共字段、请求字段、响应字段、指标量，这四个层面来展开：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-数据抽象&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-数据抽象&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b132d70_hub03109f6e94c1dd13256226bc2b0ded2_125070_69b983a0d6fd5f0422ec9f7ff619a2c2.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b132d70_hub03109f6e94c1dd13256226bc2b0ded2_125070_284527473461cf5335a185e0292801c6.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b132d70_hub03109f6e94c1dd13256226bc2b0ded2_125070_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b132d70_hub03109f6e94c1dd13256226bc2b0ded2_125070_69b983a0d6fd5f0422ec9f7ff619a2c2.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-数据抽象
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;公共字段：包含应用协议、协议版本、日志类型，其中日志类型包含请求/响应/会话类型，一般协议都是这三种，也会有一些协议有些例外，例如 OpenTelemetry 协议，仅一个会话类型。&lt;/li&gt;
&lt;li&gt;请求字段：整体抽象为请求类型、请求域名、请求资源、请求ID，例如HTTP的方法，MySQL 的命令类型，DNS 的查询类型都为请求类型，HTTP 的 host 对应请求域名，HTTP 的 Url、MySQL 的命令、DNS 的查询名称都对应请求资源，这个请求资源的抽象是参考各个 APM 的厂商的定义，例如 Datadog 的 Resource，Skywalking 的 Endpoint。&lt;/li&gt;
&lt;li&gt;响应字段：分为响应状态、响应码、响应异常、结果，整体来说基本都是对应响应码映射的。&lt;/li&gt;
&lt;li&gt;指标量：分为吞吐请求长度、响应长度的字段，以及响应时延字段，结合指标量可以更好的分析调用。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;0x2-应用调用日志-自定义属性&#34;&gt;0x2: 应用调用日志-自定义属性&lt;/h2&gt;
&lt;p&gt;数据抽象的收益是统一管理，可弊端也在统一。在设计之初，其实就考虑了要做自定义属性的扩展，随着 OpenTemetry 的 Tracing 数据接入，这个事情就变的更加重要。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-自定义属性&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-自定义属性&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b43a9f8_huf6ddda08e4c3256565d4f411d44991c9_264617_ab69245383ec3e0b4b3dc78c2dc5e0e5.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b43a9f8_huf6ddda08e4c3256565d4f411d44991c9_264617_8490c6f060868f766fbcb69697876c43.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b43a9f8_huf6ddda08e4c3256565d4f411d44991c9_264617_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b43a9f8_huf6ddda08e4c3256565d4f411d44991c9_264617_ab69245383ec3e0b4b3dc78c2dc5e0e5.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-自定义属性
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;因此除了定义的标准字段外，又定义了 Attribute_Names 和 Attribute_Values 这两个数组，数组里面可以携带自定义属性和自定义属性对应的值，这个是根据不同的需求来携带，没有长度和格式的限制，非常的灵活。&lt;/p&gt;
&lt;p&gt;两个数组里面的 Key 和 Value 按顺序来进行映射，在产品化的时候，通过 Qurey 组件进行转化，用户是无感知数组的存在的，看到的都是 Key，Value 这样的属性关系，通过 Key 查询来获取 Value，这个和使用其他 Tag 查询的逻辑也是一致的。&lt;/p&gt;
&lt;h2 id=&#34;0x3-应用调用日志-autotagging&#34;&gt;0x3: 应用调用日志-AutoTagging&lt;/h2&gt;
&lt;p&gt;刚刚分析的是各种协议如何映射为调用日志，站在应用的视角已经可以统一查看调用日志了。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-autotagging&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-AutoTagging&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b1b3c9b_hu60b775c125c154d718ad03e8ac551e94_334563_a087f5cd4c78716cfc9dc4d662b67f13.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b1b3c9b_hu60b775c125c154d718ad03e8ac551e94_334563_9cfafbdc5f0f7d82f88b96778d76dc2b.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b1b3c9b_hu60b775c125c154d718ad03e8ac551e94_334563_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b1b3c9b_hu60b775c125c154d718ad03e8ac551e94_334563_a087f5cd4c78716cfc9dc4d662b67f13.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-AutoTagging
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;而如何快速过滤应用呢？这也是一个必须解决的问题，在传统架构中，一般会根据IP段或者根据所在服务器来过滤，但是应用架构逐步迁移到云上，开始使用微服务架构后，IP 已经不再稳定，而资源也不再简单是服务器了，这种时候如何来快速过滤应用呢？&lt;/p&gt;
&lt;p&gt;DeepFlow 的 AutoTagging 能力，可以给调用日志打上各种云厂商的标签，比如租户、区域、子网、云服务器、RDS、负载均衡器、NAT 网关、Kubernetes 的命名空间、容器服务、工作负载、动态 Label 等等，有了这些标签，则可以快速的根据各种云标签过滤应用，然后查看应用的调用日志了。&lt;/p&gt;
&lt;p&gt;以上主要和大家分享了应用调用日志背后数据处理的一些理论能力，接下来带大家感受下基于这样的能力，应用调用日志激发的实际价值。&lt;/p&gt;
&lt;h2 id=&#34;0x4-应用调用日志-总览&#34;&gt;0x4: 应用调用日志-总览&lt;/h2&gt;
&lt;p&gt;这是基于调用日志构建的一张 Grafana 的 Dashboard，这个 Dashboard 主要可查看服务的调用关系、RED 指标量。Dashboard 就是基于前面数据抽象来实现的。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-总览&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-总览&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b336e43_hub52690106ad4b3e1f9bfb4445854cb46_219068_9f2c1ced9ac937db2fd41fba3a9f5151.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b336e43_hub52690106ad4b3e1f9bfb4445854cb46_219068_9120dc4d6899aa5e29e29da920693c34.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b336e43_hub52690106ad4b3e1f9bfb4445854cb46_219068_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b336e43_hub52690106ad4b3e1f9bfb4445854cb46_219068_9f2c1ced9ac937db2fd41fba3a9f5151.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-总览
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;我们可以通过 AutoTagging 打上的标签，Dashboard 主要使用 Kubernetes 相关的标签，快速过滤应用，比如 DeepFlow 这个应用，就直接过滤 Namespace=DeepFlow 就可以了。然后结合 Grafana 的一些阈值能力，就可以快速的在视觉找到需要关注的服务，从而缩小问题定位的范围。&lt;/p&gt;
&lt;h2 id=&#34;0x5-应用调用日志-http访问日志&#34;&gt;0x5: 应用调用日志-HTTP访问日志&lt;/h2&gt;
&lt;p&gt;接下来看看如何查看 HTTP 的调用日志以及 DeepFlow 平台的调用日志与 AccessLog 的差异。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-http访问日志&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-HTTP访问日志&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b27ffc7_hu3cd5b11e51023acf56987b1314eb84b3_248896_b0500c6f1ea717103908b4fdfde4f964.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b27ffc7_hu3cd5b11e51023acf56987b1314eb84b3_248896_cdfbd53c7a5a66ee857b21de9c4a93d6.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b27ffc7_hu3cd5b11e51023acf56987b1314eb84b3_248896_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b27ffc7_hu3cd5b11e51023acf56987b1314eb84b3_248896_b0500c6f1ea717103908b4fdfde4f964.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-HTTP访问日志
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;左边是在 Grafana 上构建的应用调用日志的 Dashboard，可根据 TAG 过滤应用，根据 Protocol 过滤 HTTP、HTTPS、HTTP2 协议，即可查看当前服务的 HTTP 的调用日志。&lt;/p&gt;
&lt;p&gt;右边是将 AccessLog 与 DeepFlow 的应用调用日志做的一个映射，通过对比，可看出来除了 remote_user 其他都能映射的非常好。&lt;/p&gt;
&lt;p&gt;HTTP 访问日志除了作为代替 AccessLog，还可以结合调用日志的状态和指标量，快速知道哪些调用存在异常，哪些调用响应慢。&lt;/p&gt;
&lt;h2 id=&#34;0x6-应用调用日志-mysql慢查询日志&#34;&gt;0x6: 应用调用日志-MySQL慢查询日志&lt;/h2&gt;
&lt;p&gt;对于 MySQL 慢查询的日志，在云上数据库实例化后，想看数据库的日志，其实并不容易，需要在云上开启各种设置和权限，及时看到了日志，也比较难快速的去过滤对应的应用日志&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-mysql慢查询日志&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-MySQL慢查询日志&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b392aa4_hu32b03da4e200016a978ea8945cf5b8dd_278781_8272f3b666809254effd54a3a8994ea5.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b392aa4_hu32b03da4e200016a978ea8945cf5b8dd_278781_20f97ed061bbcc49f0265d095754bb5d.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b392aa4_hu32b03da4e200016a978ea8945cf5b8dd_278781_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b392aa4_hu32b03da4e200016a978ea8945cf5b8dd_278781_8272f3b666809254effd54a3a8994ea5.webp&#34;
               width=&#34;760&#34;
               height=&#34;431&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-MySQL慢查询日志
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;我们来看看 DeepFlow 是如何查看慢查询日志的，这个是刚刚 HTTP 调用日志一样的 Dashboard，仅需要切换下搜索条件即可，将协议切换为 MySQL，request_type 输入为 COM_QUREY，以及 request_resource 为 SELECT*。&lt;/p&gt;
&lt;p&gt;设置好这样的过滤条件，得到的就是 MySQL 的查询日志，接着再对响应时延排序过滤，就可以找到慢查询了。&lt;/p&gt;
&lt;h2 id=&#34;0x7-应用调用日志-分布式追踪-span-日志&#34;&gt;0x7: 应用调用日志-分布式追踪 Span 日志&lt;/h2&gt;
&lt;p&gt;除了看网络应用协议的调用日志外，通过前面的数据来源我们也知道，调用日志也支持接入分布式追踪协议的Span信息。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-分布式追踪-span-日志&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-分布式追踪 Span 日志&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b4dd0dd_hu68da37370e7e8c038ee666ae1b19803e_278352_ac72f3a839b7b9b56bdb84a6d33a745c.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b4dd0dd_hu68da37370e7e8c038ee666ae1b19803e_278352_3c12a53f517d94d8d172e8f40bb95f35.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b4dd0dd_hu68da37370e7e8c038ee666ae1b19803e_278352_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b4dd0dd_hu68da37370e7e8c038ee666ae1b19803e_278352_ac72f3a839b7b9b56bdb84a6d33a745c.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-分布式追踪 Span 日志
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;目前 DeepFlow 已经支持对接 OpenTelemtry 的 Span 信息，每个 Span 其实都对应着一个调用，当前展示的就是 Opentelemtry的一个 Span 日志。&lt;/p&gt;
&lt;p&gt;接入 Span 的信息后，除了可以看日志，根据状态、指标量来定位调用问题外，还有一个重要的目的，就是还可以基于目前 DeepFlow 平台已有的网络中采集的调用和通过 eBPF 采集的调用，进行全栈全链路的追踪。&lt;/p&gt;
&lt;h2 id=&#34;0x8-应用调用日志-全栈全链路追踪&#34;&gt;0x8: 应用调用日志-全栈全链路追踪&lt;/h2&gt;
&lt;p&gt;这就是一个最终追踪出来的火焰图，这个火焰图上不仅包含应用代码层面的调用，也包含了系统层面、网络层面，针对如何追踪这个事，由于时间问题，今天就不展开细说，我会利用后续的直播继续给大家详细的去分享，如何对应用进行全栈全链路的追踪。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-全栈全链路追踪&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-全栈全链路追踪&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b56aa4f_hua213e2ee03787a7acf451ae201ca34d6_234727_f670eb49f6aeda2d73ad74b508b89f7f.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b56aa4f_hua213e2ee03787a7acf451ae201ca34d6_234727_c1e7638af28f09d8ccdb908992fde2e1.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b56aa4f_hua213e2ee03787a7acf451ae201ca34d6_234727_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b56aa4f_hua213e2ee03787a7acf451ae201ca34d6_234727_f670eb49f6aeda2d73ad74b508b89f7f.webp&#34;
               width=&#34;760&#34;
               height=&#34;426&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-全栈全链路追踪
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;应用调用日志，仅能观测到应用层面的一些问题，DeepFlow 可以通过 FlowID 将应用调用背后的网络流日志关联起来。接下来分享网络流日志能有什么样的能力。&lt;/p&gt;
&lt;h2 id=&#34;0x9-网络流日志-功能定义&#34;&gt;0x9: 网络流日志-功能定义&lt;/h2&gt;
&lt;p&gt;先看下公有云对网络流日志的功能说明，这是阿里云的一个定义，是捕获特定位置的流量，将流量转化为流日志记录下来，流日志是记录捕获特定时间窗口的特定五元组的网络流。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-网络流日志-功能定义&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;网络流日志-功能定义&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b7ac14f_hu01b3a75c3d695aa81e5d7ff7532a3273_284596_85eaa436c0666f92067948b906a153b8.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b7ac14f_hu01b3a75c3d695aa81e5d7ff7532a3273_284596_f8162fdf79835c7f8cf9fc99f04ec169.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b7ac14f_hu01b3a75c3d695aa81e5d7ff7532a3273_284596_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b7ac14f_hu01b3a75c3d695aa81e5d7ff7532a3273_284596_85eaa436c0666f92067948b906a153b8.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      网络流日志-功能定义
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于基础功能的定义，DeepFlow 是有遵循公有云的定义的，并在此基础上还有更丰富的能力。&lt;/p&gt;
&lt;h2 id=&#34;0x10-网络流日志-deepflow-与公有云对比&#34;&gt;0x10: 网络流日志-DeepFlow 与公有云对比&lt;/h2&gt;
&lt;p&gt;接下来看看 DeepFlow 流日志与公有云流日志的对比，解读其中的一些差异点。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-网络流日志-deepflow-与公有云对比&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;网络流日志-DeepFlow 与公有云对比&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b69ae96_hu041b1aaf6f7326c1fcf703d2de33209c_163180_c2259af1f66dc6d317a93767f0c46049.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b69ae96_hu041b1aaf6f7326c1fcf703d2de33209c_163180_d14db8e162d3507f5c30f03ffbd3f930.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b69ae96_hu041b1aaf6f7326c1fcf703d2de33209c_163180_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b69ae96_hu041b1aaf6f7326c1fcf703d2de33209c_163180_c2259af1f66dc6d317a93767f0c46049.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      网络流日志-DeepFlow 与公有云对比
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;先看看捕获周期，DeepFlow 的粒度能小到1分钟，同时捕获位置 DeepFlow 也更丰富，除了VPC网络，也会覆盖到容器网络、物理网络、也能从网络层面扩展到系统层面。&lt;/p&gt;
&lt;p&gt;接着看看 TAG，配合 DeepFlow 的 AutoTagging 的能力，DeepFlow 流日志的 TAG 是远比公有云更丰富的，除了 VPC 网络的一些 Tag，还包含隧道的 Tag、容器网络，以及更丰富的采集位置 Tag。&lt;/p&gt;
&lt;p&gt;接着指标量，公有云仅有 Packet/Byte 这两个，DeepFlow 则覆盖了从网络吞吐到性能，再到时延多个维度。&lt;/p&gt;
&lt;p&gt;在 DeepFlow 的流日志中，增加了流状态字段，可通过此字段快速过滤异常的流，这是目前公有云上不支持的。当然公有云支持的日志状态字段和安全策略的状态，DeepFlow 目前不支持，不过此功能也已经加入到排期中了。&lt;/p&gt;
&lt;p&gt;其次从成本上看，DeepFlow 开源版本可一键部署于自建环境。&lt;/p&gt;
&lt;p&gt;下面我们来看下 DeepFlow 网络流日志功能，具体能解决什么问题。&lt;/p&gt;
&lt;h2 id=&#34;0x11-网络流日志-总览&#34;&gt;0x11: 网络流日志-总览&lt;/h2&gt;
&lt;p&gt;这是基于网络流日志构建的 Granafa 的 Dashboard，是可以和应用调用日志一样，查看服务的调用关系，但是和应用调用日志不一样的是，这个总览的 Dashboard 查看的是网络层面的指标量，比如吞吐、重传、建连失败、建连时延等指标数据。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-网络流日志-总览&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;网络流日志-总览&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b77ef48_hub0ded50113a6e9891611b222c7d2bff2_241570_deed289debfdeafe38411b13977b756a.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b77ef48_hub0ded50113a6e9891611b222c7d2bff2_241570_a168bec9a44ebe67d58e0a7c57fba239.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b77ef48_hub0ded50113a6e9891611b222c7d2bff2_241570_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b77ef48_hub0ded50113a6e9891611b222c7d2bff2_241570_deed289debfdeafe38411b13977b756a.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      网络流日志-总览
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;0x12-网络流日志-网络时延&#34;&gt;0x12: 网络流日志-网络时延&lt;/h2&gt;
&lt;p&gt;查看应用调用日志时，经常会关注响应时延慢的调用，可这个响应慢，除了应用本身响应慢以外，还可能是TCP建连慢，也有可能是数据传输，也可能是协议栈慢，对于网络相关时延的排查，需要查看应用调用对应的流日志来分析。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-网络流日志-网络时延&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;网络流日志-网络时延&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b8aa56a_hu59c0393a07d86fdc1bb708fb9888f3e5_272163_a601adf26fc2001b9c68dc16a07d51fd.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b8aa56a_hu59c0393a07d86fdc1bb708fb9888f3e5_272163_d042be316067959d97493100446a7205.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b8aa56a_hu59c0393a07d86fdc1bb708fb9888f3e5_272163_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b8aa56a_hu59c0393a07d86fdc1bb708fb9888f3e5_272163_a601adf26fc2001b9c68dc16a07d51fd.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      网络流日志-网络时延
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;首先应用调用日志和网络流日志是如何关联的，DeepFlow 平台上是通过一个 FlowID 来将两个日志进行关联，因此可以根据调用日志的 FlowID，在流日志中进行查找，找到这条调用对应的流日志，然后分析流日志中的建连时延、系统时延和数据传输时延指标量，排查网络时延高导致了应用调用响应慢。&lt;/p&gt;
&lt;h2 id=&#34;0x13-网络流日志-流状态异常日志&#34;&gt;0x13: 网络流日志-流状态异常日志&lt;/h2&gt;
&lt;p&gt;应用调用日志是可以根据状态查看异常日志，流日志也是一样，可以对状态进行过滤查看异常的流日志，因此这个时候就能去看看调用异常背后是否因为网络异常导致。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-网络流日志-流状态异常日志&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;网络流日志-流状态异常日志&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b91b3ab_huddc5c36b8ed12ca8bd89c247c2fa4d0d_363743_5e06b098e2072a229c2ed44e5765be8f.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b91b3ab_huddc5c36b8ed12ca8bd89c247c2fa4d0d_363743_bae7eb641f0a58e94549d8ee7185c932.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b91b3ab_huddc5c36b8ed12ca8bd89c247c2fa4d0d_363743_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b91b3ab_huddc5c36b8ed12ca8bd89c247c2fa4d0d_363743_5e06b098e2072a229c2ed44e5765be8f.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      网络流日志-流状态异常日志
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;右上角给出来了 DeepFlow 流日志里面的状态定义，主要对流结束类型来进行定义，比如建连时延，因为端口复用可关闭，比如传输过程中，服务端发送 RST 报文导致的结束。&lt;/p&gt;
&lt;h2 id=&#34;0x14-网络流日志-tcp时序日志&#34;&gt;0x14: 网络流日志-TCP时序日志&lt;/h2&gt;
&lt;p&gt;接下来继续深入的结合 TCP 时序日志，分析具体的包的时延和问题。特别说明下：TCP 时序日志目前是 DeepFlow 企业版的增强功能了，现在开源的版本里面是没有的。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-网络流日志-tcp时序日志&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;网络流日志-TCP时序日志&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479b9aa702_hub972b571ddfa7861deb76eeae0ca5946_185448_d03c1a1831ed974e01f5dc2816661be8.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b9aa702_hub972b571ddfa7861deb76eeae0ca5946_185448_ed07e2ed9d587ead6803b1af5156f0a2.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479b9aa702_hub972b571ddfa7861deb76eeae0ca5946_185448_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479b9aa702_hub972b571ddfa7861deb76eeae0ca5946_185448_d03c1a1831ed974e01f5dc2816661be8.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      网络流日志-TCP时序日志
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;用一个&lt;a href=&#34;https://ce-demo.deepflow.yunshan.net/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;简单的 Demo&lt;/a&gt;，讲解开源的调用日志和流日志功能。这是我们给开源社区搭建的一个 Demo 环境，这个 Demo 环境是基于 Grafana 来构建的，已经构建了很多应用和网络相关的 Dashboard。&lt;/p&gt;
&lt;h2 id=&#34;0x15-autologging-采集&#34;&gt;0x15: AutoLogging-采集&lt;/h2&gt;
&lt;p&gt;接下来从日志采集和日志处理两个方面给大家介绍，AutoLogging 是如何基于 BPF/eBPF 来自动采集日志的。&lt;/p&gt;
&lt;p&gt;首先我们来看看采集部分，采集部需分别从调用日志和流日志两个方面来看。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-autologging-采集-1&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;AutoLogging-采集-1&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479ba66cf7_hu7838f1a539b661a14c56461242e9de3a_159705_761f1b40e357eacaefc8cb274d19dc46.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479ba66cf7_hu7838f1a539b661a14c56461242e9de3a_159705_fdb222fbd13fd0a0eed842430b0b24fd.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479ba66cf7_hu7838f1a539b661a14c56461242e9de3a_159705_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479ba66cf7_hu7838f1a539b661a14c56461242e9de3a_159705_761f1b40e357eacaefc8cb274d19dc46.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      AutoLogging-采集-1
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;流日志：流日志通过前面产品介绍可知，是根据网络流量来生成的日志，因此采集则主要集中在网络层面，目前可覆盖物理网络一直到虚拟网络，可以采集宿主机到虚拟机、一直到容器 POD 的网卡的流量，在实现上流日志通过 BPF + AF_PACKET 技术来完成，其中 Windows 系统的采集则通过使用 Winpcap 来完实现的。&lt;/li&gt;
&lt;li&gt;调用日志：调用日志的数据包含两部分数据，一部分是从网络应用协议来的，还有一部分是可观测的 Tracing 数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-autologging-采集-2&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;AutoLogging-采集-2&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479badd87a_hu59a0214204ec17cafcb721b82101bad4_182575_a33ccbd277d811d3bf62ea8d821129e1.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479badd87a_hu59a0214204ec17cafcb721b82101bad4_182575_1b44c1fa3e078a006b4f6f8d4d3ab54d.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479badd87a_hu59a0214204ec17cafcb721b82101bad4_182575_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479badd87a_hu59a0214204ec17cafcb721b82101bad4_182575_a33ccbd277d811d3bf62ea8d821129e1.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      AutoLogging-采集-2
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于网络应用协议这部分的数据，调用日志既包含了网络层面采集的，也扩展到了 Sidecar 和应用进程层面，对于网络层面采集的位置和实现技术与流日志是一致，只是处理逻辑会有一些不一样；而对于 Sidecar 和应用进程层面，则是使用 eBPF 技术来实现的，其中对于非加密和非压缩的协议，则通过 eBPF 的 Kprobe 和 Tracepoints 来完成，而对于 HTTP2、HTTPS 则需要使用 Uprobe 来完成。&lt;/p&gt;
&lt;p&gt;对于 Opentelemetry 的数据接入，是通过 Otel-Collector 将 Traces 的数据发送给 deepflow-agent，就完成了 Tracing的数据接入。采集的部分先分享到这里，接下来我们看看采集完成后，会进行些什么样的处理。&lt;/p&gt;
&lt;h2 id=&#34;0x16-autologging-处理&#34;&gt;0x16: AutoLogging-处理&lt;/h2&gt;
&lt;p&gt;对于日志的处理，分为三个部分：公共处理部分、流日志处理、调用日志处理。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-autologging-处理&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;AutoLogging-处理&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479bb95f59_huf6a5649809aee74a8f72345c7502f07d_116232_b95bbb823835a3156a9f066158ec077a.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479bb95f59_huf6a5649809aee74a8f72345c7502f07d_116232_784726316334366380c620559cd0e1a0.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479bb95f59_huf6a5649809aee74a8f72345c7502f07d_116232_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479bb95f59_huf6a5649809aee74a8f72345c7502f07d_116232_b95bbb823835a3156a9f066158ec077a.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      AutoLogging-处理
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对于网络流量的处理可分为：隧道拆解，其中对于隧道拆解，基本主流的隧道协议，都已经支持，比如 Vxlan，IPIP，Gre 等等。隧道拆解完成后，则会按协议栈的顺序依次解析协议，从链路层一直到传输层。&lt;/p&gt;
&lt;p&gt;接着需要对于流量进行 AutoTagging 的预处理，这里主要加上唯一的 Tag，方便后面 Server 根据唯一 Tag 增加全量 Tag。到这步，对于不同的日志需要分开处理了，对于网络流日志，此时可以根据产品定义去生成流日志。&lt;/p&gt;
&lt;p&gt;对应用调用日志，还需要完成应用协议识别，确定具体协议后，再进行应用协议解析，最后才能根据定义生成调用日志。&lt;/p&gt;
&lt;p&gt;对于应用调用日志，除了刚刚分享的这个处理流程，还有另外一条路径，主要是因为应用调用日志不仅包含网络应用协议，还包含 APM 定义的 Tracing 数据，对于这部分数据，可以直接接入，接入后直接解析即可。&lt;/p&gt;
&lt;h2 id=&#34;0x17-应用调用日志-协议扩展&#34;&gt;0x17: 应用调用日志-协议扩展&lt;/h2&gt;
&lt;p&gt;额外说下如何扩展一个应用协议。前面一直在说应用调用日志支持接入各种各样的协议，这里大概分享下协议接入需要做一些什么事情。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-应用调用日志-协议扩展&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;应用调用日志-协议扩展&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479bc0101a_hu0e5645ce1169f10e6c30d46c7beff9e8_495139_22f8313c9b7b05cb3b5e824c9cd6efae.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479bc0101a_hu0e5645ce1169f10e6c30d46c7beff9e8_495139_c67a59febb905e114f6f874c688312a3.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479bc0101a_hu0e5645ce1169f10e6c30d46c7beff9e8_495139_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479bc0101a_hu0e5645ce1169f10e6c30d46c7beff9e8_495139_22f8313c9b7b05cb3b5e824c9cd6efae.webp&#34;
               width=&#34;760&#34;
               height=&#34;426&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      应用调用日志-协议扩展
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;第一部分：需要解析协议；&lt;/p&gt;
&lt;p&gt;第二部分：协议解析完成后，需要将协议映射到调用日志中；&lt;/p&gt;
&lt;p&gt;第三部分：除了调用日志外，DeepFlow 还提供预聚合数据的能力，对应用 RED 指标进行计算。&lt;/p&gt;
&lt;p&gt;协议扩展要做的事情就是这些，目前 DeepFlow 已经开源，也欢迎开源社区的小伙伴们来贡献更多的协议，让应用调用日志更丰富。&lt;/p&gt;
&lt;h2 id=&#34;0x18-总结&#34;&gt;0x18： 总结&lt;/h2&gt;
&lt;p&gt;今天的分享主要侧重在框架的讲解，并不涉及太多代码细节，如果大家对实现细节感兴趣的话，可以直接查看 GitHub 上的代码，下方是 DeepFlow GitHub 的链接。&lt;/p&gt;
&lt;p&gt;GitHub地址：https://github.com/deepflowys/deepflow&lt;/p&gt;
&lt;h2 id=&#34;0x18-未来迭代的方向&#34;&gt;0x18: 未来迭代的方向&lt;/h2&gt;
&lt;p&gt;最后分享下未来 DeepFlow 关于日志的一个迭代方向。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-未来迭代的方向&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;未来迭代的方向&#34; srcset=&#34;
               /blog/autologging-for-request-log-and-flow-log/20220823630479bd045b9_hucc459657dbbdc55abd79f20f4e89d82d_98342_ebb8732d0b0c17f154694d752eb31776.webp 400w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479bd045b9_hucc459657dbbdc55abd79f20f4e89d82d_98342_3c9301d329411bfef97631a1d37a0585.webp 760w,
               /blog/autologging-for-request-log-and-flow-log/20220823630479bd045b9_hucc459657dbbdc55abd79f20f4e89d82d_98342_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/autologging-for-request-log-and-flow-log/20220823630479bd045b9_hucc459657dbbdc55abd79f20f4e89d82d_98342_ebb8732d0b0c17f154694d752eb31776.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      未来迭代的方向
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;目前 DeepFlow 在 Logging 方向上，有 AutoLogging 的能力，后面还会持续做日志集成，会接入 Promtail、Fluentd 等等的数据，并利用 AutoTagging 的能力，注入各种标签，更符合云原生设计理念。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
