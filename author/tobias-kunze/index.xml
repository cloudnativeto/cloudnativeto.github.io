<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Tobias Kunze | 云原生社区（中国）</title>
    <link>https://cloudnative.to/author/tobias-kunze/</link>
      <atom:link href="https://cloudnative.to/author/tobias-kunze/index.xml" rel="self" type="application/rss+xml" />
    <description>Tobias Kunze</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language>
    <image>
      <url>https://cloudnative.to/author/tobias-kunze/avatar_hu3801682002625739492.jpg</url>
      <title>Tobias Kunze</title>
      <link>https://cloudnative.to/author/tobias-kunze/</link>
    </image>
    
    <item>
      <title>服务网格的三个技术优势及其运维局限 - 第 2 部分</title>
      <link>https://cloudnative.to/blog/service-mesh-istio-limits-and-benefits-part-2/</link>
      <pubDate>Thu, 01 Aug 2019 10:42:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/service-mesh-istio-limits-and-benefits-part-2/</guid>
      <description>&lt;p&gt;本文为翻译文章，&lt;a href=&#34;https://glasnostic.com/blog/service-mesh-istio-limits-and-benefits-part-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;点击查看原文&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;编者按&#34;&gt;编者按&lt;/h2&gt;
&lt;p&gt;本文作者洞察全局，高屋建瓴，结合当前服务网格的形势，分析了服务网格普遍的局限性，以及从开发者角度讲述服务网格带来的三个有价值的好处：可观测性、流量控制和安全。本文的观点阐述详细而深刻，涵盖了服务网格的关键性技术与架构，以及许多备受瞩目与广泛探讨的话题。&lt;/p&gt;
&lt;h2 id=&#34;正文&#34;&gt;正文&lt;/h2&gt;
&lt;p&gt;欢迎来到关于服务网格的优势和运维局限的系列文章的第 2 部分。在&lt;a href=&#34;https://glasnostic.com/blog/service-mesh-istio-limits-and-benefits-part-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;第 1 部分&lt;/a&gt;中，我们了解了开发人员如何从服务网格为微服务架构提供附加的可观察性、流量控制和安全功能的能力中获益。在这篇文章中，我们将关注同样的三个维度，但不同于开发人员关心的问题，我们会从运维团队的角度深入研究服务网格的局限性。&lt;/p&gt;
&lt;h2 id=&#34;可观测性的限制&#34;&gt;可观测性的限制&lt;/h2&gt;
&lt;p&gt;可观察性始终是分布式系统工程师的首选。因此，服务网格尽其所能来满足这种需求就不足为奇了。然而，工程师期望的以及服务网格提供的可观察性并没有针对传统的运维行为，比如容量规划：它关注的是&lt;strong&gt;运行时调试&lt;/strong&gt;的开发活动。&lt;/p&gt;
&lt;p&gt;当然，运行时调试要求指标在请求的执行线程上下文中是“可解释的”。这与当今联合的，&lt;a href=&#34;https://glasnostic.com/blog/microservices-architecture-patterns-service-mesh-glossary#Organic-Architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;有组织发展的服务架构（Organic Architecture）&lt;/a&gt;是不一致的，它的度量标准是不可预测和不确定的。&lt;/p&gt;
&lt;p&gt;如果一个分布式系统的服务构建了一个应用，并且其设计在很长一段时间内保持静态，那么观测这个分布式系统是有意义的。这样的系统可以被基线化和合理化，收集到的结果指标是可以解释的——特别是当架构主要是同步模式的时候。&lt;/p&gt;
&lt;p&gt;但是，随着今天现代企业运行的那种联合的、有组织的架构的消失，可解释性也消失了。首先，基线——以及实践人员的“基线理解”——在一个有组织的架构世界中已经过时了。如果没有基线，对度量的最终解释可能会很有挑战。此外，服务的组合减少了单个开发团队的职责范围。联合的、有组织的服务环境是由并行的、在快速决策和学习周期中发展的自我管理团队创建的。换句话说，开发团队只负责少量的服务：他们的控制范围有限。因为没有必要跟踪到不属于团队的依赖，所以可观察性只在开发团队的控制范围内才有意义。在联合的、有组织的架构中，惟一的全局控制视野是负责“整个”服务场景的运维团队，而不仅仅是一组相关服务或应用——换句话说，是“任务控制”的运维团队。&lt;/p&gt;
&lt;p&gt;可观测性在尺度上也会变成数据密集型。随着联合的、有组织架构的增长，在遥测和追踪中收集的数据量呈指数级增长，而单个服务实例的重要性下降。换句话说，以运行时调试为目标的可观察性导致从业者收集的数据越来越多，而这些数据却越来越不重要。因此，收集到的指标几乎没有可用的。&lt;/p&gt;
&lt;p&gt;随着这些架构的发展，可观察性需要“在技术栈中向上移动”。与收集开发人员能够理解的指标不同，运维人员需要关注更高级别的 KPI，让他们能够实时检测和反应。这些 KPI 需要具有全局的意义。这也是服务网格提供的可观察性不足的地方。由于其固执己见的特性，服务网格往往在企业中被孤立地部署，特别是运行在 Kubernetes 上的环境中。另一方面，可操作的观察性需要高级的“黄金信号”指标，它可以跨裸机、虚拟机和容器部署以及跨多个区域和云工作。&lt;/p&gt;
&lt;p&gt;总之，服务网格为运行时调试提供了可观察性。这在开发人员的控制范围内是有价值的，但是指标需要是可以在请求执行的线程上下文中可解释的。然而，在当今联合的、有组织的服务环境中，缺乏基准指标和缩小的控制范围破坏了这种可解释性。&lt;/p&gt;
&lt;p&gt;运行时调试的可观察性也是数据密集型的，这导致以更高的成本收集更多的数据，但价值却更低。为了避免价值的螺旋下降，可观测性需要“向上移动栈”，收集更高级别的全局的&lt;strong&gt;黄金信号&lt;/strong&gt;，以使&lt;strong&gt;任务控制&lt;/strong&gt;运维团队能够实时检测并做出反应。服务网格提供的可观察性不符合这个目标，这不仅是因为它的目标是支持运行时调试，还因为黄金信号需要是全局的，而且服务网格过于武断和具有侵入性，不适合部署在每个地方。&lt;/p&gt;
&lt;h2 id=&#34;流量控制的限制&#34;&gt;流量控制的限制&lt;/h2&gt;
&lt;p&gt;服务网格的发展是为了解决如何将对服务的调用路由到最佳目标实例，例如可以最快地为请求提供服务的实例。这就是为什么服务网格是面向开发人员或“面向路由”的：它们服务于开发人员的视角，开发人员希望调用服务而不必处理复杂的远程服务调用。因此，服务网格被证明是不适合管理这样架构下的工作负载——涉及了数十个或上百个微服务之间的跨越开发团队、业务部门甚至是公司防火墙的交互，例如随着时间的推移，具有不断变化的服务到服务交互和依赖关系的组合服务架构会有机地发展。&lt;/p&gt;
&lt;p&gt;例如，在服务网格下要表达&lt;a href=&#34;https://glasnostic.com/blog/how-canary-deployments-work-1-kubernetes-istio-linkerd#figure-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;向前&lt;/em&gt;路由策略&lt;/a&gt;和向后的流量控制是比较简单的，而下游客户端如&lt;a href=&#34;https://glasnostic.com/blog/preventing-systemic-failure-backpressure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;施加 backpressure&lt;/a&gt;或&lt;a href=&#34;https://glasnostic.com/blog/preventing-systemic-failure-bulkheads&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;实现 bulkhead&lt;/a&gt;就要更加困难，即使也有可能实现。服务网格的数据平面基于源和目标规则去构建流量决策从理论上讲是可能的，开发人员定位像&lt;a href=&#34;https://glasnostic.com/blog/kubernetes-service-mesh-what-is-istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;这样的控制平面，让他们提供对任意的服务交互集的流量控制。&lt;/p&gt;
&lt;p&gt;这种将策略应用于任意服务交互集的能力的缺乏也使得策略的分层变得极其困难。例如，当一个&lt;a href=&#34;https://glasnostic.com/blog/preventing-systemic-failure-bulkheads&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bulkhead&lt;/a&gt;在两个可用性区域之间，但一个关键服务需要能够在需要是故障转移，这几乎不可能找到正确的服务网格规则的阈值，特别是自动扩展部署的情况下。&lt;/p&gt;
&lt;p&gt;然而，对于运维人员来说，最重要的问题是服务网格在 kubernetes 之外的有限的可部署性——这是他们“固执己见”的直接结果。修改部署和部署过程以正确的包括一个数据平面的 sidecar 通常是不可能的，将虚拟机添加到服务网格是&lt;a href=&#34;https://istio.io/docs/setup/kubernetes/additional-setup/mesh-expansion/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;最令人费解的&lt;/a&gt;，但仍然不允许运维人员捕捉内部虚拟机的流量。更糟的是，要将现有的非 Kubernetes 工作负载集成到基于 Kubernetes 的服务网格中，运维人员不仅需要调整应用程序代码，还需要根据 Kubernetes 网络进行部署。&lt;/p&gt;
&lt;p&gt;最后，通过 YAML 部署描述配置了当前服务网格实现的流量控制。部署描述是在版本控制中存储配置的一种很好的方法，因此可以用来重建定义良好的初始状态，但是它们不太适合运维团队在遇到困难时需要进行持续、实时的更改。&lt;/p&gt;
&lt;p&gt;总之，虽然服务网格提供的流量控制支持许多面向开发人员的控制机制，如目标规则和虚拟服务定义，它不支持面向无路由的操作模式如&lt;a href=&#34;https://glasnostic.com/blog/preventing-systemic-failure-backpressure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;backpressure&lt;/a&gt;或 bulkhead。面对架构更改，服务网格策略不可能预先分层，而且很难部署到 Kubernetes 之外。服务网格配置通常基于部署描述，当需要进行补救时，这些描述必然会妨碍到运维团队。&lt;/p&gt;
&lt;h2 id=&#34;安全限制&#34;&gt;安全限制&lt;/h2&gt;
&lt;p&gt;通过代理服务间调用，服务网格可以很好地提供面向开发人员的应用安全特性的核心集，比如身份验证、授权、账号、安全传输和服务标识。虽然为应用开发人员提供这些开箱即用的特性可以节省时间，但是使用 YAML 部署描述来配置这些特性往往比较困难且容易出错，这显然会降低他们的目标。&lt;/p&gt;
&lt;p&gt;从操作的角度来看，这些&lt;strong&gt;基于服务间调用&lt;/strong&gt;的安全特性最多只能提供有限的安全，对于运维团队所关心的系统安全问题（如影响可用性、拒绝服务攻击、入侵或分割违规）毫无帮助。&lt;/p&gt;
&lt;p&gt;由于服务网格的固执和侵入性，它们的应用安全特性在异构环境中会发生故障，除了 Kubernetes 之外，异构环境还包括裸机、虚拟机、PaaS、容器或 serverless 部署。类似的，当不是所有的服务都有 sidecar 时服务网格安全特性在 Kubernetes 环境中也会发生故障，在“服务器 sidecar”这个例子中，&lt;a href=&#34;https://istio.io/docs/concepts/performance-and-scalability/#latency-for-istio-hahahugoshortcode-s2-hbhb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;由于性能的原因&lt;/a&gt;只有目标服务有 sidecar 注入。&lt;/p&gt;
&lt;p&gt;面向平台、固执己见的服务网格与应用安全性相结合也有一个影响，那就是大多数的网格与其他安全解决方案不能很好地集成——这是运维团队非常关心的问题。Istio 能够使用替代的 CA 插件，外部工具可以使用 YAML 部署描述调用&lt;code&gt;kubectl&lt;/code&gt;来实现与安全性相关的策略，但是由于服务网格不支持策略分层，外部工具不可能正确和安全地实现这些策略。&lt;/p&gt;
&lt;p&gt;总之，服务网格提供了许多应用程序的安全特性，这些特性对开发人员很有价值，但对更具挑战性的运维安全问题贡献甚微。由于服务网格是自定义的平台，而不是与外部安全解决方案协作的开放工具，因此即使是由它们提供的应用程序安全性在异构环境中也会很快出现故障。&lt;/p&gt;
&lt;h2 id=&#34;操作需要&#34;&gt;操作需要&lt;/h2&gt;
&lt;p&gt;对于构建微服务应用的开发团队，服务网格提供了许多好处，可以抽象出分布式服务带来的复杂性。其中的一些好处如加密、“智能”路由和运行时可观察性帮助运维这样的应，但很快被证明太有限，随着应用程序的增长，服务会变得更加关联且业务采用联合的，有机发展的&lt;a href=&#34;https://glasnostic.com/blog/microservices-architecture-patterns-service-mesh-glossary&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;服务场景&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;运维团队需要控制的不仅仅是服务间调用。他们需要能够将&lt;a href=&#34;https://glasnostic.com/blog/microservices-architecture-patterns-service-mesh-glossary/#Operational-Patterns-and-Techniques&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;运维模式&lt;/a&gt;应用到任意一组交互中。它们还需要能够&lt;strong&gt;分层&lt;/strong&gt;的策略，以便能够在不影响彼此的情况下实现。运维团队需要能够实时控制服务环境，而不需要管理数百个 YAML 描述。要做到这一切，他们不需要武断的平台，而是需要与现有工具集成的、应用于整个服务场景的工具，而不影响任何部署。&lt;/p&gt;
&lt;p&gt;因此，如果服务网格技术，其核心是用于开发人员在 Kubernetes 之上创建复杂度有限的独立应用程序，而不是用于让运维团队确保整个异构和动态服务环境的正确运维，那么我们如何解决必要的运维问题呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解决方案 1：继续等待，直到服务网格解决运维关注的这些问题。对于我们这些将服务网格（特别是 Istio）视为每个分布式问题的一体化解决方案的人来说，最简单的解决方案就是等待服务网格支持这些问题。当然，这不大可能发生。服务网格是围绕开发人员的关注点 (如服务链接和更智能的实例路由) 而设计的，必须进行很大的更改才能支持运维模式，而这通常无法通过管理点对点连接来解决。&lt;/li&gt;
&lt;li&gt;解决方案 2：在这个问题上投入更多的工程。工程师的答案是，在这个问题上投入更多的工程。开发人员可以编写策略引擎、粘接代码来集成服务网格安全性和其他安全工具、数据聚合器来收集运维人员需要的高级指标等等。显然，这将耗费更多成本，而且不太可能在短期内令人满意。&lt;/li&gt;
&lt;li&gt;解决方案 3：采用云流量控制器。最好的选择是将服务网格留给开发团队，让运维团队采用云流量控制器。这样，运维团队可以检测到&lt;a href=&#34;https://glasnostic.com/blog/microservices-architecture-patterns-service-mesh-glossary/#Emergent-Behaviors&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;复杂紧急的行为&lt;/a&gt;，并实时纠正，创造他们所需要的自动化来有效的实现需要的运维模式，保证架构的可控。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Glasnostic 就是这样一个云流量控制器。&lt;/p&gt;
&lt;div class=&#34;video-wrapper&#34; style=&#34;position: relative;padding-bottom: 56.25%;padding-top: 25px;height: 0;&#34;&gt;
&lt;iframe src=&#34;https://player.vimeo.com/video/343154979?title=0&amp;amp;profile=0&amp;amp;byline=0&amp;amp;dnt=1&amp;amp;autoplay=1&amp;amp;muted=1&amp;amp;loop=1&#34; allow=&#34;autoplay; fullscreen&#34; allowfullscreen=&#34;&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;图 1: 是一个云流量控制器，允许运维和安全团队控制微服务应用之间的复杂交互和行为。&lt;/p&gt;
&lt;p&gt;Glasnostic 是一个用于服务环境的控制平面，它帮助运维和安全团队大规模地控制关联的微服务应用之间的复杂交互和行为。这与服务网格相反，服务网格管理应用程序中的服务到服务连接。Glasnostic 是一个独立的解决方案，而不是另一个平台。它不需要 sidecar 或代理，并且能干净地集成到任何现有环境中。&lt;/p&gt;
&lt;p&gt;通过获得对服务交互的控制，团队可以控制紧急行为、防止级联故障和避免安全漏洞。&lt;/p&gt;
&lt;p&gt;Glasnostic 是借鉴了成功的可以有机发展的架构之后才被构建的，它反对预先进行严格的设计。它使用一种独特的基于网络的方法来为运维人员提供他们需要的可观察性和控制能力，以检测和纠正服务场景中的紧急行为。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>服务网格的三个技术优势及其运维局限 - 第 1 部分</title>
      <link>https://cloudnative.to/blog/service-mesh-istio-limits-and-benefits-part-1/</link>
      <pubDate>Wed, 31 Jul 2019 00:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/service-mesh-istio-limits-and-benefits-part-1/</guid>
      <description>&lt;p&gt;本文为翻译文章，&lt;a href=&#34;https://glasnostic.com/blog/service-mesh-istio-limits-and-benefits-part-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;点击查看原文&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;编者按&#34;&gt;编者按&lt;/h2&gt;
&lt;p&gt;本文作者洞察全局，高屋建瓴，结合当前服务网格的形势，分析了服务网格普遍的局限性，以及从开发者角度讲述服务网格带来的三个有价值的好处：可观测性、流量控制和安全。本文的观点阐述详细而深刻，涵盖了服务网格的关键性技术与架构，以及许多备受瞩目与广泛探讨的话题。&lt;/p&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;今天的应用程序架构师实际上已经放弃了单一的设计，转而改用云原生微服务架构，这样他们就可以充分利用云的灵活性更快地响应不断变化的业务需求，加快开发人员的敏捷迭代。当然，采用微服务架构也有成本。由于应用程序的个数比单个应用程序多得多，所以微服务架构需要更多的管理、监控和安全。像&lt;a href=&#34;https://glasnostic.com/blog/comparing-service-meshes-linkerd-vs-istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 和 Linkerd&lt;/a&gt;这样的服务网格技术近年来已经相继出现，它们承诺将使微服务的管理、监控和安全的实现更容易。除了&lt;a href=&#34;https://glasnostic.com/blog/what-is-a-service-mesh-istio-linkerd-envoy-consul&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;管理服务间连接&lt;/a&gt;最基本的好处，即将来源服务的请求路由到最佳目的地服务实例，服务网格还为开发人员提供三个有价值的关键领域的好处：可观测性、流量控制和安全。&lt;/p&gt;
&lt;p&gt;在本系列的第 1 部分中，我们将探讨开发人员获得的这些好处。在&lt;a href=&#34;https://glasnostic.com/blog/service-mesh-istio-limits-and-benefits-part-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;第 2 部分&lt;/a&gt;中，我们将从运维的角度来研究它们的局限性。&lt;/p&gt;
&lt;h2 id=&#34;什么是服务网格&#34;&gt;什么是服务网格？&lt;/h2&gt;
&lt;p&gt;服务网格是一个专用的基础设施层，它的目标是“&lt;a href=&#34;https://buoyant.io/2017/04/25/whats-a-service-mesh-and-why-do-i-need-one/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在微服务架构中实现可靠、快速和安全的服务间调用&lt;/a&gt;”。它不是一个“服务的网格”，而是一个服务可以插入其中的“代理的网格”，以实现网络的完全抽象化。在典型的服务网格中，这些代理作为 sidecar(边车) 注入到每个服务部署中。服务不直接通过网络调用服务，而是调用它们的本地 sidecar 代理，后者代表服务管理请求，从而封装了服务间调用的复杂性。相互连接的 sidecar 代理实现了所谓的“数据平面”。这与用于配置代理和收集指标的服务网格组件形成对比，这些组件称为服务网格控制平面。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-service-mesh&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;service-mesh&#34; srcset=&#34;
               /blog/service-mesh-istio-limits-and-benefits-part-1/service-mesh_hu714860859063476012.webp 400w,
               /blog/service-mesh-istio-limits-and-benefits-part-1/service-mesh_hu4069128141641429837.webp 760w,
               /blog/service-mesh-istio-limits-and-benefits-part-1/service-mesh_hu3990395761699145091.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/service-mesh-istio-limits-and-benefits-part-1/service-mesh_hu714860859063476012.webp&#34;
               width=&#34;760&#34;
               height=&#34;550&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      service-mesh
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;一个典型的服务网格架构：数据平面代理部署为 sidecar，控制平面单独部署。&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;简而言之，服务网格旨在解决开发人员在与远程端点通信时面临的许多挑战。服务网格对&lt;a href=&#34;https://glasnostic.com/blog/should-i-use-a-service-mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;运行在容器编排器的“greenfield”应用&lt;/a&gt;特别有用，例如 Kubernetes.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-popular-service-meshes&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;popular-service-meshes&#34; srcset=&#34;
               /blog/service-mesh-istio-limits-and-benefits-part-1/popular-service-meshes_hu4939679353903546981.webp 400w,
               /blog/service-mesh-istio-limits-and-benefits-part-1/popular-service-meshes_hu17422299278924771765.webp 760w,
               /blog/service-mesh-istio-limits-and-benefits-part-1/popular-service-meshes_hu13946760528512864255.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/service-mesh-istio-limits-and-benefits-part-1/popular-service-meshes_hu4939679353903546981.webp&#34;
               width=&#34;760&#34;
               height=&#34;323&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      popular-service-meshes
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;服务网格形势&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;目前有哪些服务网格可用&#34;&gt;目前有哪些服务网格可用？&lt;/h2&gt;
&lt;p&gt;目前，最受开发者欢迎的服务网格是&lt;a href=&#34;https://glasnostic.com/blog/kubernetes-service-mesh-what-is-istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt;项目，该项目最初由谷歌、IBM 和 Lyft 开发。虽然不像 Istio 那么流行，Buoyant 公司的&lt;a href=&#34;https://glasnostic.com/blog/an-introduction-to-what-is-linkerd-service-mesh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Linkerd&lt;/a&gt; 才是“原始的”服务网格，今天仍然被广泛使用着。几个部署了大型微服务的网络级公司已经开发了自己的基于&lt;a href=&#34;https://www.envoyproxy.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Envoy&lt;/a&gt;代理的内部服务网格，巧合的是，Istio 也是基于该代理构建的。其他的服务网格也出现了，包括 F5 的 AspenMesh、HashiCorp 的 Consul Connect、Kong、AWS 的 AppMesh 和微软发起的一项标准化各种服务网格接口的倡议 SMI。要了解微服务生态系统的当前状态，请查看我们的文章&lt;a href=&#34;https://glasnostic.com/blog/the-2019-microservices-ecosystem&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“2019 微服务生态系统”&lt;/a&gt;获取更详细的信息。&lt;/p&gt;
&lt;h2 id=&#34;服务网格的局限性&#34;&gt;服务网格的局限性&lt;/h2&gt;
&lt;p&gt;像服务网格这样雄心勃勃的技术，其目标是解决许多或者大部分的由于运行一个庞大的微服务架构而导致的问题，也难免受到批评。有些批评集中在服务网格可能会带来以下不良影响：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;增加的复杂性&lt;/strong&gt;: 在一个已经很复杂的环境中引入代理、sidecar 和其他组件会极大地增加开发和运维的复杂性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要的专业知识&lt;/strong&gt;: 在容器编排器 (如 Kubernetes) 之上添加 Istio 之类的服务网格通常需要运维人员成为这两种技术的专家。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;延迟&lt;/strong&gt;: 服务网格是一种入侵的、复杂的技术，它能向服务架构中添加&lt;a href=&#34;https://istio.io/docs/concepts/performance-and-scalability/#latency-for-istio-hahahugoshortcode-s2-hbhb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;显著的延迟&lt;/a&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;平台的依赖性&lt;/strong&gt;: 服务网格的侵入性迫使开发人员和运维人员适应一个高度自治的平台，并遵守其规则。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;尽管存在这些限制，但在正确的环境中，服务网格显然不是没有它们的优点，尤其是运行在 Kubernetes 上的微服务。尽管运营团队仍保持谨慎，但开发人员已经开始迎头赶上，他们被其看似全面的可观察性、流量控制和安全特性所吸引。在接下来的几节中，我们将详细探讨这些优点。&lt;/p&gt;
&lt;h2 id=&#34;可观察性&#34;&gt;可观察性&lt;/h2&gt;
&lt;p&gt;将应用程序分解为多个微服务并不会自动将其转换为独立服务的网络。应用程序仍然像以前一样作为一个单一的、独立的应用程序—它只是变成了“分布式的”。它的各个微服务通常共享相同的代码库，并且是单个架构蓝图的一部分。它们更像是父应用程序的“组件”，而不是跨多个应用程序共享的服务。&lt;/p&gt;
&lt;p&gt;因为这样的微服务程序仍然作为一个独立的应用程序，而不是作为独立服务的网络，所以开发团队对它们进行故障排查，就像他们对一个整体进行故障排查一样。当然，调试这样的微服务程序变得更加困难，因为应用程序组件现在是分布式的。这正是为什么工程师非常希望能够跨远程服务追踪请求以进行调试的原因。与这种分布式调试相关的术语通常被称为“可观察性”。&lt;/p&gt;
&lt;p&gt;因为服务网格是一个专用的基础设施层，所有的服务间通信都要通过它，所以它在技术堆栈中处于独特的位置，以便在服务调用级别上提供统一的遥测指标。这意味着，无论好坏，服务都被监控为“黑匣子”。服务网格捕获诸如来源、目的地、协议、URL、状态码、延迟、持续时间等线路数据。这本质上等同于 web 服务器日志可以提供的数据，但是当然，服务网格可以为所有服务捕获这些数据，而不仅仅是单个服务的 web 层。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-grafana-istio&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;grafana-istio&#34; srcset=&#34;
               /blog/service-mesh-istio-limits-and-benefits-part-1/grafana-istio_hu10784512641042295929.webp 400w,
               /blog/service-mesh-istio-limits-and-benefits-part-1/grafana-istio_hu11832858646946320276.webp 760w,
               /blog/service-mesh-istio-limits-and-benefits-part-1/grafana-istio_hu9958736476925567475.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/service-mesh-istio-limits-and-benefits-part-1/grafana-istio_hu10784512641042295929.webp&#34;
               width=&#34;760&#34;
               height=&#34;393&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      grafana-istio
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;由 Grafana 可视化的 Istio 遥测数据 (图片来源：istio.io).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;一旦捕获，度量 (metrics) 和日志 (logs) 将由服务网格的控制平面收集并传递给后端所选择的监控工具。对于严重依赖开源技术的公司来说，Prometheus 和 Grafana 分别是存储和可视化的热门选择。&lt;/p&gt;
&lt;p&gt;除了度量服务间的调用之外，一些服务网格还支持请求追踪。通过有效的追踪，工程师能够排除各种问题，如排序问题、服务调用树异常和特定请求的问题等。由于使用了&lt;a href=&#34;https://opentracing.io/docs/overview/spans/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;span 标识符&lt;/a&gt;和&lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/configuration/http_conn_man/headers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;转发的上下文标头&lt;/a&gt;，服务网格可以进行追踪。当然，要使追踪工作正常，需要修改每个服务，以便在输入时读取追踪头信息，将它们传递给所有相关的执行线程，然后将它们添加到对其他服务的每个请求调用中。&lt;/p&gt;
&lt;p&gt;需要指出的是，收集数据仅仅是解决微服务应用程序中可观察性问题的一部分。收集和存储度量需要额外能力的机制的补充，借此来分析数据，然后通过作用于警报、实例自动伸缩或者应用程序的一个操作模式&lt;a href=&#34;https://glasnostic.com/blog/preventing-systemic-failure-circuit-breaking-part-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;断路器&lt;/a&gt;来发挥这些数据的作用。&lt;/p&gt;
&lt;h2 id=&#34;流量控制&#34;&gt;流量控制&lt;/h2&gt;
&lt;p&gt;当涉及到满足服务级别目标 (如延迟和正常运行时间) 时，管理服务之间通信的能力是至关重要的。这是因为它允许运营团队实现像&lt;a href=&#34;https://glasnostic.com/blog/preventing-systemic-failure-circuit-breaking-part-1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;circuit breaking&lt;/a&gt;或&lt;a href=&#34;https://glasnostic.com/blog/preventing-systemic-failure-backpressure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;backpressure&lt;/a&gt;这样的操作模式，以补偿行为不佳的服务。&lt;/p&gt;
&lt;p&gt;服务网格可以提供这种类型的流量控制。因为它们的主要功能是管理服务间通信，所以它们能够相当容易地提供这些特性。然而，由于它们的设计目的是有效地将来源请求调用连接到其最优目标服务实例，所以这些流量控制特性是&lt;em&gt;面向目的地的&lt;/em&gt;。换句话说，服务网格适合在一些目的地实例之间平衡&lt;em&gt;单个请求调用&lt;/em&gt;，而不合适来控制来源为多个而目的地为一个的流量，或控制整个&lt;a href=&#34;https://glasnostic.com/blog/microservices-architecture-patterns-service-mesh-glossary#Service-Landscape&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;服务架构&lt;/a&gt;的流量。&lt;/p&gt;
&lt;p&gt;服务网格通过其控制平面提供对服务间调用的控制，控制平面最终实现组成其数据平面的代理的配置。尽管具体功能可能因服务网格的不同而有所不同，但大多数支持智能配置，即延迟感知负载均衡 (也称为“智能路由”) 和基于请求属性的路由规则。由于服务网格控制从第 4 层扩展到第 5 层及以上，有些还为开发团队提供了实现弹性模式 (如重试、超时和截止时间) 的能力，以及更高级的模式 (如断路、金丝雀发布和 A/B 发布)。&lt;/p&gt;
&lt;p&gt;例如，使用超时，开发人员可以限制微服务等待另一个服务完成请求所需的时间。如果这被证明是一个太粗糙的阈值，超时作为替代可以用来启动断路器。当断路器以这种方式“跳闸”时，它将保持“打开”一段时间，直到服务网格再次认为该服务可用为止。这样，下游客户端就不会受到上游服务过于缓慢的影响，反过来，服务也不会因积压的请求而过载。或者，如果特定客户端的请求行为从服务级别来看，威胁到向相同共享服务发出请求的其他客户端，那么开发人员可以限制高容量客户端的速率，这样其他客户端就不会被淹没。最后，服务网格可以通过强制执行配额来帮助控制流量。例如，运维人员可以根据请求向客户端收费，或者希望在给定的时间范围内限制客户端请求。&lt;/p&gt;
&lt;h2 id=&#34;安全&#34;&gt;安全&lt;/h2&gt;
&lt;p&gt;在某种程度上，单一应用程序受其单地址空间的保护。然而，一旦单一应用程序被分解为多个微服务，网络就会成为一个重要的攻击面。更多的服务意味着更多的网络流量，这对黑客来说意味着更多的机会来攻击信息流。这就是为什么服务要网格化，因为网格提供了保护网络调用的能力 (和基础设施)。&lt;/p&gt;
&lt;p&gt;服务网格的安全相关的好处体现在以下三个核心领域：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;服务的认证。&lt;/li&gt;
&lt;li&gt;服务间通讯的加密。&lt;/li&gt;
&lt;li&gt;安全相关策略的强制执行。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例如，Istio 为开发人员提供了一个证书授权来管理密钥和证书。使用 Istio，您可以为每个服务生成证书，并透明地管理这些证书的分发、轮换和撤销。有了这些功能，服务可以彼此进行身份验证并实现适当的访问控制。通常，这以白名单和黑名单的形式出现，因此服务网格知道是否接受传入的请求。在加密方面，服务网格可以使用相互传输层安全协议 (mTLS) 锁定数据平面通信，从而使服务到服务的通信更加安全。最后，一些服务网格能够执行适用于特定 pod、特定命名空间或特定服务的各种安全策略。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;服务网格是一种新颖但有益的技术，对于那些渴望解决由于运行容器化的微服务架构而导致许多问题的开发人员来说是更是如此。从根本上说，服务网格解决了服务发现和请求路由到最佳服务实例的问题。然而，除了“链接”服务之外，服务网格还可以为开发人员提供有价值的可观察性、流量控制和安全性等好处。&lt;/p&gt;
&lt;p&gt;服务网格也是有局限性的。首先，它们是复杂且高度自治的技术，这在很大程度上限制了它们对 Kubernetes 上“greenfield”应用程序的适用性。它们也可能带来很大的延迟，这限制了它们能够支持的应用程序的规模和复杂性。因此，它们最适合运行在容器调度器 (如 Kubernetes) 上的相当小的、容器化的微服务应用程序。&lt;/p&gt;
&lt;p&gt;然而，服务网格在可观测性、流量控制和安全领域提供的价值也是有限的。可观察性实际上只在自包含的分布式应用程序中起作用，这些应用程序由一个公共蓝图管理 (并托管在一个 git 存储库中)。流量控制受到面向路由设计的限制，这使得在任意端点集之间执行策略实际上是不可能的。最后，服务网格的孤立性限制了安全性。自动加密是迈向零信任环境的一大步，但如果没有异构架构的大规模支持，最终的价值将是有限的。&lt;/p&gt;
&lt;p&gt;在本系列的&lt;a href=&#34;https://glasnostic.com/blog/service-mesh-istio-limits-and-benefits-part2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;第 2 部分&lt;/a&gt;中，我们将从运维的角度研究这三个有价值的领域的局限性：可观察性、流量控制和安全性。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
