<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>程威 | 云原生社区（中国）</title>
    <link>https://cloudnativecn.com/author/%E7%A8%8B%E5%A8%81/</link>
      <atom:link href="https://cloudnativecn.com/author/%E7%A8%8B%E5%A8%81/index.xml" rel="self" type="application/rss+xml" />
    <description>程威</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Fri, 09 Apr 2021 02:40:00 +0800</lastBuildDate>
    <image>
      <url>https://cloudnativecn.com/author/%E7%A8%8B%E5%A8%81/avatar_hu16040087328221071518.jpg</url>
      <title>程威</title>
      <link>https://cloudnativecn.com/author/%E7%A8%8B%E5%A8%81/</link>
    </image>
    
    <item>
      <title>同程旅行大数据集群在 Kubernetes 上的服务化实践</title>
      <link>https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/</link>
      <pubDate>Fri, 09 Apr 2021 02:40:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/</guid>
      <description>&lt;p&gt;本文将向大家介绍同程旅行大数据集群在 Kubernetes 上服务化建设的一些实践和经验。&lt;/p&gt;
&lt;h2 id=&#34;前言&#34;&gt;前言&lt;/h2&gt;
&lt;p&gt;同程旅行大数据集群从 2017 年开始容器化改造，经历了自研调度 docker 容器，到现在的&lt;code&gt;云舱&lt;/code&gt;平台，采用 &lt;code&gt;Kubernetes&lt;/code&gt; 调度编排工具管理大数据集群服务。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/k8s%E6%BC%94%E8%BF%9B_hu12552818705488387887.webp 400w,
               /blog/2021-tongchenglvxing-shared/k8s%E6%BC%94%E8%BF%9B_hu6650528628609366321.webp 760w,
               /blog/2021-tongchenglvxing-shared/k8s%E6%BC%94%E8%BF%9B_hu17408789883869721509.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/k8s%E6%BC%94%E8%BF%9B_hu12552818705488387887.webp&#34;
               width=&#34;760&#34;
               height=&#34;181&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在这个过程中遇到很多问题和难点，本文会向大家介绍上云过程中总结的经验和教训。&lt;/p&gt;
&lt;p&gt;今天的议题主要分下面几点来阐述：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么要将大数据集群服务搬到 Kubernetes 上&lt;/li&gt;
&lt;li&gt;在上云的过程遇到哪些痛点&lt;/li&gt;
&lt;li&gt;大数据服务上云攻略&lt;/li&gt;
&lt;li&gt;现状和未来发展&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;集群即服务的理念&#34;&gt;集群即服务的理念&lt;/h2&gt;
&lt;p&gt;部门内部很早就提出集群即服务的理念，作为基础组件研发，希望从产品的角度来看待组件或者集群，让业务研发能直接触达底层集群，可以包含节点、日志、监控等功能，让集群使用更简单。&lt;/p&gt;
&lt;h3 id=&#34;推行小集群化&#34;&gt;推行小集群化&lt;/h3&gt;
&lt;p&gt;以前组件研发部署一个组件集群，这个集群会陆续承接一些业务，时常会遇到 A 业务影响 B 业务，集群负责人会开始考虑拆分，搭建出一个新集群将消耗资源的业务拆分出去。这种是以人工介入的方式去评估业务体量并分配资源。&lt;/p&gt;
&lt;p&gt;现在部门开始推行小集群模式，每个业务研发组都可以申请一个或者多个集群，在物理层面做到资源隔离，互不影响，不会因为 A 业务的流量上升而影响其他业务。&lt;/p&gt;
&lt;h3 id=&#34;自动化运维建设&#34;&gt;自动化运维建设&lt;/h3&gt;
&lt;p&gt;小集群化会导致集群数量成倍的上升，如果不做自动化运维，人力会远远跟不上业务增长，到那时组件研发会淹没在救火和运维的海洋。&lt;/p&gt;
&lt;p&gt;所以需要构建一个集群全流程自动化平台。这里面包含服务申请，服务部署，服务运维等功能。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/devops%E5%8A%9F%E8%83%BD_hu4631385847257142812.webp 400w,
               /blog/2021-tongchenglvxing-shared/devops%E5%8A%9F%E8%83%BD_hu18113215885623029458.webp 760w,
               /blog/2021-tongchenglvxing-shared/devops%E5%8A%9F%E8%83%BD_hu10831752773962182212.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/devops%E5%8A%9F%E8%83%BD_hu4631385847257142812.webp&#34;
               width=&#34;760&#34;
               height=&#34;83&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;如何利用-kubernetes-利器&#34;&gt;如何利用 Kubernetes 利器&lt;/h3&gt;
&lt;p&gt;起初自研编排工具去调度容器，但是实现的东西太多，在人力有限的情况下，认为这条路不可行。&lt;/p&gt;
&lt;p&gt;2019 年开始采用 &lt;code&gt;Kubernetes&lt;/code&gt; 调度编排容器，先后采取过用&lt;code&gt;Helm&lt;/code&gt; 工具编写模板部署组件，用&lt;code&gt;Operator&lt;/code&gt;的方式管理服务，用 Statefulset/Deployment 部署大数据集群。这些方式最后都被放弃。Helm 只是解决了部署的问题，想要基于 &lt;code&gt;Helm&lt;/code&gt; 做平台精细化运维比较麻烦。Operator 的理念是针对某个组件做自定义 CRD，大数据服务有十几种组件，为每个组件专门定制 Operator，运维和开发成本过大，基于此还要解决 Operator 和平台层的交互逻辑，这个也不适合同程的人力配比。&lt;code&gt;Statefulset&lt;/code&gt;和&lt;code&gt;Deployment&lt;/code&gt; 没法做到精细化运维，比如业务提出关闭某个指定的点，当业务逻辑和底层运维逻辑耦合在一起的时候，已经封装好的 Workload 并不能拿来即用。&lt;/p&gt;
&lt;p&gt;由于是大数据生态，同程选择采用&lt;code&gt;Java Client&lt;/code&gt; 和 &lt;code&gt;Kubernetes&lt;/code&gt; 进行交互，在&lt;code&gt;Kuberentes&lt;/code&gt; 上自研 &lt;code&gt;云舱&lt;/code&gt; 调度器，将运维侧业务逻辑和平台交互代码放在一起，构建了一套适合自己的大数据服务自动化运维框架，当前覆盖了几乎所有的大数据服务，计算组件有 Hive、Presto、Yarn，存储组件有 HDFS、ClickHouse、Kafka、Kudu 等。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B_hu13103540012293217375.webp 400w,
               /blog/2021-tongchenglvxing-shared/%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B_hu11742735412214539169.webp 760w,
               /blog/2021-tongchenglvxing-shared/%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B_hu518306757647298313.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/%E4%BA%A4%E4%BA%92%E6%B5%81%E7%A8%8B_hu13103540012293217375.webp&#34;
               width=&#34;760&#34;
               height=&#34;240&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;上云过程遇到了哪些痛点&#34;&gt;上云过程遇到了哪些痛点&lt;/h2&gt;
&lt;h3 id=&#34;kubernetes-环境问题&#34;&gt;Kubernetes 环境问题&lt;/h3&gt;
&lt;p&gt;由于大数据组件有很多是分布式存储系统，组件本身会要求客户端和服务端能够网络互通，端到端的建立连接。这就需要&lt;code&gt;Kubernetes&lt;/code&gt;容器网络要和外部物理网络打通，当然也可以采用&lt;code&gt;Proxy&lt;/code&gt;层来屏蔽底层存储。同程大数据选择构建 &lt;code&gt;Underlay&lt;/code&gt; 的容器网络，做到 IP 保持，容器 IP 提前分配，IP 自动回收等功能。&lt;/p&gt;
&lt;p&gt;将&lt;code&gt;Service&lt;/code&gt;层网络和公司四层负载 &lt;code&gt;TVS&lt;/code&gt; 服务做到很好的集成，利用 Endpoints 和 Service 事件监听来保证负载数据的一致性。由于网络环境的限制，一个机房没有办法只搭建一个 Kuberntes 集群，需要支持一个应用跨多&lt;code&gt;Kubernetes&lt;/code&gt;集群部署，负载服务要支持跨多个&lt;code&gt;Kubernetes&lt;/code&gt;集群的应用负载。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DNS&lt;/code&gt; 层采用子域的方式做到 Kubernetes 内部&lt;code&gt;CoreDns&lt;/code&gt; 和公司&lt;code&gt;DNS&lt;/code&gt;服务器数据同步，保证一致性，保证内外部域名通信一致。由于一些组件迁移的需求，需要提供在容器拉起来之前预先配置&lt;code&gt;DNS&lt;/code&gt;和&lt;code&gt;IP&lt;/code&gt;映射的功能，所以只好根据已知的&lt;code&gt;Pod&lt;/code&gt;标识，提前分配 IP。&lt;/p&gt;
&lt;h3 id=&#34;基于-pod-的方式管理容器&#34;&gt;基于 Pod 的方式管理容器&lt;/h3&gt;
&lt;p&gt;刚开始的时候采用&lt;code&gt;Statefulset&lt;/code&gt;来部署一些服务，一些开源的 Operator 也是基于&lt;code&gt;STS&lt;/code&gt;管理服务，比如我正在持续贡献的 &lt;code&gt;TiDB Operator&lt;/code&gt; 、&lt;code&gt;Prometheus Operator&lt;/code&gt;。虽然可以复用已有 Workload 的功能，但是当场景复杂，这么做反而会缝缝补补。大数据组件就是这样一个复杂的场景，所以决定采用纯&lt;code&gt;Pod&lt;/code&gt;管理容器，基于 Pod 去组装成 &lt;code&gt;Group&lt;/code&gt;。比如 HDFS 组件，会拆分成 &lt;code&gt;namenode&lt;/code&gt; 、&lt;code&gt;journalnode&lt;/code&gt;、&lt;code&gt;datanode&lt;/code&gt; 这三个&lt;code&gt;Group&lt;/code&gt;，每个&lt;code&gt;Group&lt;/code&gt;可以理解为是同一种节点类型的容器。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;%e5%ae%b9%e5%99%a8%e4%be%8b%e5%ad%90&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;pod-配置有状态&#34;&gt;Pod 配置有状态&lt;/h3&gt;
&lt;p&gt;存储组件有个明显的特性就是配置文件中会有一个唯一标识，比如&lt;code&gt;Zookeeper&lt;/code&gt;的 &lt;code&gt;myid&lt;/code&gt; , &lt;code&gt;Kafka&lt;/code&gt; 的 &lt;code&gt;broker id&lt;/code&gt;。将老集群逐步迁移到&lt;code&gt;Kubernetes&lt;/code&gt;上的时候，这些配置项需要自定义且持久化。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/%E9%85%8D%E7%BD%AE%E7%8A%B6%E6%80%81%E9%A1%B5%E9%9D%A2_hu8999205835617487072.webp 400w,
               /blog/2021-tongchenglvxing-shared/%E9%85%8D%E7%BD%AE%E7%8A%B6%E6%80%81%E9%A1%B5%E9%9D%A2_hu18034151470273815691.webp 760w,
               /blog/2021-tongchenglvxing-shared/%E9%85%8D%E7%BD%AE%E7%8A%B6%E6%80%81%E9%A1%B5%E9%9D%A2_hu13236965954040177923.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/%E9%85%8D%E7%BD%AE%E7%8A%B6%E6%80%81%E9%A1%B5%E9%9D%A2_hu8999205835617487072.webp&#34;
               width=&#34;760&#34;
               height=&#34;417&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;如果组件本身的配置文件格式比较固定，会做成模板化，将特定的配置项抽出来提供给组件研发配置，通过环境变量的方式注入到容器中。对于自定义特别强的组件，会基于&lt;code&gt;ConfigMap&lt;/code&gt;做配置的版本控制，让组件研发可以很方便的填写配置并推送配置，&lt;code&gt;ClickHouse&lt;/code&gt; 就是非常自定义配置的组件。&lt;/p&gt;
&lt;h3 id=&#34;以虚拟机的方式启动容器&#34;&gt;以虚拟机的方式启动容器&lt;/h3&gt;
&lt;p&gt;用&lt;code&gt;Kubernetes&lt;/code&gt;部署有状态服务的时候，由于配置错误会导致容器反复&lt;code&gt;crash&lt;/code&gt;,这个时候组件研发只希望快速进入现场排查问题，所以针对存储类组件均采用&lt;code&gt;tail -F&lt;/code&gt;的方式启动容器，让服务进程作为后台进程启动，配置完善的健康检查，快速发现节点的不健康性。&lt;/p&gt;
&lt;p&gt;这种方式虽然违反了&lt;code&gt;Kubernetes&lt;/code&gt;的设计原则，但是易用性会显著提升。在部署 Yarn 组件的时候，由于&lt;code&gt;tail -F&lt;/code&gt;命令为主进程，导致大量僵尸进程，最后改用&lt;code&gt;bash&lt;/code&gt;命令启动。&lt;/p&gt;
&lt;h3 id=&#34;资源异构问题和多盘挂载问题&#34;&gt;资源异构问题和多盘挂载问题&lt;/h3&gt;
&lt;p&gt;在部署 Yarn 组件过程中，由于机器规格的问题，导致同一个应用节点之间的资源配置不一样，我们设计采用划分资源池，将相同规格的机器分为一个资源池，一个应用根据资源池的配置来调整合适的资源。&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;Kubernetes&lt;/code&gt;中使用本地盘，一般会推荐&lt;code&gt;localpv&lt;/code&gt;的方式，大数据某些组件会采用多盘写入的方式部署，&lt;code&gt;local pv&lt;/code&gt;的方式并不能解决这个问题。同程大数据选择采用&lt;code&gt;hostpath&lt;/code&gt;+&lt;code&gt;nodeselector&lt;/code&gt;的方式来做到多盘绑定且节点不漂移。在提交给&lt;code&gt;Kubernetes Scheduler&lt;/code&gt;之前，会在&lt;code&gt;云舱Scheduler&lt;/code&gt;基于资源池和节点信息对容器提前做一层调度。
起初准备用&lt;code&gt;hostpath&lt;/code&gt;+&lt;code&gt;nodename&lt;/code&gt;的方式来做到节点不漂移，但是&lt;code&gt;nodename&lt;/code&gt; 会跳过 Scheduler update 步骤，并不会进行 &lt;code&gt;bind&lt;/code&gt; pvc 等步骤。详情可以参考 &lt;a href=&#34;https://github.com/kubernetes/kubernetes/issues/93145&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issue 93145&lt;/a&gt;。&lt;/p&gt;
&lt;h3 id=&#34;dns-问题&#34;&gt;DNS 问题&lt;/h3&gt;
&lt;p&gt;大数据里面很多组件节点都采用 &lt;code&gt;hostname&lt;/code&gt; 作为节点标识，比如&lt;code&gt;NodeManager&lt;/code&gt;采用&lt;code&gt;hostname&lt;/code&gt;注册，&lt;code&gt;Hbase&lt;/code&gt;组件要支持域名反解，&lt;code&gt;Kudu&lt;/code&gt;的 master 节点依赖自身的域名提前通信。这些都违背了 Kubernetes 的设计理念，&lt;code&gt;Kubernetes&lt;/code&gt; 创建容器，CNI 分配得到 IP，进程启动 OK，容器变成 Ready 状态，Pod 的 Service 域名才能通信。&lt;/p&gt;
&lt;p&gt;同程大数据选择用&lt;code&gt;Host&lt;/code&gt;网络部署大部分的存储组件，沿用宿主机网络，除了&lt;code&gt;Kubernetes&lt;/code&gt;集群子域外再创建一个子域用于组件本身标识，这样组件迁移会很方便，也不有网络损耗的烦恼。但是要做好宿主机端口的管理划分。&lt;/p&gt;
&lt;h3 id=&#34;调度问题&#34;&gt;调度问题&lt;/h3&gt;
&lt;p&gt;为了提升资源利用率，&lt;code&gt;云舱&lt;/code&gt; 平台会有很多分时段的部署任务和资源销毁任务。比如某个&lt;code&gt;Yarn&lt;/code&gt;集群，晚上的时候，对可以混部的资源池打上标签，在晚高峰的时候尽可能的扩容&lt;code&gt;NodeManager&lt;/code&gt;。这个类似于&lt;code&gt;HPA&lt;/code&gt;，由于业务逻辑的复杂性，同程基于自研 &lt;code&gt;云舱Scheduler&lt;/code&gt; 做到这一点。&lt;/p&gt;
&lt;h2 id=&#34;大数据服务基于-kubernetes-的架构体系&#34;&gt;大数据服务基于 Kubernetes 的架构体系&lt;/h2&gt;
&lt;p&gt;从&lt;code&gt;2019&lt;/code&gt;年开始转向 &lt;code&gt;Kubernetes&lt;/code&gt; 到现在，同程已经建立了一套成熟的大数据服务&lt;code&gt;PAAS&lt;/code&gt;体系。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/%E6%9E%B6%E6%9E%84_hu7804573487903145154.webp 400w,
               /blog/2021-tongchenglvxing-shared/%E6%9E%B6%E6%9E%84_hu4472288679174941598.webp 760w,
               /blog/2021-tongchenglvxing-shared/%E6%9E%B6%E6%9E%84_hu17353375903987510327.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/%E6%9E%B6%E6%9E%84_hu7804573487903145154.webp&#34;
               width=&#34;717&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;基于 Kubernetes 屏蔽底层的基础设施，支持多机房多&lt;code&gt;Kubernetes&lt;/code&gt;集群的应用部署，除了要考虑各种大数据服务如何迁移上云，也要考虑整个平台的易用性，让组件研发无需登录机器进行运维和迁移等操作。同程自研了&lt;code&gt;云舱&lt;/code&gt;平台，主要承担这一职责。&lt;/p&gt;
&lt;p&gt;考虑到业务研发的接入成本，学习成本，研发&lt;code&gt;控制台&lt;/code&gt;平台，让只读的集群信息和集群管理结合起来。改变以前底层信息触摸不到的情景，让业务研发也能在平台层获取更多的信息，可以对自己的服务做出一些合理的判断。&lt;/p&gt;
&lt;h3 id=&#34;监控收集&#34;&gt;监控收集&lt;/h3&gt;
&lt;p&gt;使用&lt;code&gt;Thanos&lt;/code&gt;+&lt;code&gt;Prometheus Operator&lt;/code&gt;框架部署收集各个组件集群的监控，按照以下原则来做到监控的可扩展。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个组件集群对应一个&lt;code&gt;Prometheus&lt;/code&gt;节点&lt;/li&gt;
&lt;li&gt;每个组件都对应一套独立的 Thanos 集群，&lt;code&gt;Thanos Query&lt;/code&gt; 聚合同一组件的所有集群，&lt;code&gt;Thanos Rule&lt;/code&gt; 通过自研的&lt;code&gt;Sidecar&lt;/code&gt;同步组件报警规则，部署独立的&lt;code&gt;AlterManager&lt;/code&gt;，独立的&lt;code&gt;Grafana&lt;/code&gt;应用。&lt;/li&gt;
&lt;li&gt;每个组件都有一个 ceph bucket，将历史监控数据存储到&lt;code&gt;Ceph&lt;/code&gt;中。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/%E7%9B%91%E6%8E%A7_hu9620501066101648121.webp 400w,
               /blog/2021-tongchenglvxing-shared/%E7%9B%91%E6%8E%A7_hu14283603017917894343.webp 760w,
               /blog/2021-tongchenglvxing-shared/%E7%9B%91%E6%8E%A7_hu18283909278467117835.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/%E7%9B%91%E6%8E%A7_hu9620501066101648121.webp&#34;
               width=&#34;760&#34;
               height=&#34;427&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;监控域名规则配置如下：&lt;/p&gt;
&lt;p&gt;Prometheus: &lt;code&gt;&amp;lt;域名&amp;gt;/prometheus/&amp;lt;组件名&amp;gt;/&amp;lt;集群名称&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Thanos Query:&lt;code&gt; &amp;lt;域名&amp;gt;/thanos/&amp;lt;组件名&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Thanos Rule:&lt;code&gt; &amp;lt;域名&amp;gt;/thanos/rule/&amp;lt;组件名&amp;gt;/alerts&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;AlertManger: &lt;code&gt; &amp;lt;域名&amp;gt;/thanos/alert/&amp;lt;组件名&amp;gt;/#/alerts&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Grafana: &lt;code&gt; &amp;lt;域名&amp;gt;/grafana/&amp;lt;组件名&amp;gt;&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;集群服务日志收集&#34;&gt;集群服务日志收集&lt;/h3&gt;
&lt;p&gt;使用&lt;code&gt;Filebeat&lt;/code&gt;采集集群节点的服务日志，将&lt;code&gt;Filebeat&lt;/code&gt;容器和服务容器放在一个&lt;code&gt;Pod&lt;/code&gt;中，用富容器的方式来启动服务。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86_hu9471894182432472101.webp 400w,
               /blog/2021-tongchenglvxing-shared/%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86_hu3295972579256756059.webp 760w,
               /blog/2021-tongchenglvxing-shared/%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86_hu10639971270745625301.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86_hu9471894182432472101.webp&#34;
               width=&#34;739&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在&lt;code&gt;Flink&lt;/code&gt;计算层做日志诊断，提供配置规则动态更新，便于更快速发现集群的故障问题。&lt;/p&gt;
&lt;h3 id=&#34;集群生命周期平台化&#34;&gt;集群生命周期平台化&lt;/h3&gt;
&lt;p&gt;一个组件的集群从申请创建到服务销毁中间包含很多环节，应该将这些环节程序并平台化，让基础技术能以平台代码的形式沉淀下来。&lt;/p&gt;
&lt;p&gt;下图是用户申请&lt;code&gt;Hbase&lt;/code&gt;集群服务的工单，用户在申请的时候只需要填写少量配置。简单就是让业务少思考。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;hbase%e6%9c%8d%e5%8a%a1%e7%94%b3%e8%af%b7&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;组件&lt;code&gt;控制台&lt;/code&gt;为业务研发侧提供只读信息，例如集群信息、监控、日志、报警等功能，和组件本身管控平台相结合，不提供操作或者运维集群的功能。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;hbase%e6%8e%a7%e5%88%b6%e5%8f%b0&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;云舱平台&lt;/code&gt;会为组件研发提供完善的运维和诊断功能，让他们无需关心底层基础设施层。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;hbase%e4%ba%91%e8%88%b1&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;集群服务化后，计费，报警配置，日志诊断能功能都能轻松的集成起来。
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/%E8%AE%A1%E8%B4%B9_hu10537656328367717063.webp 400w,
               /blog/2021-tongchenglvxing-shared/%E8%AE%A1%E8%B4%B9_hu10552472887181125278.webp 760w,
               /blog/2021-tongchenglvxing-shared/%E8%AE%A1%E8%B4%B9_hu4086752440651686485.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/%E8%AE%A1%E8%B4%B9_hu10537656328367717063.webp&#34;
               width=&#34;760&#34;
               height=&#34;306&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;自研大数据云原生服务框架&#34;&gt;自研大数据云原生服务框架&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;云舱&lt;/code&gt;平台将服务分为单个容器和多个容器，用数量来区分，在此之上用组装的方式支持多节点类型，一个节点类型对应一个&lt;code&gt;Group&lt;/code&gt;,这个 Group 就是一组相同规格的容器。比如 Kudu 组件就分成两个&lt;code&gt;Group&lt;/code&gt;,master 和 tserver 两个&lt;code&gt;Group&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;用一个 UML 图来简单描述代码层结构：
















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/2021-tongchenglvxing-shared/UML%E5%9B%BE_hu10578193804617529135.webp 400w,
               /blog/2021-tongchenglvxing-shared/UML%E5%9B%BE_hu3413362112973174332.webp 760w,
               /blog/2021-tongchenglvxing-shared/UML%E5%9B%BE_hu12533351858611661189.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/2021-tongchenglvxing-shared/UML%E5%9B%BE_hu10578193804617529135.webp&#34;
               width=&#34;760&#34;
               height=&#34;378&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对&lt;code&gt;Kubernetes&lt;/code&gt;集群的操作会分解成多个&lt;code&gt;Task&lt;/code&gt;,&lt;code&gt;Task&lt;/code&gt;之间有依赖关系，组装成&lt;code&gt;Job&lt;/code&gt;发送给&lt;code&gt;Kafka&lt;/code&gt;，云舱 Scheduler 进行消费和处理。比如部署一个&lt;code&gt;Zookeeper&lt;/code&gt;集群，先创建容器，再创建&lt;code&gt;Service&lt;/code&gt;负载，配置&lt;code&gt;DNS&lt;/code&gt;策略，配置监控，这是一个完整的部署任务。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;%e4%bb%bb%e5%8a%a1%e6%ad%a5%e9%aa%a4&#34; alt=&#34;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;现状&#34;&gt;现状&lt;/h2&gt;
&lt;p&gt;当前同程将几乎所有的大数据服务都采用 &lt;code&gt;Kubernetes&lt;/code&gt; 工具部署和调度，有近&lt;code&gt;400+&lt;/code&gt;集群服务跑在&lt;code&gt;Kubernetes&lt;/code&gt;上，一个新的组件集群可以在 15 分钟之内完成交付，极大地减少组件部署消耗的时间。当所有的集群服务被平台化管理后，对于机器资源层的调度和利用率提升的需求越来越明显，同程基于资源监控对组件做混合部署，利用率提升&lt;code&gt;30%&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;大数据底层一般会分为计算和存储，但是随着机器资源越来越多，资源层的研发也是很关键的一环。同程希望将数据，资源，算法流程打通，让数据使用更简单，让数据处理更快更稳定。&lt;/p&gt;
&lt;p&gt;业界有很多公司会考虑将大数据计算任务 &lt;code&gt;native on Kubernetes&lt;/code&gt;,同程也进行调研和尝试，当前大家都只是解决了部署的问题，任务的完整生命周期还需要研发和测试。所以同程还是着重于 &lt;code&gt;Yarn&lt;/code&gt; on &lt;code&gt;Kubernetes&lt;/code&gt;,一些算法和分析类的&lt;code&gt;Python&lt;/code&gt;任务会采用容器调度方式运行。&lt;/p&gt;
&lt;h2 id=&#34;未来方向&#34;&gt;未来方向&lt;/h2&gt;
&lt;p&gt;同程大数据上云还有很多问题没有去优雅的解决，比如已有服务如何平滑的通过平台的方式迁移上云，现在还有很多中间过程需要资源研发介入。&lt;/p&gt;
&lt;p&gt;未来的方向主要分为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采用混部和分时调度，提升集群资源整体利用率。&lt;/li&gt;
&lt;li&gt;用混沌工程的方式提升组件稳定性。&lt;/li&gt;
&lt;li&gt;计算任务 &lt;code&gt;native on Kubernetes&lt;/code&gt;,提供高优保障。&lt;/li&gt;
&lt;li&gt;持续提升 PAAS 平台易用性。&lt;/li&gt;
&lt;li&gt;让底层资源触手可及。&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
