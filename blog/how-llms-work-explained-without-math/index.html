<!DOCTYPE html><html lang="zh" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="本文用通俗语言解释了大语言模型（LLM）的工作原理，从 token 机制、生成过程到训练方式和注意力机制，帮助你理解 LLM 的本质是一种复杂但可理解的算法，而非魔法。" />

  
  <link rel="alternate" hreflang="zh" href="https://cloudnativecn.com/blog/how-llms-work-explained-without-math/" />

  
  
  
    <meta name="theme-color" content="#0a55a7" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.008259417e6adf8980695ebbbb46553f.css" />

  



  


  


  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?f3dc895ea3bd6186cd835841d365c103";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>



  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu_5687ba431e9f3ad9.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu_e6256ffc27bfd989.png" />

  <link rel="canonical" href="https://cloudnativecn.com/blog/how-llms-work-explained-without-math/" />

  
  
  
  
  
  
  
  
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary_large_image" />
  
    <meta property="twitter:site" content="@CloudNativeCN" />
    <meta property="twitter:creator" content="@CloudNativeCN" />
  
  <meta property="og:site_name" content="云原生社区（中国）" />
  <meta property="og:url" content="https://cloudnativecn.com/blog/how-llms-work-explained-without-math/" />
  <meta property="og:title" content="大语言模型是怎么工作的？通俗解释版 | 云原生社区（中国）" />
  <meta property="og:description" content="本文用通俗语言解释了大语言模型（LLM）的工作原理，从 token 机制、生成过程到训练方式和注意力机制，帮助你理解 LLM 的本质是一种复杂但可理解的算法，而非魔法。" /><meta property="og:image" content="https://cloudnativecn.com/media/sharing.png" />
    <meta property="twitter:image" content="https://cloudnativecn.com/media/sharing.png" /><meta property="og:locale" content="zh" />
  
    
      <meta
        property="article:published_time"
        content="2025-04-21T16:40:00&#43;08:00"
      />
    
    <meta property="article:modified_time" content="2025-04-25T15:09:43&#43;08:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://cloudnativecn.com/blog/how-llms-work-explained-without-math/"
  },
  "headline": "大语言模型是怎么工作的？通俗解释版",
  
  "datePublished": "2025-04-21T16:40:00+08:00",
  "dateModified": "2025-04-25T15:09:43+08:00",
  
  "author": {
    "@type": "Person",
    "name": "Miguel Grinberg"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "云原生社区（中国）",
    "logo": {
      "@type": "ImageObject",
      "url": "https://cloudnativecn.com/media/logo.svg"
    }
  },
  "description": "本文用通俗语言解释了大语言模型（LLM）的工作原理，从 token 机制、生成过程到训练方式和注意力机制，帮助你理解 LLM 的本质是一种复杂但可理解的算法，而非魔法。"
}
</script>

  

  

  

  





  <title>大语言模型是怎么工作的？通俗解释版 | 云原生社区（中国）</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="a66c96361b678a45e0d5f44cad37181a" >
  <button onclick="topFunction()" id="backTopBtn" title="Go to top"><i class="fa-solid fa-circle-up" aria-hidden="true"></i></button>
  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.62d6f8dfe8493f1c68557dde65bec362.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6 search-title">
          <p>搜索</p>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="关闭"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="搜索..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="搜索...">
        
      </div>

      
      

      
    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

      <div id="search-common-queries">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    











  


<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/"><img src="/media/logo.svg" alt="云原生社区（中国）"
            
            ></a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="切换导航">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/"><img src="/media/logo.svg" alt="云原生社区（中国）"
          
          ></a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/community"><span>社区</span></a>
          </li>

          
          

          

          
          
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link  active" href="/blog"><span>博客</span></a>
          </li>

          
          

          
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>资料</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/envoy/"><span>Envoy 中文文档</span></a>
              
                <a class="dropdown-item" href="/kubebuilder/"><span>Kubebuilder 中文文档</span></a>
              
                <a class="dropdown-item" href="https://istio.io/latest/zh/"><span>云原生资料库</span></a>
              
            </div>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/event"><span>活动</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>关于</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        

        
        <li class="nav-item">
            <a class="nav-link" href="/community/join/" data-toggle="tooltip" data-placement="bottom" title="加入社区" aria-label="主站"><i class="fa-brands fa-weixin" aria-hidden="true"></i></a>
        </li>
        

        
        
        <li class="nav-item">
            <a class="nav-link js-search" href="#" data-toggle="tooltip" data-placement="bottom" title="搜索" aria-label="搜索"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://github.com/cloudnativeto/cloudnative.to" target="_blank" rel="noopener" data-toggle="tooltip" data-placement="bottom" title="查看源码" aria-label="查看源码" aria-label="GitHub"><i class="fa-brands fa-github" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item">
          <a href="#" class="nav-link set-theme">
            <i class="fa fa-sun" aria-hidden="true" id="theme-icon"></i>
          </a>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    <div class="container-xl">
    <div class="post-container">
        












  

  
  
  
<div class="article-container pt-3">
  <h1>大语言模型是怎么工作的？通俗解释版</h1>

  

  
    


<div class="article-metadata">

  <div>
  
  
  
  
    <i class="fa-solid fa-feather"></i>
    

  <span >
      <a href="/author/miguel-grinberg/">Miguel Grinberg</a></span>
    
    <span class="middot-divider"></span>
    
  
  
  
  
  
  
    <i class="fa fa-language"></i>
    

  <span >
      <a href="/translators/%E4%BA%91%E5%8E%9F%E7%94%9F%E7%A4%BE%E5%8C%BA/">云原生社区</a></span>
    <span class="middot-divider"></span>
  
  
  
  
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/ai/" class="text-capitalize">AI</a></span>
  
  </div>

  
  <span class="article-date">
    
    
      
          
          发布于
      
    
    2025-04-21
  </span>
  

  

  
  <span class="middot-divider"></span>
  字数 6496
  <span class="middot-divider"></span>
  <span class="article-reading-time">
      阅读时长 30 分钟
  </span>
  

  
  
  
  

</div>

    




<div class="btn-links mb-2">
  
  








  


















  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math" target="_blank" rel="noopener">
    <i class="fa fa-language mr-1"></i>阅读英文版原文</a>


</div>


  
</div>


    </div>
    <div class="border-bottom mb-2"></div>
    <div class="row flex-xl-nowrap">
        <div class="col-3 d-none d-xl-block docs-toc">
            <!-- toc -->
            
<div class="">
    <ul class="nav toc-top">
        <li>
            <a href="#" id="back_to_top" class="docs-toc-title">目录</a>
        </li>
    </ul>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#大语言模型到底在做什么">大语言模型到底在做什么？</a>
      <ul>
        <li><a href="#什么是-token">什么是 Token？</a></li>
        <li><a href="#预测下一个-token">预测下一个 Token</a></li>
        <li><a href="#生成一段完整的文字">生成一段完整的文字</a></li>
      </ul>
    </li>
    <li><a href="#模型是如何训练的">模型是如何训练的？</a>
      <ul>
        <li><a href="#关于训练数据空洞的思考">关于“训练数据空洞”的思考</a></li>
        <li><a href="#模拟实现如何用这张概率表生成预测">模拟实现：如何用这张概率表生成预测？</a></li>
        <li><a href="#上下文窗口context-window">上下文窗口（Context Window）</a></li>
        <li><a href="#如果增大上下文窗口呢">如果增大上下文窗口呢？</a></li>
        <li><a href="#如果用马尔可夫链来实现-gpt-2-的上下文窗口">如果用马尔可夫链来实现 GPT-2 的上下文窗口？</a></li>
        <li><a href="#马尔可夫链方法的极限">马尔可夫链方法的极限</a></li>
        <li><a href="#从马尔可夫链到神经网络">从马尔可夫链到神经网络</a></li>
        <li><a href="#层layerstransformer-和注意力机制attention">层（Layers）、Transformer 和注意力机制（Attention）</a></li>
      </ul>
    </li>
    <li><a href="#大语言模型真的有智能吗">大语言模型真的“有智能”吗？</a></li>
    <li><a href="#写在最后">写在最后</a></li>
  </ul>
</nav>
</div>

            <!-- /toc -->
            
            <div class="subscribe-module col-12 mt-1">
    <img src="/img/wechat.jpg" alt="image" title="云原生社区的微信公众号"/>
    <p class="text-center pt-1">关注「云原生社区动态」微信公众号，获取本站更新</p>
</div>

            
        </div>
        <main class="article-body col-9 container docs-content" role="main">
            <article class="article">
                <div class="article-style">
                    
                    <details class="toc-inpage d-print-none d-show-block my-4">
  <summary class="font-weight-bold">点击查看目录</summary>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#大语言模型到底在做什么">大语言模型到底在做什么？</a>
      <ul>
        <li><a href="#什么是-token">什么是 Token？</a></li>
        <li><a href="#预测下一个-token">预测下一个 Token</a></li>
        <li><a href="#生成一段完整的文字">生成一段完整的文字</a></li>
      </ul>
    </li>
    <li><a href="#模型是如何训练的">模型是如何训练的？</a>
      <ul>
        <li><a href="#关于训练数据空洞的思考">关于“训练数据空洞”的思考</a></li>
        <li><a href="#模拟实现如何用这张概率表生成预测">模拟实现：如何用这张概率表生成预测？</a></li>
        <li><a href="#上下文窗口context-window">上下文窗口（Context Window）</a></li>
        <li><a href="#如果增大上下文窗口呢">如果增大上下文窗口呢？</a></li>
        <li><a href="#如果用马尔可夫链来实现-gpt-2-的上下文窗口">如果用马尔可夫链来实现 GPT-2 的上下文窗口？</a></li>
        <li><a href="#马尔可夫链方法的极限">马尔可夫链方法的极限</a></li>
        <li><a href="#从马尔可夫链到神经网络">从马尔可夫链到神经网络</a></li>
        <li><a href="#层layerstransformer-和注意力机制attention">层（Layers）、Transformer 和注意力机制（Attention）</a></li>
      </ul>
    </li>
    <li><a href="#大语言模型真的有智能吗">大语言模型真的“有智能”吗？</a></li>
    <li><a href="#写在最后">写在最后</a></li>
  </ul>
</nav>
</details>

                    
                    <p>我相信你也会同意，现在已经无法忽视<strong>生成式 AI</strong>（Generative AI，简称 GenAI）了。关于大语言模型（Large Language Models，LLMs）的新闻铺天盖地。你很可能已经用过 <a href="https://chat.openai.com/" target="_blank" rel="noopener">ChatGPT</a>，甚至一直把它开着当助手用。</p>
<p>但很多人心中有一个基本疑问：这些模型看上去“很聪明”，这种“聪明”到底是从哪儿来的？</p>
<p>这篇文章就是想用简单的方式、尽量不涉及复杂数学，来解释文本生成模型是如何工作的，帮助你把它们当作<strong>计算机算法</strong>来理解，而不是神奇魔法。</p>
<h2 id="大语言模型到底在做什么">大语言模型到底在做什么？</h2>
<p>我们先来澄清一个很多人对大语言模型的误解。很多人以为这些模型是“会聊天”、“会答题”的，但实际上，它们唯一真正擅长的事情是：</p>
<p>👉 <strong>给它一段文字，它就根据上下文“猜”下一个词（准确说是“Token”）是什么。</strong></p>
<p>所以，我们可以从“Token”这个核心概念开始揭开 LLM 的神秘面纱。</p>
<h3 id="什么是-token">什么是 Token？</h3>
<p>Token（词元）是大语言模型处理文本的最小单位。</p>
<p>你可以大致把它当成是“词”，但实际上，Token 既可以是一个字母、一个词，也可以是一段词根、甚至是空格或标点。LLM 的目标是<strong>尽可能高效地表示文本</strong>，所以不会总是按单词来切分。</p>
<p>一个语言模型的全部 Token 列表就叫它的“词汇表”（vocabulary）。这个词汇表是通过一种叫做 <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank" rel="noopener">BPE（字节对编码）</a> 的算法，从大量语料中训练出来的。</p>
<p>举个例子，开源的 <a href="https://github.com/openai/gpt-2" target="_blank" rel="noopener">GPT-2</a> 模型就使用了一个包含 <strong>50,257 个 Token</strong> 的词汇表。</p>
<p>每个 Token 都有一个唯一编号，模型会用一个叫做 <strong>tokenizer（分词器）</strong> 的工具，把你输入的文字转换成 Token 编号列表。你也可以在 Python 中尝试这个过程：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">pip install tiktoken
</span></span></code></pre></div><p>然后在 Python 中运行以下代码：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">tiktoken</span>
</span></span><span class="line"><span class="cl"><span class="n">encoding</span> <span class="o">=</span> <span class="n">tiktoken</span><span class="o">.</span><span class="n">encoding_for_model</span><span class="p">(</span><span class="s2">&#34;gpt-2&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&#34;The quick brown fox jumps over the lazy dog.&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 输出: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">464</span><span class="p">])</span>    <span class="c1"># &#39;The&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">2068</span><span class="p">])</span>   <span class="c1"># &#39; quick&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">13</span><span class="p">])</span>     <span class="c1"># &#39;.&#39;</span>
</span></span></code></pre></div><p>可以看到：</p>
<ul>
<li>Token <code>464</code> 表示 &ldquo;The&rdquo;</li>
<li>Token <code>2068</code> 表示 &quot; quick&quot;（注意前导空格也包括进去了）</li>
<li>Token <code>13</code> 表示句号 &ldquo;.&rdquo;</li>
</ul>
<p>由于分词是算法决定的，有时候你会发现一些奇怪的现象：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;The&#39;</span><span class="p">)</span>    <span class="c1"># [464]</span>
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39;the&#39;</span><span class="p">)</span>    <span class="c1"># [1169]</span>
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s1">&#39; the&#39;</span><span class="p">)</span>   <span class="c1"># [262]</span>
</span></span></code></pre></div><p>同一个词，根据是否有大小写、是否有前导空格，会被编码成不同的 Token。</p>
<p>此外，<strong>使用频率低的词</strong>不会单独占用一个 Token，而是被拆成多个 Token。例如：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&#34;Payment&#34;</span><span class="p">)</span>      <span class="c1"># [19197, 434]</span>
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">19197</span><span class="p">])</span>        <span class="c1"># &#39;Pay&#39;</span>
</span></span><span class="line"><span class="cl"><span class="n">encoding</span><span class="o">.</span><span class="n">decode</span><span class="p">([</span><span class="mi">434</span><span class="p">])</span>          <span class="c1"># &#39;ment&#39;</span>
</span></span></code></pre></div><h3 id="预测下一个-token">预测下一个 Token</h3>
<p>正如前面提到的，语言模型的任务就是<strong>预测下一个 Token 是什么</strong>。</p>
<p>假设你已经输入了：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">[</span><span class="s1">&#39;The&#39;</span><span class="p">,</span> <span class="s1">&#39; quick&#39;</span><span class="p">,</span> <span class="s1">&#39; brown&#39;</span><span class="p">,</span> <span class="s1">&#39; fox&#39;</span><span class="p">]</span>
</span></span></code></pre></div><p>然后运行伪代码：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">predictions</span> <span class="o">=</span> <span class="n">get_token_predictions</span><span class="p">([</span><span class="s1">&#39;The&#39;</span><span class="p">,</span> <span class="s1">&#39; quick&#39;</span><span class="p">,</span> <span class="s1">&#39; brown&#39;</span><span class="p">,</span> <span class="s1">&#39; fox&#39;</span><span class="p">])</span>
</span></span></code></pre></div><p>这一步会返回一份<strong>概率分布表</strong>，告诉你在这个上下文下，每个 Token 作为下一个词的概率是多少。</p>
<p>以 GPT-2 为例，这个返回值就是一个包含 <strong>50,257 个浮点数</strong> 的列表，每个数表示对应 Token 出现在下一个位置的概率。</p>
<p>你可以想象，像 &ldquo;jumps&rdquo; 这样的词很可能被赋予较高概率，而像 &ldquo;potato&rdquo; 这样的无关词概率则接近 0。</p>
<p>为了能做出这种预测，模型必须经过一个<strong>训练过程</strong>：它读了大量的文本，从中学习“哪些词通常跟随哪些词”。</p>
<p>最终它构建了一个复杂的数据结构，能根据输入 Token 序列预测下一个 Token 的概率。</p>
<p>这和你原来想象的一样吗？现在你是不是开始觉得，这其实并没有那么神秘了？</p>
<h3 id="生成一段完整的文字">生成一段完整的文字</h3>
<p>因为语言模型每次只能预测一个下一个 token（词元），所以如果我们想要它生成一整段句子，唯一的办法就是<strong>通过循环多次调用模型</strong>，每次生成一个新的 token，直到长度足够。</p>
<p>每一次循环中，模型都会根据当前的输入预测下一个 token 的概率分布，然后从中选择一个 token，添加到输入序列的末尾，再继续下一轮预测。这就像接力一样，模型每次生成一个词，然后拿这个词继续接着往下写。</p>
<p>我们来看一段更完整的伪代码（Python 风格）：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">generate_text</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">num_tokens</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenize</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">predictions</span> <span class="o">=</span> <span class="n">get_token_predictions</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">next_token</span> <span class="o">=</span> <span class="n">select_next_token</span><span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_token</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="s1">&#39;&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</span></span></code></pre></div><ul>
<li><code>generate_text()</code> 函数接受一个用户的输入提示（prompt），比如一句话或者一个问题。</li>
<li><code>tokenize()</code> 是一个辅助函数，它会用如 <code>tiktoken</code> 这样的工具把文本转成 Token 编号的列表。</li>
<li>在 <code>for</code> 循环中，每轮会调用 <code>get_token_predictions()</code>，也就是实际调用 AI 模型，让它预测下一个 token 的概率。</li>
<li>然后通过 <code>select_next_token()</code> 这个函数，从这些概率中选出一个 token 来作为输出。</li>
</ul>
<p>这个选 token 的函数可以用多种策略：</p>
<ul>
<li>最简单的是<strong>选择概率最高的那个</strong>（在机器学习中称为“贪婪选择”/greedy selection）；</li>
<li>更聪明的方法是<strong>用随机性来加点变化</strong>：根据概率分布用随机数决定哪个 token 被选中，这样同一个 prompt 可以生成不一样的内容，增加“创造力”。</li>
</ul>
<p>为了进一步控制生成结果的风格，我们可以用一些**超参数（hyperparameters）**来影响 token 的选择过程。这些超参数是通过参数传给 <code>generate_text()</code> 的：</p>
<ul>
<li>比如 <code>temperature</code>（温度）参数：它会影响模型对“冷门”词的选择倾向。温度越高，概率分布会“拉平”，选择一些低概率 token 的可能性就会增加，生成结果就更有想象力。</li>
<li>还有 <code>top_p</code> 和 <code>top_k</code>，用于控制模型考虑的“最可能的几个 token”，从中再做选择。</li>
</ul>
<p>一旦选定了下一个 token，就会进入下一轮循环，把这个新 token 加入输入中，继续预测下一个 token，直到生成足够长度的内容。</p>
<p>注意：这个过程不懂得“句子”或“段落”的概念，它只是一个词接一个词地往下预测。所以生成的文本可能会在句子中间突然结束。为了解决这个问题，<code>num_tokens</code> 参数可以设置为“最多生成几个 token”，而不是固定数量。你还可以设置当生成出句号 token（如 <code>.</code>）时就自动结束。</p>
<p>如果你读到这里并理解了上述内容，恭喜你！你已经掌握了 LLM 的基本工作机制。如果你还想更进一步了解，那么接下来我们会进入稍微技术一点的内容（但依然尽量避免复杂数学）。</p>
<h2 id="模型是如何训练的">模型是如何训练的？</h2>
<p>谈到模型是怎么训练出来的，其实很难完全避开数学。不过这里我会用一个非常简单直观的例子来帮助你理解这个过程。</p>
<p>因为语言模型的核心任务是“预测接下来可能出现的 token”，所以最基础的训练方式，就是去统计在训练语料中连续出现的 token 对（也就是“前一个词+下一个词”的组合），并建立一个概率表。</p>
<p>我们从一个简单的例子开始：假设模型的词汇表中只有以下 5 个 token：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">[</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s1">&#39;like&#39;</span><span class="p">,</span> <span class="s1">&#39;apples&#39;</span><span class="p">,</span> <span class="s1">&#39;bananas&#39;</span><span class="p">]</span>
</span></span></code></pre></div><p>为了简化，我们这里不考虑空格和标点符号。</p>
<p>训练语料由三句话组成：</p>
<ul>
<li>I like apples</li>
<li>I like bananas</li>
<li>you like bananas</li>
</ul>
<p>现在我们可以构建一个 5×5 的表格，行表示“前一个 token”，列表示“后一个 token”。我们统计每个组合在语料中出现的次数：</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>I</th>
          <th>you</th>
          <th>like</th>
          <th>apples</th>
          <th>bananas</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>I</strong></td>
          <td></td>
          <td></td>
          <td>2</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td><strong>you</strong></td>
          <td></td>
          <td></td>
          <td>1</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td><strong>like</strong></td>
          <td></td>
          <td></td>
          <td></td>
          <td>1</td>
          <td>2</td>
      </tr>
      <tr>
          <td><strong>apples</strong></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td><strong>bananas</strong></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>这个表表示：</p>
<ul>
<li>“I like” 出现了 2 次；</li>
<li>“you like” 出现了 1 次；</li>
<li>“like apples” 出现了 1 次；</li>
<li>“like bananas” 出现了 2 次。</li>
</ul>
<p>接下来我们把这张“频次表”转换成“概率表”。方法是：每一行中出现的次数加总后，按比例转换成概率。</p>
<p>例如，&ldquo;like&rdquo; 这一行有两种后续词：</p>
<ul>
<li>&ldquo;apples&rdquo; 出现 1 次；</li>
<li>&ldquo;bananas&rdquo; 出现 2 次；</li>
</ul>
<p>总共 3 次，所以：</p>
<ul>
<li>&ldquo;apples&rdquo; 的概率是 1/3 ≈ 33.3%；</li>
<li>&ldquo;bananas&rdquo; 的概率是 2/3 ≈ 66.7%。</li>
</ul>
<p>于是最终的概率表如下（空格表示概率为 0%）：</p>
<table>
  <thead>
      <tr>
          <th></th>
          <th>I</th>
          <th>you</th>
          <th>like</th>
          <th>apples</th>
          <th>bananas</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td><strong>I</strong></td>
          <td></td>
          <td></td>
          <td>100%</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td><strong>you</strong></td>
          <td></td>
          <td></td>
          <td>100%</td>
          <td></td>
          <td></td>
      </tr>
      <tr>
          <td><strong>like</strong></td>
          <td></td>
          <td></td>
          <td></td>
          <td>33.3%</td>
          <td>66.7%</td>
      </tr>
      <tr>
          <td><strong>apples</strong></td>
          <td>25%</td>
          <td>25%</td>
          <td>25%</td>
          <td></td>
          <td>25%</td>
      </tr>
      <tr>
          <td><strong>bananas</strong></td>
          <td>25%</td>
          <td>25%</td>
          <td>25%</td>
          <td>25%</td>
          <td></td>
      </tr>
  </tbody>
</table>
<p>你可能注意到了，&ldquo;apples&rdquo; 和 &ldquo;bananas&rdquo; 的那两行看上去很奇怪 —— 因为在我们的训练数据中，它们后面没有再跟任何词，这就是训练数据中的“空洞”。</p>
<p>为了让模型不会在遇到这些 token 时“卡住”，我采用了一种折中办法：给每个可能的下一个 token 都分配了相等的概率（即平均分配）。虽然这样做不一定准确，但至少能保证模型不会直接“失效”。</p>
<h3 id="关于训练数据空洞的思考">关于“训练数据空洞”的思考</h3>
<p>这个例子中的“空洞”很明显，在真实的大模型中可能不容易察觉。但即使是使用了超大规模语料的数据集，也还是可能存在一些训练不足的区域。</p>
<p>这些“稀疏区域”的预测质量通常比较差，虽然看起来语句没问题，但可能会出现事实错误、逻辑不通等现象 —— 这就是所谓的 <strong>幻觉（hallucination）</strong>，即模型生成的内容“像真的，但其实是假的”。</p>
<h3 id="模拟实现如何用这张概率表生成预测">模拟实现：如何用这张概率表生成预测？</h3>
<p>现在你已经有了这个概率表，那么语言模型内部的 <code>get_token_predictions()</code> 函数其实就可以非常简单地实现了：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">get_token_predictions</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="n">last_token</span> <span class="o">=</span> <span class="n">input_tokens</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">probabilities_table</span><span class="p">[</span><span class="n">last_token</span><span class="p">]</span>
</span></span></code></pre></div><p>你传入一串 token（比如用户的 prompt），模型只会关注最后一个 token，然后返回它在概率表中对应的一行。</p>
<p>例如输入：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="p">[</span><span class="s1">&#39;you&#39;</span><span class="p">,</span> <span class="s1">&#39;like&#39;</span><span class="p">]</span>
</span></span></code></pre></div><p>函数就会读取 &ldquo;like&rdquo; 这一行，告诉你：</p>
<ul>
<li>&ldquo;apples&rdquo; 的概率是 33.3%</li>
<li>&ldquo;bananas&rdquo; 的概率是 66.7%</li>
</ul>
<p>那么在上一个伪代码中 <code>select_next_token()</code> 函数就有 1/3 的概率选中 &ldquo;apples&rdquo;，2/3 的概率选中 &ldquo;bananas&rdquo;。</p>
<p>如果模型选择了 &ldquo;apples&rdquo;，生成的句子就是：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-text" data-lang="text"><span class="line"><span class="cl">you like apples
</span></span></code></pre></div><p>这是一句<strong>训练数据中没有</strong>的句子，但它却是非常合理的。模型仅凭过去学到的词序关系，就能“合成”出一个看似全新的输出。</p>
<p>你现在应该能理解：模型的“创造力”其实是通过把已学过的词汇和模式<strong>巧妙地拼接在一起</strong>而来，这种方式虽然没有真正的“思考”，但效果却很惊艳。</p>
<h3 id="上下文窗口context-window">上下文窗口（Context Window）</h3>
<p>在上一节中，我用来训练迷你语言模型的方法，属于一种叫做<a href="https://zh.wikipedia.org/wiki/%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%93%be" target="_blank" rel="noopener">马尔可夫链（Markov chain）</a>的技术。</p>
<p>这种方法的一个核心问题是：它**只考虑上一个 token（词元）**来做预测。也就是说，在选择下一个 token 时，之前出现的所有内容都会被“遗忘”。因此，我们可以说这种方法的上下文窗口是 <strong>1 个 token</strong> —— 实在太小了。</p>
<p>当上下文窗口这么小，模型就像失忆症一样，每次预测都只能看到最近的一个词，很难保持上下文的一致性或连贯性。</p>
<h3 id="如果增大上下文窗口呢">如果增大上下文窗口呢？</h3>
<p>一个办法是：<strong>扩展概率表，使用更长的 token 序列作为上下文</strong>。</p>
<p>比如：</p>
<ul>
<li>若使用 2 个 token 做上下文（窗口大小为 2），就需要在概率表中添加所有可能的两词组合；</li>
<li>用我前面举的例子（5 个 token），就需要再添加 5×5=25 行，再加上原本的 5 行，一共 30 行；</li>
<li>模型训练时要学会每三个 token 的组合关系，才能预测下一个 token；</li>
<li>运行时，每次都取最后两个 token 作为上下文，用来查找对应的概率分布。</li>
</ul>
<p>不过，<strong>2 个 token 作为上下文依然远远不够</strong>。想要生成更自然、上下文更一致的文本，我们需要更长的上下文窗口。否则，模型无法让新生成的 token 和之前的内容保持一致或有逻辑联系。</p>
<p>那我们需要多大窗口呢？</p>
<p>如果窗口从 2 提高到 3，那么就需要 5×5×5 = 125 个组合，表格就要增加 125 行。但效果仍然很有限。</p>
<h3 id="如果用马尔可夫链来实现-gpt-2-的上下文窗口">如果用马尔可夫链来实现 GPT-2 的上下文窗口？</h3>
<p>GPT-2 的上下文窗口是 <strong>1024 个 token</strong>。</p>
<p>如果我们用马尔可夫链来实现这样的模型，就意味着我们要准备一个超大的概率表，每一行都代表一串最多 1024 个 token 的组合。</p>
<p>举个例子，如果词汇表中只有 5 个 token，那么可能的 1024 长度的组合数量是：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">pow</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="mf">55626846462680034577255817933310101605480399.</span><span class="o">..</span><span class="p">(</span><span class="n">后面还有上百位</span><span class="p">)</span>
</span></span></code></pre></div><p>这是一个<strong>天文数字级别的行数</strong>，光是存这个表都需要超出地球所有硬盘的总容量！</p>
<p>而且这还只是考虑长度为 1024 的组合。实际上我们还要支持长度为 1、2、3……1023 的组合，因为用户的输入可能并没有那么长。那整个表的规模会更大，完全不可行。</p>
<h3 id="马尔可夫链方法的极限">马尔可夫链方法的极限</h3>
<p>随着模型的发展：</p>
<ul>
<li>GPT-3 的上下文窗口提升到 2048；</li>
<li>GPT-3.5 提升到了 4096；</li>
<li>GPT-4 起步是 8192，后来又提升到 32K，甚至是 128K（没错，是 12.8 万个 token）；</li>
<li>最新的研究中，甚至出现了支持 <strong>上百万 token 上下文窗口</strong> 的模型。</li>
</ul>
<p>这意味着模型已经能够理解和关联更大段落之间的语义，生成的内容也更加连贯、一致。</p>
<p><strong>马尔可夫链</strong>是一种很棒的思维工具，它让我们理解“预测下一个 token”的基本逻辑。但它<strong>不适合构建真正的大语言模型</strong>，因为它在上下文长度、存储空间和可扩展性方面有根本性的瓶颈。</p>
<p>为了处理大规模上下文，我们需要更智能、更高效的机制 —— 而这正是神经网络和 Transformer 架构真正擅长的领域。</p>
<h3 id="从马尔可夫链到神经网络">从马尔可夫链到神经网络</h3>
<p>显然，我们已经无法继续依赖“概率表”的方式了 —— 因为如果想支持较大的上下文窗口，那么这张表将变得大得离谱，根本无法存在于内存中。</p>
<p>那我们该怎么办？</p>
<p>答案是：<strong>用一个函数代替表格</strong>。这个函数不再存储所有可能性，而是通过算法“算”出一个近似的概率分布 —— 这正是<strong>神经网络</strong>擅长的事情。</p>
<p>神经网络是一种特殊类型的函数，它：</p>
<ul>
<li>接收一些输入（比如 token）；</li>
<li>对这些输入进行复杂的计算；</li>
<li>输出一个结果 —— 在语言模型中，就是下一个 token 的概率分布。</li>
</ul>
<p>为什么说它“特殊”？</p>
<p>因为神经网络的计算行为不光由代码控制，还依赖于大量<strong>外部定义的参数（parameters）</strong>。</p>
<p>这些参数在一开始是未知的、随机的，所以模型一开始的输出几乎毫无用处。</p>
<p><strong>训练的过程</strong>，就是找到那些<strong>最适合的参数组合</strong>，使得神经网络的预测结果尽可能地贴近训练数据中的真实情况。</p>
<p>训练过程会反复进行多轮，每一轮对参数进行一点点微调，这个过程叫做<a href="https://en.wikipedia.org/wiki/Backpropagation" target="_blank" rel="noopener">反向传播（Backpropagation）</a>，它本质上是一个数学优化过程（本文不会深入讨论）。</p>
<p>每次参数更新之后，模型会再次评估它在训练数据上的表现，再基于反馈继续调整参数。这个循环会持续进行，直到模型能在训练集上做出合理的预测为止。</p>
<p>为了帮助你理解神经网络的规模，我们来看几个数据：</p>
<ul>
<li><strong>GPT-2</strong>：约 15 亿个参数</li>
<li><strong>GPT-3</strong>：1750 亿个参数</li>
<li><strong>GPT-4</strong>：据说达到了 1.76 万亿个参数！</li>
</ul>
<p>训练这种规模的神经网络，即使使用现代最强的计算资源，也需要花费数周甚至数月。</p>
<p>由于神经网络中隐藏了如此多的参数，而且这些参数全是通过自动化迭代得出的，<strong>连开发者自己都很难完全理解模型内部是如何“思考”的</strong>。</p>
<p>一个训练完成的 LLM 更像是一个<strong>黑盒</strong> —— 你看不到它的内部“逻辑”，只能观察它的输入输出。就连训练它的人也很难解释它为什么会给出某个答案。</p>
<h3 id="层layerstransformer-和注意力机制attention">层（Layers）、Transformer 和注意力机制（Attention）</h3>
<p>你也许会好奇：这些神经网络到底在内部做了些什么？它们是如何“把一堆 token 输入，经过参数的计算，然后输出合理的下一个 token 的概率”的？</p>
<p>这就涉及到神经网络的“分层计算”结构。</p>
<p>一个神经网络由多个称为**层（layer）**的计算模块组成。其基本流程是：</p>
<ol>
<li>第一层接收输入；</li>
<li>对输入做一次变换（数学运算）；</li>
<li>把变换后的结果传给下一层；</li>
<li>重复这个过程，直到最后一层生成输出。</li>
</ol>
<p>每一层都像一个“加工车间”，把输入“改造”一下，然后再传递下去。</p>
<p>机器学习研究者会设计不同类型的层，用来处理不同的数据类型，比如图像或文本。有些层是通用的，有些则是专门为处理“token 化后的文本”设计的。</p>
<p>目前在大语言模型中，最常用的神经网络架构叫做 <strong>Transformer</strong>。这种架构非常适合处理序列数据（例如自然语言中的单词序列），它的核心机制是：</p>
<blockquote>
<p><strong>注意力机制（Attention）</strong></p></blockquote>
<p>Attention 能帮助模型理解“在上下文中，哪些 token 比较重要”，并基于这种理解生成更合理的下一个词的概率分布。</p>
<p>最早，这种机制是用在机器翻译中的，用来判断“源语言的句子中，哪些词在翻译中最关键”。它可以模拟人类阅读时“聚焦重点词语”的过程。</p>
<p>今天，Attention 已成为 Transformer 架构的核心能力，使得 LLM 能够从长上下文中捕捉信息，从而生成连贯、逻辑一致的文本。</p>
<p>如果你已经理解了这一部分，那么你就已经初步了解了大语言模型“背后的魔法”其实是<strong>数学 + 神经网络 + 算法工程</strong>的结合成果。下一步，我们可以探讨这些模型是否真的“具备智能”。</p>
<h2 id="大语言模型真的有智能吗">大语言模型真的“有智能”吗？</h2>
<p>看到这里，你可能已经开始有自己的判断：LLM（大语言模型）在生成文本时，是否体现出某种“智能”。</p>
<p>就我个人而言，我并不认为 LLM 具有真正的推理能力，或者能够产生原创性的思想。但这并不意味着它们没有价值。</p>
<p>凭借对上下文中 token 的精妙计算，LLM 能够识别出用户提示（prompt）中的模式，并将其与训练时学到的相似模式进行匹配。它生成的文本，虽然大多是从训练数据中“拼拼凑凑”而来，但拼接的方式非常巧妙，<strong>看起来像是“自己想出来的”</strong>，而且往往很有用。</p>
<p>不过，考虑到 LLM 存在**幻觉（hallucination）**的倾向（即生成内容看起来合理但却是虚假的），我不建议把 LLM 的输出直接交给最终用户使用，除非事先经过人工审核和验证。</p>
<p>那么，未来几个月或几年中出现的更大规模的 LLM，有可能具备真正的智能吗？</p>
<p>我认为，<strong>至少在目前 GPT 架构的框架下，还很难达到真正意义上的智能</strong> —— 因为它还有很多根本性的限制。但谁知道呢？也许未来的某些创新，真的能带我们走得更远。</p>
<h2 id="写在最后">写在最后</h2>
<p>感谢你一路读到这里！</p>
<p>我希望这篇文章能够激起你继续深入学习的兴趣，哪怕最后你真的要面对那些“可怕的数学”，因为如果你想彻底理解 LLM 的每一个细节，那是绕不过去的。</p>
<p>如果你准备好面对这些挑战，我强烈推荐 Andrej Karpathy 的视频课程：<a href="https://karpathy.ai/zero-to-hero.html" target="_blank" rel="noopener">Neural Networks: Zero to Hero</a>。它是理解神经网络从入门到精通的绝佳起点。</p>

                </div>
                

<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/llm/">LLM</a>
  
  <a class="badge badge-light" href="/tag/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">大语言模型</a>
  
  <a class="badge badge-light" href="/tag/transformer/">Transformer</a>
  
  <a class="badge badge-light" href="/tag/%E7%94%9F%E6%88%90%E5%BC%8Fai/">生成式AI</a>
  
  <a class="badge badge-light" href="/tag/ai%E5%9F%BA%E7%A1%80/">AI基础</a>
  
</div>











  
  
    



  
  
  
  
  
  <div class="media author-card content-widget-hr mb-4">
    

    <div class="media-body">
      <p class="card-title"><a href="/author/miguel-grinberg/">Miguel Grinberg</a></p>
      
      
      
    </div>
  </div>


  


  
  
    




  




<div class="article-widget">
  
<div class="container-xl row post-nav">
  
  
  
  <a class="col-6 post-nav-item btn btn-lg mb-md-1" href="/blog/gateway-api-inference-extension-deep-dive/" rel="next">
    <div class="meta-nav">下一页</div>
    <p>深入解析 Gateway API Inference Extension（推理扩展）</p></a>
  
  
  
  <a class="col-6 post-nav-item btn btn-lg mb-md-1"  href="/blog/kubernetes-resource-orchestration-kro/" rel="prev">
    <div class="meta-nav">上一页</div>
    <p>Kro 项目：一次三大云厂商联合赋能 Kubernetes 用户的范例</p></a>
  
</div>

</div>










  

<p class="edit-page">
  <a href="https://github.com/cloudnativeto/cloudnative.to/edit/master/content/blog/how-llms-work-explained-without-math/index.md">
    <i class="fas fa-pen pr-2"></i>编辑本页
  </a>
</p>




  
  
  <div class="article-widget content-widget-hr">
    <p class="related-title">相关推荐</p>
    <ul>
      
      <li><a href="/blog/gateway-api-inference-extension-deep-dive/">深入解析 Gateway API Inference Extension（推理扩展）</a></li>
      
      <li><a href="/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/">什么是 AI Agent？简要介绍与构建指南</a></li>
      
      <li><a href="/blog/a-gentle-introduction-to-llms-for-platform-engineers/">平台工程师的 LLM 入门指南</a></li>
      
    </ul>
  </div>
  





  
  
  

  

  
  <section id="comments" class="mb-3 pt-0">
    <script>
  let themeNumber = localStorage.getItem('wcTheme');
  var giscusTheme = "light";
  if (themeNumber == 1){
    giscusTheme = "dark";
  }
  let giscusAttributes = {
    "src": "https://giscus.app/client.js",
    "data-theme": giscusTheme,
    "data-repo":"cloudnativeto\/cloudnative.to",
    "data-repo-id":"MDEwOlJlcG9zaXRvcnkyMzc3NDUxOTA=",
    "data-category":"General",
    "data-category-id":"MDE4OkRpc2N1c3Npb25DYXRlZ29yeTMyMDU5MzUy",
    "data-mapping":"pathname",
    "data-reactions-enabled":"",
    "data-emit-metadata":"0",
    "data-input-position":"top",
    "data-theme":giscusTheme,
    "data-lang":"zh-CN",
    "data-loading":"lazy",
    "crossorigin":"annoymous",
    "origins":"https://cloudnativecn.com",
    "originsRegex":"http://localhost:[0-9]+",
    "async": "",
  };

  let giscusScript = document.createElement("script");
  Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
  document.querySelector('#comments').appendChild(giscusScript);
</script>

  </section>
  



            </article>
        </main>
    </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  
  <div class="copyright py-4 bg-footer">
      <div class="row justify-content-center">
        <div class="text-center footer-color">
          <p class="mb-0">© 2020-2024 云原生社区保留所有权利</p>
        </div>
    </div>
  </div>

</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.0965c02a44d7bc3f6aebc3730ec59495.js"></script>




  

  
  

  

  
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.1/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
    
    
  










  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <div class="article-metadata search-hit-type">{{relpermalink}}</div>
          <a href="{{relpermalink}}">{{title}}</a>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  







<script id="page-data" type="application/json">{"use_headroom":false}</script>










  
  


<script src="/zh/js/wowchemy.min.1f4bda0700ff78dcd04ab5307750c948.js"></script>







<script>

var mybutton = document.getElementById("backTopBtn");


window.onscroll = function() {scrollFunction()};

function scrollFunction() {
  if (document.body.scrollTop > 20 || document.documentElement.scrollTop > 20) {
    mybutton.style.display = "block";
  } else {
    mybutton.style.display = "none";
  }
}


function topFunction() {
  document.body.scrollTop = 0;
  document.documentElement.scrollTop = 0;
}
</script>






<script src="https://cdnjs.cloudflare.com/ajax/libs/anchor-js/4.2.2/anchor.min.js" integrity="sha512-I7w3ZdSFzw5j3jU3ZkNikBNeIrl3i+hEuEdwNmqUJvwNcaBUNcijnP2gd9DtGlgVYDplfjGoD8vTNsID+lCjqg==" crossorigin="anonymous"></script>
<script>
  anchors.add();
</script>



<script>



(function() {
  'use strict';

  if(!document.queryCommandSupported('copy')) {
    return;
  }

  function flashCopyMessage(el, msg) {
    el.className = "highlight-copy-btn";
    el.textContent = msg;
    setTimeout(function() {
      el.textContent = "";
      el.className = "highlight-copy-btn fa fa-copy";
    }, 1000);
  }

  function selectText(node) {
    var selection = window.getSelection();
    var range = document.createRange();
    range.selectNodeContents(node);
    selection.removeAllRanges();
    selection.addRange(range);
    return selection;
  }

  function addCopyButton(containerEl) {
    var copyBtn = document.createElement("button");
    copyBtn.className = "highlight-copy-btn fa fa-copy";
    copyBtn.textContent = "";

    var codeEl = containerEl.firstElementChild;
    copyBtn.addEventListener('click', function() {
      try {
        var selection = selectText(codeEl);
        document.execCommand('copy');
        selection.removeAllRanges();
        
        flashCopyMessage(copyBtn, '已复制')
        
      } catch(e) {
        console && console.log(e);
        flashCopyMessage(copyBtn, 'Failed :\'(')
      }
    });

    containerEl.appendChild(copyBtn);
  }

  
  var highlightBlocks = document.getElementsByClassName('highlight');
  Array.prototype.forEach.call(highlightBlocks, addCopyButton);
})();
</script>



<script>

function Collapse(e){
  var node = document.getElementById(e);
  if (node.className.indexOf('fa-angle-down') > -1){
    node.setAttribute("class", "fa-solid fa-angle-right");
    }else{
    node.setAttribute("class", "fa-solid fa-angle-down");
    }
}
</script>


</body>
</html>
