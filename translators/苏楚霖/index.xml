<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>苏楚霖 | 云原生社区（中国）</title>
    <link>https://cloudnative.to/translators/%E8%8B%8F%E6%A5%9A%E9%9C%96/</link>
      <atom:link href="https://cloudnative.to/translators/%E8%8B%8F%E6%A5%9A%E9%9C%96/index.xml" rel="self" type="application/rss+xml" />
    <description>苏楚霖</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Thu, 15 Jul 2021 10:42:00 +0800</lastBuildDate>
    <image>
      <url>https://cloudnative.to/translators/%E8%8B%8F%E6%A5%9A%E9%9C%96/avatar_hue38add62c87b7486d80c9f3fda25dfc1_12220_270x270_fill_q75_lanczos_center.jpg</url>
      <title>苏楚霖</title>
      <link>https://cloudnative.to/translators/%E8%8B%8F%E6%A5%9A%E9%9C%96/</link>
    </image>
    
    <item>
      <title>在 Kubernetes 上部署应用时如何更新数据库</title>
      <link>https://cloudnative.to/blog/running-database-migration-when-deploying-to-kubernetes/</link>
      <pubDate>Thu, 15 Jul 2021 10:42:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/running-database-migration-when-deploying-to-kubernetes/</guid>
      <description>&lt;p&gt;本文为翻译文章，&lt;a href=&#34;https://andrewlock.net/deploying-asp-net-core-applications-to-kubernetes-part-7-running-database-migrations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;点击查看原文&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;在这篇文章中，我会介绍在生产环境部署多副本应用时会碰到的一个经典问题： 数据库 schema 更新。&lt;/p&gt;
&lt;p&gt;接下来将会讨论几种可能的解决方案，从简单到复杂。最后会总结一下我是如何在生产环境解决这个问题的：使用 Kubernetes Job 和 init container。 了解了这些解决方案（最重要的是谨慎的设计数据库 schema ）你可以比较从容的应对应用升级时的数据库数据更新的问题。&lt;/p&gt;
&lt;h2 id=&#34;经典问题-不断更新的数据库&#34;&gt;经典问题： 不断更新的数据库&lt;/h2&gt;
&lt;p&gt;这个问题其实非常普遍，新版本的应用需要使用到新的数据库 schema，所以当部署新版本应用时，就需要更新数据库的 schema，通常我们称之为 数据库 schema 变更(database migrations),几乎所有涉及数据库的应用都会碰到类似的问题。&lt;/p&gt;
&lt;p&gt;有很多不同的方式来管理数据库的更新，比如说 EF Core，自动生成对应的更新数据来更新数据库。或者也有一些库可供使用，比如 DbUp 或者 FluentMigrator 可以让你以代码或者 SQL 脚本的方式定义数据库变更并自动的应用这些变更。&lt;/p&gt;
&lt;p&gt;这些方式都类似的会记录那些已经应用在数据库中的更新，哪些还没有更新，并执行那些待更新的部分将变更部署到数据库里面，但是这些工具无法控制的一点就是什么时候去部署变更。&lt;/p&gt;
&lt;h2 id=&#34;简单的-schema-更新策略&#34;&gt;简单的 schema 更新策略&lt;/h2&gt;
&lt;p&gt;什么时候进行数据库 schema 变更这个问题看起来好像有个很明确的答案：在新应用运行之前。好了，实际应用起来就会发现，这对于大型的网页服务场景将会变得异常困难，特别是应用有很多实例的时候。&lt;/p&gt;
&lt;p&gt;一个简单常见的方式可能是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;将流量切换到备份站点&lt;/li&gt;
&lt;li&gt;停止应用&lt;/li&gt;
&lt;li&gt;部署数据库 schema 更新&lt;/li&gt;
&lt;li&gt;将流量切换回主站点&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;很明显这对于当前 7*24 的商业世界来说，将不会是一个可接受的方案。如果是一个简单的没什么流量的项目，它可能可以满足你的要求。这只是绕开了问题而没有根本的解决这个问题，因此我不会具体再深入讨论这个方案。&lt;/p&gt;
&lt;p&gt;接下来探讨的方案会适用于任何网页服务的场景，但是我会主要使用 Kubernetes 的概念和例子，因为这是关于 Kubernetes 的文章。&lt;/p&gt;
&lt;p&gt;如果你不打算使用这些方案，但是又想达到零停机的效果，那你需要意识到在数据库变更发生的期间，应用代码会访问到不同版本的数据库实例。&lt;/p&gt;
&lt;h2 id=&#34;安全的执行数据库-schema-更新&#34;&gt;安全的执行数据库 schema 更新&lt;/h2&gt;
&lt;p&gt;让我们看一个更加实用的例子，假设你运行一个博客平台，打算给 BlogPost 加一个 Category 分类功能，每个博文会分配到一个唯一的上层分类，因此需要添加一个包含了分类列表的 categories 表，然后在 blog_posts 表里添加一个 category_id 列，引用 categories 表。&lt;/p&gt;
&lt;p&gt;现在让我们考虑怎么部署，我们的应用会有两种状态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Category 代码添加前&lt;/li&gt;
&lt;li&gt;Category 代码添加后&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对应的数据库也会有两种状态：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;未创建 categories 表和 category_id 列&lt;/li&gt;
&lt;li&gt;已创建 categories 表和 category_id 列&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那如何应用数据库迁移就有了两种选择：&lt;/p&gt;
&lt;p&gt;部署新应用，然后执行数据库变更。这样应用的第一种状态就只需要处理数据库的第一种状态，但是第二种状态的应用则需要处理混合两种状态的数据库&lt;/p&gt;
&lt;p&gt;执行数据库变更，然后部署新应用。这样应用的第一种状态需要处理数据库的两种状态，但是第二种状态的应用则可以确保所需的表已经添加。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-数据库迁移&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./pics/database-migrations.png&#34; alt=&#34;数据库迁移&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      数据库迁移
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;以上两种方式都适用于数据库变更，但是我偏向于后者。因为数据库的变更通常是整个部署流程中最麻烦的一步，因此我希望能优先处理这一步，如果发生什么问题，整个部署流程会终止，那新版本的应用也就不会部署上去了。&lt;/p&gt;
&lt;p&gt;实际操作中，数据库新的 schema 应该是可以适配于旧版本的应用，也就是说你不能给已存在的表插入 NOT NULL 的列，因为应用并不知道哪一列会无法插入。所以要不就把新加的列设置为必须项，这样在接下来的部署中就会强制插入数据，或者配置一个默认值，来确保这一列是非空的。&lt;/p&gt;
&lt;p&gt;总的来说，只要你了解了这些方法，数据库的变更不应该会给你带来很大的麻烦。当准备数据库变更的时候，问自己 &lt;strong&gt;”如果只是执行数据库的数据变更，会不会破坏现有的应用运行？“&lt;/strong&gt;， 如果这个问题的答案是不会的话，那这次变更就不会有什么麻烦。&lt;/p&gt;
&lt;h2 id=&#34;什么时候执行数据库迁移&#34;&gt;什么时候执行数据库迁移&lt;/h2&gt;
&lt;p&gt;现在我们同意在新应用启动之前执行数据库的数据更新，但是这还是不够具体清晰，因为我看到了三种不同的做法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用启动时执行数据库 schema 更新&lt;/li&gt;
&lt;li&gt;将数据库 schema 更新作为部署流程的一部分&lt;/li&gt;
&lt;li&gt;使用 Kuberentes Job 和 初始化容器（init container）来执行数据库 schema 更新&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每种做法都各有利弊，接下来让我们逐个讨论。&lt;/p&gt;
&lt;p&gt;让我们接着上面的例子，假设你的博客应用部署在 Kubernetes 里面，你会有一个 ingress，一个 service 和一个 deployment，就像我在&lt;a href=&#34;https://andrewlock.net/deploying-asp-net-core-applications-to-kubernetes-part-1-an-introduction-to-kubernetes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这篇文章&lt;/a&gt;提到过的。Deployment 会确保在任何时候都有三个应用的副本来处理网页的流量。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-数据库引擎&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./pics/blog-engine.png&#34; alt=&#34;数据库引擎&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      数据库引擎
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;你需要在部署新版本应用的时候，执行数据库 schema 更新，让我们分析一下几个不同方案。&lt;/p&gt;
&lt;h3 id=&#34;应用启动时执行数据库-schema-更新&#34;&gt;应用启动时执行数据库 schema 更新&lt;/h3&gt;
&lt;p&gt;第一个方案是在应用启动的时候执行数据库 schema 更新，可以通过调用 EF Core 的 &lt;code&gt;DbContext.Database.Migrate()&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kd&#34;&gt;public&lt;/span&gt; &lt;span class=&#34;kd&#34;&gt;static&lt;/span&gt; &lt;span class=&#34;kt&#34;&gt;void&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;Main&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;string&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[]&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;host&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;CreateHostBuilder&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;args&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;).&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Build&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;using&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Services&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;CreateScope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;())&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;var&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;db&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scope&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;ServiceProvider&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;GetRequiredService&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ApplicationDbContext&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;();&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;db&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Database&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Migrate&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// apply the migrations
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;    &lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;na&#34;&gt;Run&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;();&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;// start handling requests
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;虽然这个方案很简单，但是它却有一些重要的问题没解决：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用的每个副本都会尝试去应用同样的数据库 schema 更新&lt;/li&gt;
&lt;li&gt;应用有足够的权限对数据库做摧毁性的改动&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;第一点是一个很明显的问题，如果我们有三个副本，当我们尝试滚动更新应用的时候会导致的结果就是几个应用同时去修改数据库，这个在我所知道的工具中并不是一个支持的功能，而且伴随而来的是数据崩溃的风险。&lt;/p&gt;
&lt;p&gt;当然并非没有办法去确认只有一个副本会修改数据库，但是我并不打算介绍任何一种，因为我不愿意因为任何一种原因导致应用有机会破坏我的数据库。&lt;/p&gt;
&lt;p&gt;第二点其实是安全相关的，数据库迁移是一项很危险的操作，因为它有机会导致数据丢失或者毁坏。比较安全的做法是日常应用运行时跟数据库迁移时使用不同的数据库账户， 这样能减少应用运行时意外（或者恶意）的数据丢失的发生机率。 如果应用没有权限执行 &lt;code&gt;DROP&lt;/code&gt; 表的动作，那就几乎没有这种意外发生的机会了。&lt;/p&gt;
&lt;p&gt;如果你的应用除了运行业务代码外还要执行数据库变更的操作，那无法避免的是它一定需要特殊权限。这时候最好的做法就是确保只有在需要执行数据库 schema 更新的时候才使用那些特权账号，当然最最安全的做法就是不允许它使用特权账户。&lt;/p&gt;
&lt;h3 id=&#34;将数据库-schema-更新作为部署流程的一部分&#34;&gt;将数据库 schema 更新作为部署流程的一部分&lt;/h3&gt;
&lt;p&gt;另外一个常见的做法是将数据库 schema 更新作为部署流程的一部分， 比如说 Octopus Deploy 的部署方案。它解决了我上面描述的两个问题，能确保 Octopus 不会并发的去修改数据库 schema, 而且当数据库的变更完成后，才会部署应用的代码。因为这两个部署的步骤被拆分开了，使得不同的权限账户的使用变得更加方便容易。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-octopus-部署&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./pics/deployment-process.png&#34; alt=&#34;Octopus 部署&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Octopus 部署
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;可能大家会问，Octopus 是如何执行数据库的 schema 更新了，怎么确认这些更新完成了呢？上面的例子里，Octopus 需要一个更新脚本并直接执行这个脚本。好的一面是这样很容易执行你的更新任务，而且不需要在你的应用代码里去解决这些更新的问题。但是它却让你高度依赖 Octopus ，这可能会是你不想看到的一点。&lt;/p&gt;
&lt;p&gt;实际上，我自己实用的办法是使用一个简单的 .NET Core CLI 应用作为数据库工具来使用，或者说我简直把它当成一个数据库通用工具来使用，比如说使用 Jeremy D Miller 的 Oakton, 这样甚至可以利用来执行一些常见命令。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这篇文章会比较冗长，因此我并不打算在这里解释如何写一个类似的 CLI 工具， 而且这是一个比较简单的 .NET Core 的命令行项目，应该不会有太多麻烦的地方。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;如果你采取这种方式，那还有一个问题需要解决，就是在哪里运行这个 CLI 工具？ 你的 CLI 工具应该会运行在某台机器上面，因此你在准备部署的时候也需要考虑到这方面的需求。Serverless 可能不太合适，因为通常数据库的数据操作会是耗时比较长的动作。&lt;/p&gt;
&lt;p&gt;你看，Octopus 可以比较好的解决这个问题，如果你刚好正在使用这个项目，很明显，它将会是你的首选方案了。&lt;/p&gt;
&lt;p&gt;接下来是一个完全基于 Kubernetes 的方案，也就是我现在采用的方案。&lt;/p&gt;
&lt;h3 id=&#34;使用-kuberentes-job-和初始化容器来执行数据库-schema-更新&#34;&gt;使用 Kuberentes Job 和初始化容器来执行数据库 schema 更新&lt;/h3&gt;
&lt;p&gt;我喜欢的 Kubernetes 原生的解决方案，使用 Job 和初始化容器。&lt;/p&gt;
&lt;h4 id=&#34;job&#34;&gt;Job&lt;/h4&gt;
&lt;p&gt;Kubernetes Job 会运行一个或者多个 pod 来处理工作负载，如果 pod 出错了，可以重试执行，如果 pod 正常结束，则 Job 顺利完成。&lt;/p&gt;
&lt;p&gt;这正是我们想要用来做数据库数据更新的办法，创建一个 job 来执行数据库的 CLI 工具，出现网络问题时可以有选择的重试。因为它是一个 Kubernetes 原生概念，因此我们可以利用 Helm 的功能來模板化管理，将它包含在我们应用的 helm chart 里面。 从下面的图片可以看到，我给 test-app 添加了一个 CLI 项目，主要是一个 job.yaml。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-helm-chart&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./pics/charts.png&#34; alt=&#34;Helm chart&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Helm chart
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这个方案难在如何在应用启动之前确保 Job 已经完成，我是通过使用 Kubernetes 的初始化容器來解決这个问题的。&lt;/p&gt;
&lt;h4 id=&#34;初始化容器-init-container&#34;&gt;初始化容器 (Init Container)&lt;/h4&gt;
&lt;p&gt;如果你还记得这一系列文章里的第一篇，我讲过 Pod 是 Kubernetes 里包含一个或者多个容器的最小的调度单位。大部分情况下，Pod 会有一个主容器提供应用的主要功能，可能还会有一个或者多个 sidecar 容器提供一些附加的功能，比如说 metrics 或者 service mesh。&lt;/p&gt;
&lt;p&gt;我们也可以选择在 Pod 里使用初始化容器，这样 Kubernetes 部署 pod 的时候，就会先运行所有的初始化容器。只有当所有的初始化容器都正常运作并退出时，主容器才会开始运行，通常这样的功能会被用于下载或者做一些主容器要求的预先配置，这样可以让你的主容器只负责运行业务代码而不须关注环境配置。&lt;/p&gt;
&lt;h4 id=&#34;使用-job-和初始化容器来处理数据库数据更新&#34;&gt;使用 Job 和初始化容器来处理数据库数据更新&lt;/h4&gt;
&lt;p&gt;当我刚开始探索初始化容器的时候，我尝试过直接使用初始化容器来负责数据库数据更新，但是这又碰到了那个问题，在应用运行的时候无法避免的出现了同时间几个应用副本都去修改数据库的问题。因此我决定使用两步走的方案：使用初始化容器来延迟主容器启动直至确认数据库迁移的 Job 顺利完成。&lt;/p&gt;
&lt;p&gt;主要步骤看起来像是这样：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;应用的 Helm chart 包含了应用的部署和数据库数据更新的 Job。&lt;/li&gt;
&lt;li&gt;每个应用的 Pod 都会包含一个初始化容器，这个初始化容器会一直休眠直至相关的 Job 成功完成。当初始化容器发现 Job 完成时，它自己会正常退出，这样主容器才会开始运行。&lt;/li&gt;
&lt;li&gt;作为滚动更新的一部分，数据库迁移的 Job 一旦部署会马上执行，新版本的应用也会被创建出来但是不会运行，因为数据库数据更新的 Job 还在运行，初始化容器也就还处于休眠状态中，这样新版本的 Pod 就会被推迟运行。而且旧版本的 Pod 不会受到影响，还能继续处理业务流量。&lt;/li&gt;
&lt;li&gt;当数据库数据更新完成时，Job 就会正常结束。&lt;/li&gt;
&lt;li&gt;当初始化容器发现相关的 Job 正常结束了，它自己也就会退出。这样应用的容器才会开始启动并处理业务流量。&lt;/li&gt;
&lt;li&gt;旧版本的 pod 会根据滚动更新的策略被移除。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-job-init&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./pics/jobs-and-init-containers-1.png&#34; alt=&#34;Job init&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Job init
    &lt;/figcaption&gt;&lt;/figure&gt;

















&lt;figure  id=&#34;figure-job-int&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./pics/jobs-and-init-containers-2.png&#34; alt=&#34;Job int&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Job int
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;就像我说过的，这个方案我已经成功的使用了好几年，因此我觉得它是一个合适的解决方案。我在下一篇文章会详细的解释如何实施这个方案。&lt;/p&gt;
&lt;p&gt;在我们完成本次探讨之前，我想再聊聊最后的一个做法：使用 Helm Chart Hooks。&lt;/p&gt;
&lt;h4 id=&#34;helm-chart-hooks&#34;&gt;Helm Chart Hooks&lt;/h4&gt;
&lt;p&gt;字面上来看，&lt;a href=&#34;https://helm.sh/docs/topics/charts_hooks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Helm Chart Hooks&lt;/a&gt; 看起来好像可以实现我们想要的东西，它可以让我们在执行主要部署任务时，提前运行一个 Job 作为部署这个 helm chart 的一部分。&lt;/p&gt;
&lt;p&gt;在我第一次尝试使用这个方法时，这正就是我的做法。 我们可以将一个 Kubernetes 的 Job 通过添加 annotation 来转化成为 Helm Hook，看下面这个例子：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;batch/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Job&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;{{ .Release.Name }}&amp;#34;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# This is what defines this resource as a hook&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;&amp;#34;helm.sh/hook&amp;#34;: &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;pre-install&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;只要添加上这一行就能确保 Helm 不会将这一部分作为主要的安装或者升级流程，相反，它会在部署主应用之前先运行 job ，并等待这个 job 完成。只要这个 job 顺利完成， 这整个 chart 就会开始安装，并对你的应用执行滚动更新。如果 Job 失败了，这个 chart 不会继续安装，正在运行的应用也不会受影响。&lt;/p&gt;
&lt;p&gt;我刚开始测试这个方案的时候是可以的，但是有个意外。 如果数据库的更新耗时较长超过了 Helm 的 timeout 时间， Helm 会 timeout退出，这样整个部署就不会继续，但是并不知道数据库迁移到底成功了还是失败了。&lt;/p&gt;
&lt;p&gt;事实上生产环境通常有更多的数据量更更高的负载，因此对于数据库的变更很大机率会变得很慢，结果会导致上面的意外发生，所以最终我们还是选择了前面描述的初始化容器的方式。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我也有段时间没有了解过 Helm Chart Hooks 了，有可能这个问题已经解决了，但是我并没有在文档中见到相关描述。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;这篇文章中我描述了应用部署在 kubernetes 的时候如何解决数据库数据更新的问题，也探讨了让数据库数据更新适应多版本应用的必要性和几种不同的解决方案。&lt;/p&gt;
&lt;p&gt;我推荐的方案是创建一个 CLI 工具负责执行数据库数据更新，并以 Kubernetes Job 的形式运行。另外，在应用的每个 Pod 添加一个初始化容器，确保 Job 成功完成才启动应用容器。&lt;/p&gt;
&lt;p&gt;这篇文章只是探讨了一些概念上的东西，在接下来的文章中我会仔细描述如何实施，如何以 Helm Chart 的形式部署一个 Kubernetes Job 并使用初始化容器来控制应用容器的启动流程。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>如何使用 Istio 处理亲和性网络请求</title>
      <link>https://cloudnative.to/blog/request-affinity-with-istio/</link>
      <pubDate>Tue, 13 Oct 2020 23:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/request-affinity-with-istio/</guid>
      <description>&lt;p&gt;本文译自 &lt;a href=&#34;https://cashapp.github.io/2020-08-04/request-affinity-with-istio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Request affinity with istio&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;背景介绍&#34;&gt;背景介绍&lt;/h2&gt;
&lt;p&gt;很多 Cash App 的应用都会有一系列运行在 AWS 的 Kubernetes 集群上的分布式服务。们的工程团队也在几年前开始把应用迁移到 Kubernetes 上面， 最近我们才开始使用 Istio 作为一种服务网格的解决方案来解决我们服务之间的通信。在这篇文章里，我会专注在 Istio 接管流量负载均衡这一功能是如何大幅度的提高我们的服务的性能和稳定性。&lt;/p&gt;
&lt;p&gt;服务网格以解决服务发现以及服务通信的问题见长，而其中一个吸引我们的功能就是如何将流量负载均衡分配到服务的 Pod 上面。在使用 Istio 之前，我们给每一个服务安放了一个它专用的 ELB 来负载分发流量。使用了 Istio 后，负载分发流量的任务就被下发至每个 Pod，不仅从网络流量中移除了负载均衡器，还提供了更加丰富的负载均衡配置功能。&lt;/p&gt;
&lt;h2 id=&#34;问题所在&#34;&gt;问题所在&lt;/h2&gt;
&lt;p&gt;我们在 AWS 环境中部署了一套 Kafka 集群，用于异步的消息订阅和发布。另外还使用了一套 Kotlin 服务用于连接传统服务，evently-cloud，座落在传统服务跟 Kafka 之间，它暴露出一些 API 用于根据指定偏移量获取 Kafka 消息。&lt;/p&gt;
&lt;p&gt;Kafka 客户端需要数秒钟时间来初始化并定位到 Kafka 主题中的指定偏移处。但是我们并不希望所有 API 调用都需要耗费数秒时间，因此，使用 evently-cloud 为每个消费者提前主动地缓存 Kafka 消息。为了保证这个缓存被高效使用，同一个主题和消费者的请求必须是由相同的 pod 来处理。也就是说，请求基于对应的 kafka 主题和特定的 pod 有一定的亲和性需求，而 pod 跟主题之间的亲和性问题则是由 evently-cloud 的代码解决的。在以前，当收到一个请求时，evently-cloud 会使用连续的哈希来确认哪些 pod 负责对应的主题，然后调用那些 pod 来获取 Kafka 数据，因为 evently-cloud 有很多的 pod，因此大部分的请求都是由两个不同的 pod 负责处理完成的：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ldquoistio-beforerdquo&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./images/istio-before.png&#34; alt=&#34;&amp;amp;ldquo;istio-before&amp;amp;rdquo;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &amp;amp;ldquo;istio-before&amp;amp;rdquo;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;实际上，这就会导致大范围的稳定性问题，其中主要问题就是会导致问题层层蔓延下去。由于一个请求刚好落到正确的 pod 上的几率是非常低的，因此很多 pod 浪费了大量的资源用于转发请求。而且性能调优又非常困难。更麻烦的是，当一个 pod 降级的时候会导致调用它的 pod 也跟着受牵连最终也降级，出于这些原因，evently-cloud 是非常脆弱并且难以使用的。&lt;/p&gt;
&lt;h2 id=&#34;解决办法将亲和性需求交给-istio&#34;&gt;解决办法：将亲和性需求交给 Istio&lt;/h2&gt;
&lt;p&gt;Istio 配置如何将请求负载均衡分配给其他服务，并且提供不同的负载均衡规则。默认情况下，Istio 使用 round robin 策略来平均的将请求分配给 pod，这个是可以根据不同服务配置不同规则的。&lt;/p&gt;
&lt;p&gt;当我们从 evently-cloud 迁移到 Istio 时，我们对 Istio 进行了一些配置使得客户端使用确定性一致哈希算法。这些配置可以让客户端给每个请求配置一个可用于确认哪些 pod 会接收到这个请求的 HTTP 头。像以前一样，pod 的亲和性还是依赖于连续的哈希算法，但是现在哈希是在基础架构层解决的而不是在应用代码层面。&lt;/p&gt;
&lt;p&gt;Istio 大部分的配置都是通过 Kubernetes 的 CRD 完成的，它也提供了一个概念叫 DestinationRule 来声明特定服务被调用时要触发什么动作。因此我们在 Kubernetes 集群里应用了以下的 YAML 来实现我们的需求：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;networking.istio.io/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;DestinationRule&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;evently-cloud-deterministic-routing&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;evently-cloud&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;host&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;evently-cloud.evently-cloud.svc.cluster.local&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;exportTo&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;*&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;trafficPolicy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;loadBalancer&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;consistentHash&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;httpHeaderName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;x-evently-topic-key&amp;#39;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;你看，这就是我们在 Istio 上做的配置，不需要任何应用代码的变动甚至重启。&lt;/p&gt;
&lt;p&gt;既然已经由 Istio 来决定并路由请求了，我们更新了 evently-cloud 让它碰到新的 Http 头时不要再转发到其他节点，这样也可以帮我们减少了一些复杂的代码。为了可以安全的实现这一机制，我们在 event-cloud 及其调用者中添加了各种功能标志，以在需要时允许旧的请求转发行为。&lt;/p&gt;
&lt;p&gt;随着请求转发的需求移除了，大部分的请求跳动也就随着移除了：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ldquoistio-afterrdquo&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./images/istio-after.png&#34; alt=&#34;&amp;amp;ldquo;istio-after&amp;amp;rdquo;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &amp;amp;ldquo;istio-after&amp;amp;rdquo;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;结果&#34;&gt;结果&lt;/h2&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ldquoistio-rolloutpngrdquo&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;./images/istio-rollout.png&#34; alt=&#34;&amp;amp;ldquo;istio-rollout.png&amp;amp;rdquo;&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &amp;amp;ldquo;istio-rollout.png&amp;amp;rdquo;
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;在这次变更之后，我们发现性能跟可用性都有了大幅的提升。&lt;/p&gt;
&lt;p&gt;没有了请求转发的开销，我们在不影响 SLA 的情况下，将 evently-cloud 的 pod 数量从 100 个减少到 15 个。在上面的图形里，蓝线表示我们手动小幅度的减少 pod 的数量，相应的每个 pod 负责处理的请求数量也就上升了（粉红色线）。&lt;/p&gt;
&lt;p&gt;这次变更不仅带来了更好的资源利用率，也更好的隔离了 pod 错误。降级的 pod 只会影响它对应的主题，不再会影响到整个 evently-cloud 集群了。&lt;/p&gt;
&lt;p&gt;这篇文章中描述的 Istio 接管路由请求的方法只是我们在使用 Istio 中发现的诸多好处中的一个例子，其他亮点包括请求负载在服务 pod 之间的分发，使用 Istio Envoy sidecar 做 TLS 会话终结比默认的 JVM 配置更具性能优势，将 TLS 证书管理交给 Istio 等等。&lt;/p&gt;
&lt;p&gt;希望您会喜欢学习 Istio 并用于解决实际问题。如果您喜欢处理基础架构问题，我们 Cash App 正在招聘哦！&lt;/p&gt;
&lt;p&gt;致谢：我的同事 Ryan Hall 帮忙研究并实施这些变更，Jan Zantinge 协助我处理 Istio 项目迁移。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
