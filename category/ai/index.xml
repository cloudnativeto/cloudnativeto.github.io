<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI | 云原生社区（中国）</title>
    <link>https://cloudnativecn.com/category/ai/</link>
      <atom:link href="https://cloudnativecn.com/category/ai/index.xml" rel="self" type="application/rss+xml" />
    <description>AI</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Tue, 22 Apr 2025 15:00:00 +0800</lastBuildDate>
    <image>
      <url>https://cloudnativecn.com/media/sharing.png</url>
      <title>AI</title>
      <link>https://cloudnativecn.com/category/ai/</link>
    </image>
    
    <item>
      <title>深入解析 Gateway API Inference Extension（推理扩展）</title>
      <link>https://cloudnativecn.com/blog/gateway-api-inference-extension-deep-dive/</link>
      <pubDate>Tue, 22 Apr 2025 15:00:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/gateway-api-inference-extension-deep-dive/</guid>
      <description>&lt;p&gt;在 Kubernetes 上运行 AI 推理工作负载具有一些独特的特点和挑战，Gateway API Inference Extension 项目旨在解决其中的一些问题。我最近在 &lt;a href=&#34;https://kgateway.dev/blog/smarter-ai-reference-kubernetes-gateway-api/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kgateway 项目&lt;/a&gt; 中写过关于这些新能力的文章，而本文将深入讲解其工作原理。&lt;/p&gt;
&lt;p&gt;大多数人将 Kubernetes 中的请求路由理解为基于 Gateway API、Ingress 或 Service Mesh（统称为 L7 路由器）的机制。这些实现的原理类似：你定义一些根据请求属性（如 header、path 等）进行匹配的路由规则，L7 路由器会基于这些规则决定请求应发送到哪个后端，并使用某种负载均衡算法（如 &lt;a href=&#34;https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/upstream/load_balancing/load_balancing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;轮询、最少请求、环哈希、区域感知、优先级&lt;/a&gt; 等）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-负载均衡算法&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;负载均衡算法&#34; srcset=&#34;
               /blog/gateway-api-inference-extension-deep-dive/f0_hu_23d316aa77a75702.webp 400w,
               /blog/gateway-api-inference-extension-deep-dive/f0_hu_eedd08f6f7565af.webp 760w,
               /blog/gateway-api-inference-extension-deep-dive/f0_hu_e74b667536db7155.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/gateway-api-inference-extension-deep-dive/f0_hu_23d316aa77a75702.webp&#34;
               width=&#34;760&#34;
               height=&#34;517&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      负载均衡算法
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;然而，传统的负载均衡算法并不适合 AI/LLM 模型后端。与典型的无状态 Web API 不同，基于 GPU 的 LLM 运行方式特殊，处理方式不当将导致资源浪费与高成本。如果我们能够利用模型和 GPU 的实时指标做出更智能的路由与负载均衡决策，会怎样？例如，当某个后端 LLM 已加载某个特定的 LoRA 微调适配器时，相关请求应该优先路由至该实例，以避免其他实例因动态加载适配器而浪费 GPU 时间。再如，当某个后端请求队列已积压过多，请求继续发送只会拖慢响应。如果所有后端都已饱和，是否可以启用“负载丢弃”机制，保障系统稳定？&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-实现负载丢弃&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;实现负载丢弃&#34; srcset=&#34;
               /blog/gateway-api-inference-extension-deep-dive/f1_hu_61f7f8878282ae88.webp 400w,
               /blog/gateway-api-inference-extension-deep-dive/f1_hu_7626eca0698c0896.webp 760w,
               /blog/gateway-api-inference-extension-deep-dive/f1_hu_44d7d76e07fcd2ad.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/gateway-api-inference-extension-deep-dive/f1_hu_61f7f8878282ae88.webp&#34;
               width=&#34;760&#34;
               height=&#34;500&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      实现负载丢弃
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这正是 &lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Gateway API Inference Extension&lt;/a&gt; 项目所实现的功能。它引入了两个新的 Kubernetes CRD：&lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/concepts/api-overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InferenceModel 和 InferencePool&lt;/a&gt;，并提出了“endpoint picker（端点选择器）”的概念，可以扩展 L7 路由能力。该选择器可以结合底层 LLM 的指标做出更合理的路由决策，支持项目如 &lt;a href=&#34;https://kgateway.dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;kgateway&lt;/a&gt; 和 &lt;a href=&#34;https://istio.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio&lt;/a&gt; 的集成。&lt;/p&gt;
&lt;h2 id=&#34;推理扩展如何扩展-gateway-api&#34;&gt;推理扩展如何扩展 Gateway API&lt;/h2&gt;
&lt;p&gt;Gateway API Inference Extension 引入了两个新的自定义资源定义（CRD）：&lt;code&gt;InferenceModel&lt;/code&gt; 和 &lt;code&gt;InferencePool&lt;/code&gt;。结合端点选择器使用，可将现有 L7 路由架构升级为“推理网关”，以支持自托管的大模型/生成式 AI（GenAI）服务，采用“模型即服务”的架构理念。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/api-types/inferencemodel/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InferenceModel CRD&lt;/a&gt; 主要面向 AI 工程师，用于定义逻辑模型的推理入口。它支持将用户可见的模型名映射到实际后端模型，并支持在多个微调模型之间进行流量切分。例如，你想将模型 &lt;code&gt;llama2&lt;/code&gt; 提供给用户，而实际后端模型可能叫做 &lt;code&gt;vllm-llama2-7b-2024-11-20&lt;/code&gt; 或 &lt;code&gt;vllm-llama2-7b-2025-03-24&lt;/code&gt;，使用 InferenceModel 即可实现：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;inference.networking.x-k8s.io/v1alpha2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;InferenceModel&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;inferencemodel-llama2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;modelName&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;llama2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;criticality&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Critical&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;poolRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vllm-llama2-7b-pool&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;targetModels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vllm-llama2-7b-2024-11-20&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;75&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vllm-llama2-7b-2025-03-24&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;weight&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;此外，它还允许工作负载所有者为请求指定“重要性等级”，确保实时服务优先于批量处理任务。后续我们会看到该设置如何结合 LLM 实时指标进行调度。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/api-types/inferencepool/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InferencePool CRD&lt;/a&gt; 则面向平台运维人员，表示一组模型服务实例，是 AI 工作负载的后端服务。借助该 CR，可将 HTTPRoute 请求路由至一个推理实例池，并通过定义 &lt;code&gt;endpoint picker&lt;/code&gt; 实现根据实时指标（如请求队列长度、GPU 内存使用等）做出智能决策：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;inference.networking.x-k8s.io/v1alpha2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;InferencePool&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vllm-llama2-7b-pool&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;targetPortNumber&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;8000&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vllm-llama2-7b&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;extensionRef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vllm-llama2-7b-endpoint-picker&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;要实现从推理网关到 LLM 后端的流量转发，需定义一个将流量引导至 InferencePool 的 HTTPRoute：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;gateway.networking.k8s.io/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;HTTPRoute&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;llm-route&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;parentRefs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;inference-gateway&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;rules&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;backendRefs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;group&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;inference.networking.x-k8s.io&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;InferencePool&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;vllm-llama2-7b&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;matches&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;type&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PathPrefix&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;value&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;一旦请求到达推理网关，匹配规则后将转发至 InferencePool，此时请求会首先进入一个名为 “Endpoint Selection Extension”（ESE）的组件。这个 ESE 根据 LLM 的实时指标来选择具体的后端 Pod。我们来深入了解其工作机制。&lt;/p&gt;
&lt;h2 id=&#34;端点选择机制详解&#34;&gt;端点选择机制详解&lt;/h2&gt;
&lt;p&gt;当请求到达 Endpoint Selection Extension（ESE）时，它会从请求体中提取 &lt;code&gt;modelName&lt;/code&gt; 字段，目前该请求体需符合 &lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI API 格式&lt;/a&gt;。识别出模型名后，ESE 会比对现有的 InferenceModel 资源，定位对应的后端模型名或 LoRA 微调适配器。&lt;/p&gt;
&lt;p&gt;如请求中指定了 &lt;code&gt;llama2&lt;/code&gt;，ESE 会查找并选择对应的后端模型，如 &lt;code&gt;vllm-llama2-7b-2024-11-20&lt;/code&gt; 或 &lt;code&gt;vllm-llama2-7b-2025-03-24&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;选择后端的逻辑由一系列“过滤器”组成，ESE 会依次评估以下指标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;请求的关键性（Critical 或 Sheddable）&lt;/li&gt;
&lt;li&gt;LLM 的请求队列长度&lt;/li&gt;
&lt;li&gt;LoRA 适配器是否已加载&lt;/li&gt;
&lt;li&gt;KV 缓存占用率&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-评估流程图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;评估流程图&#34; srcset=&#34;
               /blog/gateway-api-inference-extension-deep-dive/f2_hu_1406b505e34593f0.webp 400w,
               /blog/gateway-api-inference-extension-deep-dive/f2_hu_83a55039c76a99a4.webp 760w,
               /blog/gateway-api-inference-extension-deep-dive/f2_hu_e51d8806c2ee7a9f.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/gateway-api-inference-extension-deep-dive/f2_hu_1406b505e34593f0.webp&#34;
               width=&#34;760&#34;
               height=&#34;362&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      评估流程图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;评估流程示意如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;判断请求是否为 Critical。&lt;/li&gt;
&lt;li&gt;如果是 Critical，请求只会进入队列长度 &amp;lt; 50 的实例。&lt;/li&gt;
&lt;li&gt;再判断是否已加载对应的 LoRA。&lt;/li&gt;
&lt;li&gt;若没有加载，也会尝试选择能加载的备选。&lt;/li&gt;
&lt;li&gt;如果是 Sheddable 请求，则只选择 KV 缓存占用率 &amp;lt; 80%、队列长度 &amp;lt; 5 的实例；否则请求会被丢弃。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-丢弃低优先级请求&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;丢弃低优先级请求&#34; srcset=&#34;
               /blog/gateway-api-inference-extension-deep-dive/f3_hu_e7fe980ddab58af.webp 400w,
               /blog/gateway-api-inference-extension-deep-dive/f3_hu_5bc828615308e30.webp 760w,
               /blog/gateway-api-inference-extension-deep-dive/f3_hu_7e9a6378161623e2.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/gateway-api-inference-extension-deep-dive/f3_hu_e7fe980ddab58af.webp&#34;
               width=&#34;760&#34;
               height=&#34;504&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      丢弃低优先级请求
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;示例分析&#34;&gt;示例分析&lt;/h3&gt;
&lt;h4 id=&#34;示例1critical-请求--lora-适配器要求&#34;&gt;示例1：Critical 请求 + LoRA 适配器要求&lt;/h4&gt;
&lt;p&gt;假设有以下 Pod：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pod A: 队列=10，KV 缓存=30%，已加载 LoRA-X&lt;/li&gt;
&lt;li&gt;Pod B: 队列=5，KV 缓存=70%，未加载 LoRA-X&lt;/li&gt;
&lt;li&gt;Pod C: 队列=60，KV 缓存=20%，已加载 LoRA-X&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终选择：&lt;strong&gt;Pod A&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;示例2非-critical-请求&#34;&gt;示例2：非 Critical 请求&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pod A: 队列=6，KV=85%&lt;/li&gt;
&lt;li&gt;Pod B: 队列=4，KV=75%&lt;/li&gt;
&lt;li&gt;Pod C: 队列=7，KV=60%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终选择：&lt;strong&gt;Pod B&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;示例3critical-请求--所有队列都很高&#34;&gt;示例3：Critical 请求 + 所有队列都很高&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Pod A: 队列=70，KV=40%，有 LoRA-Y&lt;/li&gt;
&lt;li&gt;Pod B: 队列=80，KV=60%，有 LoRA-Y&lt;/li&gt;
&lt;li&gt;Pod C: 队列=65，KV=70%，无 LoRA-Y&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终选择：&lt;strong&gt;Pod A&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;
&lt;p&gt;Gateway API Inference Extension 项目为 Kubernetes 上运行的大模型和 GPU 提供了智能模型选择和负载均衡能力。在 GPU 资源紧缺、成本高昂的背景下，该项目可以大幅提升吞吐性能和资源利用率，并节省企业成本。你可以在 &lt;a href=&#34;https://gateway-api-inference-extension.sigs.k8s.io/performance/benchmark/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;项目网站&lt;/a&gt; 上查看最新性能基准测试数据。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>大语言模型是怎么工作的？通俗解释版</title>
      <link>https://cloudnativecn.com/blog/how-llms-work-explained-without-math/</link>
      <pubDate>Mon, 21 Apr 2025 16:40:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/how-llms-work-explained-without-math/</guid>
      <description>&lt;p&gt;我相信你也会同意，现在已经无法忽视&lt;strong&gt;生成式 AI&lt;/strong&gt;（Generative AI，简称 GenAI）了。关于大语言模型（Large Language Models，LLMs）的新闻铺天盖地。你很可能已经用过 &lt;a href=&#34;https://chat.openai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT&lt;/a&gt;，甚至一直把它开着当助手用。&lt;/p&gt;
&lt;p&gt;但很多人心中有一个基本疑问：这些模型看上去“很聪明”，这种“聪明”到底是从哪儿来的？&lt;/p&gt;
&lt;p&gt;这篇文章就是想用简单的方式、尽量不涉及复杂数学，来解释文本生成模型是如何工作的，帮助你把它们当作&lt;strong&gt;计算机算法&lt;/strong&gt;来理解，而不是神奇魔法。&lt;/p&gt;
&lt;h2 id=&#34;大语言模型到底在做什么&#34;&gt;大语言模型到底在做什么？&lt;/h2&gt;
&lt;p&gt;我们先来澄清一个很多人对大语言模型的误解。很多人以为这些模型是“会聊天”、“会答题”的，但实际上，它们唯一真正擅长的事情是：&lt;/p&gt;
&lt;p&gt;👉 &lt;strong&gt;给它一段文字，它就根据上下文“猜”下一个词（准确说是“Token”）是什么。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以，我们可以从“Token”这个核心概念开始揭开 LLM 的神秘面纱。&lt;/p&gt;
&lt;h3 id=&#34;什么是-token&#34;&gt;什么是 Token？&lt;/h3&gt;
&lt;p&gt;Token（词元）是大语言模型处理文本的最小单位。&lt;/p&gt;
&lt;p&gt;你可以大致把它当成是“词”，但实际上，Token 既可以是一个字母、一个词，也可以是一段词根、甚至是空格或标点。LLM 的目标是&lt;strong&gt;尽可能高效地表示文本&lt;/strong&gt;，所以不会总是按单词来切分。&lt;/p&gt;
&lt;p&gt;一个语言模型的全部 Token 列表就叫它的“词汇表”（vocabulary）。这个词汇表是通过一种叫做 &lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BPE（字节对编码）&lt;/a&gt; 的算法，从大量语料中训练出来的。&lt;/p&gt;
&lt;p&gt;举个例子，开源的 &lt;a href=&#34;https://github.com/openai/gpt-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-2&lt;/a&gt; 模型就使用了一个包含 &lt;strong&gt;50,257 个 Token&lt;/strong&gt; 的词汇表。&lt;/p&gt;
&lt;p&gt;每个 Token 都有一个唯一编号，模型会用一个叫做 &lt;strong&gt;tokenizer（分词器）&lt;/strong&gt; 的工具，把你输入的文字转换成 Token 编号列表。你也可以在 Python 中尝试这个过程：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install tiktoken
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后在 Python 中运行以下代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tiktoken&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tiktoken&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encoding_for_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt-2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;The quick brown fox jumps over the lazy dog.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 输出: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;464&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# &amp;#39;The&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2068&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# &amp;#39; quick&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# &amp;#39;.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Token &lt;code&gt;464&lt;/code&gt; 表示 &amp;ldquo;The&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Token &lt;code&gt;2068&lt;/code&gt; 表示 &amp;quot; quick&amp;quot;（注意前导空格也包括进去了）&lt;/li&gt;
&lt;li&gt;Token &lt;code&gt;13&lt;/code&gt; 表示句号 &amp;ldquo;.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于分词是算法决定的，有时候你会发现一些奇怪的现象：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;The&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# [464]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# [1169]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; the&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# [262]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同一个词，根据是否有大小写、是否有前导空格，会被编码成不同的 Token。&lt;/p&gt;
&lt;p&gt;此外，&lt;strong&gt;使用频率低的词&lt;/strong&gt;不会单独占用一个 Token，而是被拆成多个 Token。例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Payment&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# [19197, 434]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;19197&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;        &lt;span class=&#34;c1&#34;&gt;# &amp;#39;Pay&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;434&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;          &lt;span class=&#34;c1&#34;&gt;# &amp;#39;ment&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;预测下一个-token&#34;&gt;预测下一个 Token&lt;/h3&gt;
&lt;p&gt;正如前面提到的，语言模型的任务就是&lt;strong&gt;预测下一个 Token 是什么&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;假设你已经输入了：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;The&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; quick&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; brown&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; fox&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后运行伪代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_token_predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;The&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; quick&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; brown&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; fox&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这一步会返回一份&lt;strong&gt;概率分布表&lt;/strong&gt;，告诉你在这个上下文下，每个 Token 作为下一个词的概率是多少。&lt;/p&gt;
&lt;p&gt;以 GPT-2 为例，这个返回值就是一个包含 &lt;strong&gt;50,257 个浮点数&lt;/strong&gt; 的列表，每个数表示对应 Token 出现在下一个位置的概率。&lt;/p&gt;
&lt;p&gt;你可以想象，像 &amp;ldquo;jumps&amp;rdquo; 这样的词很可能被赋予较高概率，而像 &amp;ldquo;potato&amp;rdquo; 这样的无关词概率则接近 0。&lt;/p&gt;
&lt;p&gt;为了能做出这种预测，模型必须经过一个&lt;strong&gt;训练过程&lt;/strong&gt;：它读了大量的文本，从中学习“哪些词通常跟随哪些词”。&lt;/p&gt;
&lt;p&gt;最终它构建了一个复杂的数据结构，能根据输入 Token 序列预测下一个 Token 的概率。&lt;/p&gt;
&lt;p&gt;这和你原来想象的一样吗？现在你是不是开始觉得，这其实并没有那么神秘了？&lt;/p&gt;
&lt;h3 id=&#34;生成一段完整的文字&#34;&gt;生成一段完整的文字&lt;/h3&gt;
&lt;p&gt;因为语言模型每次只能预测一个下一个 token（词元），所以如果我们想要它生成一整段句子，唯一的办法就是&lt;strong&gt;通过循环多次调用模型&lt;/strong&gt;，每次生成一个新的 token，直到长度足够。&lt;/p&gt;
&lt;p&gt;每一次循环中，模型都会根据当前的输入预测下一个 token 的概率分布，然后从中选择一个 token，添加到输入序列的末尾，再继续下一轮预测。这就像接力一样，模型每次生成一个词，然后拿这个词继续接着往下写。&lt;/p&gt;
&lt;p&gt;我们来看一段更完整的伪代码（Python 风格）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;generate_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hyperparameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_token_predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;next_token&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;select_next_token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hyperparameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;next_token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;generate_text()&lt;/code&gt; 函数接受一个用户的输入提示（prompt），比如一句话或者一个问题。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tokenize()&lt;/code&gt; 是一个辅助函数，它会用如 &lt;code&gt;tiktoken&lt;/code&gt; 这样的工具把文本转成 Token 编号的列表。&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;for&lt;/code&gt; 循环中，每轮会调用 &lt;code&gt;get_token_predictions()&lt;/code&gt;，也就是实际调用 AI 模型，让它预测下一个 token 的概率。&lt;/li&gt;
&lt;li&gt;然后通过 &lt;code&gt;select_next_token()&lt;/code&gt; 这个函数，从这些概率中选出一个 token 来作为输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个选 token 的函数可以用多种策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最简单的是&lt;strong&gt;选择概率最高的那个&lt;/strong&gt;（在机器学习中称为“贪婪选择”/greedy selection）；&lt;/li&gt;
&lt;li&gt;更聪明的方法是&lt;strong&gt;用随机性来加点变化&lt;/strong&gt;：根据概率分布用随机数决定哪个 token 被选中，这样同一个 prompt 可以生成不一样的内容，增加“创造力”。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了进一步控制生成结果的风格，我们可以用一些**超参数（hyperparameters）**来影响 token 的选择过程。这些超参数是通过参数传给 &lt;code&gt;generate_text()&lt;/code&gt; 的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比如 &lt;code&gt;temperature&lt;/code&gt;（温度）参数：它会影响模型对“冷门”词的选择倾向。温度越高，概率分布会“拉平”，选择一些低概率 token 的可能性就会增加，生成结果就更有想象力。&lt;/li&gt;
&lt;li&gt;还有 &lt;code&gt;top_p&lt;/code&gt; 和 &lt;code&gt;top_k&lt;/code&gt;，用于控制模型考虑的“最可能的几个 token”，从中再做选择。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一旦选定了下一个 token，就会进入下一轮循环，把这个新 token 加入输入中，继续预测下一个 token，直到生成足够长度的内容。&lt;/p&gt;
&lt;p&gt;注意：这个过程不懂得“句子”或“段落”的概念，它只是一个词接一个词地往下预测。所以生成的文本可能会在句子中间突然结束。为了解决这个问题，&lt;code&gt;num_tokens&lt;/code&gt; 参数可以设置为“最多生成几个 token”，而不是固定数量。你还可以设置当生成出句号 token（如 &lt;code&gt;.&lt;/code&gt;）时就自动结束。&lt;/p&gt;
&lt;p&gt;如果你读到这里并理解了上述内容，恭喜你！你已经掌握了 LLM 的基本工作机制。如果你还想更进一步了解，那么接下来我们会进入稍微技术一点的内容（但依然尽量避免复杂数学）。&lt;/p&gt;
&lt;h2 id=&#34;模型是如何训练的&#34;&gt;模型是如何训练的？&lt;/h2&gt;
&lt;p&gt;谈到模型是怎么训练出来的，其实很难完全避开数学。不过这里我会用一个非常简单直观的例子来帮助你理解这个过程。&lt;/p&gt;
&lt;p&gt;因为语言模型的核心任务是“预测接下来可能出现的 token”，所以最基础的训练方式，就是去统计在训练语料中连续出现的 token 对（也就是“前一个词+下一个词”的组合），并建立一个概率表。&lt;/p&gt;
&lt;p&gt;我们从一个简单的例子开始：假设模型的词汇表中只有以下 5 个 token：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;I&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;you&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;like&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;apples&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;bananas&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;为了简化，我们这里不考虑空格和标点符号。&lt;/p&gt;
&lt;p&gt;训练语料由三句话组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I like apples&lt;/li&gt;
&lt;li&gt;I like bananas&lt;/li&gt;
&lt;li&gt;you like bananas&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在我们可以构建一个 5×5 的表格，行表示“前一个 token”，列表示“后一个 token”。我们统计每个组合在语料中出现的次数：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;I&lt;/th&gt;
          &lt;th&gt;you&lt;/th&gt;
          &lt;th&gt;like&lt;/th&gt;
          &lt;th&gt;apples&lt;/th&gt;
          &lt;th&gt;bananas&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;I&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;you&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;like&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;apples&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;bananas&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这个表表示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“I like” 出现了 2 次；&lt;/li&gt;
&lt;li&gt;“you like” 出现了 1 次；&lt;/li&gt;
&lt;li&gt;“like apples” 出现了 1 次；&lt;/li&gt;
&lt;li&gt;“like bananas” 出现了 2 次。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来我们把这张“频次表”转换成“概率表”。方法是：每一行中出现的次数加总后，按比例转换成概率。&lt;/p&gt;
&lt;p&gt;例如，&amp;ldquo;like&amp;rdquo; 这一行有两种后续词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;apples&amp;rdquo; 出现 1 次；&lt;/li&gt;
&lt;li&gt;&amp;ldquo;bananas&amp;rdquo; 出现 2 次；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总共 3 次，所以：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;apples&amp;rdquo; 的概率是 1/3 ≈ 33.3%；&lt;/li&gt;
&lt;li&gt;&amp;ldquo;bananas&amp;rdquo; 的概率是 2/3 ≈ 66.7%。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;于是最终的概率表如下（空格表示概率为 0%）：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;I&lt;/th&gt;
          &lt;th&gt;you&lt;/th&gt;
          &lt;th&gt;like&lt;/th&gt;
          &lt;th&gt;apples&lt;/th&gt;
          &lt;th&gt;bananas&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;I&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;100%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;you&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;100%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;like&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;33.3%&lt;/td&gt;
          &lt;td&gt;66.7%&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;apples&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;bananas&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;你可能注意到了，&amp;ldquo;apples&amp;rdquo; 和 &amp;ldquo;bananas&amp;rdquo; 的那两行看上去很奇怪 —— 因为在我们的训练数据中，它们后面没有再跟任何词，这就是训练数据中的“空洞”。&lt;/p&gt;
&lt;p&gt;为了让模型不会在遇到这些 token 时“卡住”，我采用了一种折中办法：给每个可能的下一个 token 都分配了相等的概率（即平均分配）。虽然这样做不一定准确，但至少能保证模型不会直接“失效”。&lt;/p&gt;
&lt;h3 id=&#34;关于训练数据空洞的思考&#34;&gt;关于“训练数据空洞”的思考&lt;/h3&gt;
&lt;p&gt;这个例子中的“空洞”很明显，在真实的大模型中可能不容易察觉。但即使是使用了超大规模语料的数据集，也还是可能存在一些训练不足的区域。&lt;/p&gt;
&lt;p&gt;这些“稀疏区域”的预测质量通常比较差，虽然看起来语句没问题，但可能会出现事实错误、逻辑不通等现象 —— 这就是所谓的 &lt;strong&gt;幻觉（hallucination）&lt;/strong&gt;，即模型生成的内容“像真的，但其实是假的”。&lt;/p&gt;
&lt;h3 id=&#34;模拟实现如何用这张概率表生成预测&#34;&gt;模拟实现：如何用这张概率表生成预测？&lt;/h3&gt;
&lt;p&gt;现在你已经有了这个概率表，那么语言模型内部的 &lt;code&gt;get_token_predictions()&lt;/code&gt; 函数其实就可以非常简单地实现了：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;get_token_predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;last_token&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;probabilities_table&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;last_token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;你传入一串 token（比如用户的 prompt），模型只会关注最后一个 token，然后返回它在概率表中对应的一行。&lt;/p&gt;
&lt;p&gt;例如输入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;you&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;like&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;函数就会读取 &amp;ldquo;like&amp;rdquo; 这一行，告诉你：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;apples&amp;rdquo; 的概率是 33.3%&lt;/li&gt;
&lt;li&gt;&amp;ldquo;bananas&amp;rdquo; 的概率是 66.7%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么在上一个伪代码中 &lt;code&gt;select_next_token()&lt;/code&gt; 函数就有 1/3 的概率选中 &amp;ldquo;apples&amp;rdquo;，2/3 的概率选中 &amp;ldquo;bananas&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;如果模型选择了 &amp;ldquo;apples&amp;rdquo;，生成的句子就是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;you like apples
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这是一句&lt;strong&gt;训练数据中没有&lt;/strong&gt;的句子，但它却是非常合理的。模型仅凭过去学到的词序关系，就能“合成”出一个看似全新的输出。&lt;/p&gt;
&lt;p&gt;你现在应该能理解：模型的“创造力”其实是通过把已学过的词汇和模式&lt;strong&gt;巧妙地拼接在一起&lt;/strong&gt;而来，这种方式虽然没有真正的“思考”，但效果却很惊艳。&lt;/p&gt;
&lt;h3 id=&#34;上下文窗口context-window&#34;&gt;上下文窗口（Context Window）&lt;/h3&gt;
&lt;p&gt;在上一节中，我用来训练迷你语言模型的方法，属于一种叫做&lt;a href=&#34;https://zh.wikipedia.org/wiki/%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%93%be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;马尔可夫链（Markov chain）&lt;/a&gt;的技术。&lt;/p&gt;
&lt;p&gt;这种方法的一个核心问题是：它**只考虑上一个 token（词元）**来做预测。也就是说，在选择下一个 token 时，之前出现的所有内容都会被“遗忘”。因此，我们可以说这种方法的上下文窗口是 &lt;strong&gt;1 个 token&lt;/strong&gt; —— 实在太小了。&lt;/p&gt;
&lt;p&gt;当上下文窗口这么小，模型就像失忆症一样，每次预测都只能看到最近的一个词，很难保持上下文的一致性或连贯性。&lt;/p&gt;
&lt;h3 id=&#34;如果增大上下文窗口呢&#34;&gt;如果增大上下文窗口呢？&lt;/h3&gt;
&lt;p&gt;一个办法是：&lt;strong&gt;扩展概率表，使用更长的 token 序列作为上下文&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若使用 2 个 token 做上下文（窗口大小为 2），就需要在概率表中添加所有可能的两词组合；&lt;/li&gt;
&lt;li&gt;用我前面举的例子（5 个 token），就需要再添加 5×5=25 行，再加上原本的 5 行，一共 30 行；&lt;/li&gt;
&lt;li&gt;模型训练时要学会每三个 token 的组合关系，才能预测下一个 token；&lt;/li&gt;
&lt;li&gt;运行时，每次都取最后两个 token 作为上下文，用来查找对应的概率分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不过，&lt;strong&gt;2 个 token 作为上下文依然远远不够&lt;/strong&gt;。想要生成更自然、上下文更一致的文本，我们需要更长的上下文窗口。否则，模型无法让新生成的 token 和之前的内容保持一致或有逻辑联系。&lt;/p&gt;
&lt;p&gt;那我们需要多大窗口呢？&lt;/p&gt;
&lt;p&gt;如果窗口从 2 提高到 3，那么就需要 5×5×5 = 125 个组合，表格就要增加 125 行。但效果仍然很有限。&lt;/p&gt;
&lt;h3 id=&#34;如果用马尔可夫链来实现-gpt-2-的上下文窗口&#34;&gt;如果用马尔可夫链来实现 GPT-2 的上下文窗口？&lt;/h3&gt;
&lt;p&gt;GPT-2 的上下文窗口是 &lt;strong&gt;1024 个 token&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果我们用马尔可夫链来实现这样的模型，就意味着我们要准备一个超大的概率表，每一行都代表一串最多 1024 个 token 的组合。&lt;/p&gt;
&lt;p&gt;举个例子，如果词汇表中只有 5 个 token，那么可能的 1024 长度的组合数量是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;pow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;55626846462680034577255817933310101605480399.&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;..&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;后面还有上百位&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这是一个&lt;strong&gt;天文数字级别的行数&lt;/strong&gt;，光是存这个表都需要超出地球所有硬盘的总容量！&lt;/p&gt;
&lt;p&gt;而且这还只是考虑长度为 1024 的组合。实际上我们还要支持长度为 1、2、3……1023 的组合，因为用户的输入可能并没有那么长。那整个表的规模会更大，完全不可行。&lt;/p&gt;
&lt;h3 id=&#34;马尔可夫链方法的极限&#34;&gt;马尔可夫链方法的极限&lt;/h3&gt;
&lt;p&gt;随着模型的发展：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-3 的上下文窗口提升到 2048；&lt;/li&gt;
&lt;li&gt;GPT-3.5 提升到了 4096；&lt;/li&gt;
&lt;li&gt;GPT-4 起步是 8192，后来又提升到 32K，甚至是 128K（没错，是 12.8 万个 token）；&lt;/li&gt;
&lt;li&gt;最新的研究中，甚至出现了支持 &lt;strong&gt;上百万 token 上下文窗口&lt;/strong&gt; 的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这意味着模型已经能够理解和关联更大段落之间的语义，生成的内容也更加连贯、一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;马尔可夫链&lt;/strong&gt;是一种很棒的思维工具，它让我们理解“预测下一个 token”的基本逻辑。但它&lt;strong&gt;不适合构建真正的大语言模型&lt;/strong&gt;，因为它在上下文长度、存储空间和可扩展性方面有根本性的瓶颈。&lt;/p&gt;
&lt;p&gt;为了处理大规模上下文，我们需要更智能、更高效的机制 —— 而这正是神经网络和 Transformer 架构真正擅长的领域。&lt;/p&gt;
&lt;h3 id=&#34;从马尔可夫链到神经网络&#34;&gt;从马尔可夫链到神经网络&lt;/h3&gt;
&lt;p&gt;显然，我们已经无法继续依赖“概率表”的方式了 —— 因为如果想支持较大的上下文窗口，那么这张表将变得大得离谱，根本无法存在于内存中。&lt;/p&gt;
&lt;p&gt;那我们该怎么办？&lt;/p&gt;
&lt;p&gt;答案是：&lt;strong&gt;用一个函数代替表格&lt;/strong&gt;。这个函数不再存储所有可能性，而是通过算法“算”出一个近似的概率分布 —— 这正是&lt;strong&gt;神经网络&lt;/strong&gt;擅长的事情。&lt;/p&gt;
&lt;p&gt;神经网络是一种特殊类型的函数，它：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;接收一些输入（比如 token）；&lt;/li&gt;
&lt;li&gt;对这些输入进行复杂的计算；&lt;/li&gt;
&lt;li&gt;输出一个结果 —— 在语言模型中，就是下一个 token 的概率分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为什么说它“特殊”？&lt;/p&gt;
&lt;p&gt;因为神经网络的计算行为不光由代码控制，还依赖于大量&lt;strong&gt;外部定义的参数（parameters）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这些参数在一开始是未知的、随机的，所以模型一开始的输出几乎毫无用处。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;训练的过程&lt;/strong&gt;，就是找到那些&lt;strong&gt;最适合的参数组合&lt;/strong&gt;，使得神经网络的预测结果尽可能地贴近训练数据中的真实情况。&lt;/p&gt;
&lt;p&gt;训练过程会反复进行多轮，每一轮对参数进行一点点微调，这个过程叫做&lt;a href=&#34;https://en.wikipedia.org/wiki/Backpropagation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;反向传播（Backpropagation）&lt;/a&gt;，它本质上是一个数学优化过程（本文不会深入讨论）。&lt;/p&gt;
&lt;p&gt;每次参数更新之后，模型会再次评估它在训练数据上的表现，再基于反馈继续调整参数。这个循环会持续进行，直到模型能在训练集上做出合理的预测为止。&lt;/p&gt;
&lt;p&gt;为了帮助你理解神经网络的规模，我们来看几个数据：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPT-2&lt;/strong&gt;：约 15 亿个参数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-3&lt;/strong&gt;：1750 亿个参数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-4&lt;/strong&gt;：据说达到了 1.76 万亿个参数！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练这种规模的神经网络，即使使用现代最强的计算资源，也需要花费数周甚至数月。&lt;/p&gt;
&lt;p&gt;由于神经网络中隐藏了如此多的参数，而且这些参数全是通过自动化迭代得出的，&lt;strong&gt;连开发者自己都很难完全理解模型内部是如何“思考”的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;一个训练完成的 LLM 更像是一个&lt;strong&gt;黑盒&lt;/strong&gt; —— 你看不到它的内部“逻辑”，只能观察它的输入输出。就连训练它的人也很难解释它为什么会给出某个答案。&lt;/p&gt;
&lt;h3 id=&#34;层layerstransformer-和注意力机制attention&#34;&gt;层（Layers）、Transformer 和注意力机制（Attention）&lt;/h3&gt;
&lt;p&gt;你也许会好奇：这些神经网络到底在内部做了些什么？它们是如何“把一堆 token 输入，经过参数的计算，然后输出合理的下一个 token 的概率”的？&lt;/p&gt;
&lt;p&gt;这就涉及到神经网络的“分层计算”结构。&lt;/p&gt;
&lt;p&gt;一个神经网络由多个称为**层（layer）**的计算模块组成。其基本流程是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;第一层接收输入；&lt;/li&gt;
&lt;li&gt;对输入做一次变换（数学运算）；&lt;/li&gt;
&lt;li&gt;把变换后的结果传给下一层；&lt;/li&gt;
&lt;li&gt;重复这个过程，直到最后一层生成输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;每一层都像一个“加工车间”，把输入“改造”一下，然后再传递下去。&lt;/p&gt;
&lt;p&gt;机器学习研究者会设计不同类型的层，用来处理不同的数据类型，比如图像或文本。有些层是通用的，有些则是专门为处理“token 化后的文本”设计的。&lt;/p&gt;
&lt;p&gt;目前在大语言模型中，最常用的神经网络架构叫做 &lt;strong&gt;Transformer&lt;/strong&gt;。这种架构非常适合处理序列数据（例如自然语言中的单词序列），它的核心机制是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;注意力机制（Attention）&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Attention 能帮助模型理解“在上下文中，哪些 token 比较重要”，并基于这种理解生成更合理的下一个词的概率分布。&lt;/p&gt;
&lt;p&gt;最早，这种机制是用在机器翻译中的，用来判断“源语言的句子中，哪些词在翻译中最关键”。它可以模拟人类阅读时“聚焦重点词语”的过程。&lt;/p&gt;
&lt;p&gt;今天，Attention 已成为 Transformer 架构的核心能力，使得 LLM 能够从长上下文中捕捉信息，从而生成连贯、逻辑一致的文本。&lt;/p&gt;
&lt;p&gt;如果你已经理解了这一部分，那么你就已经初步了解了大语言模型“背后的魔法”其实是&lt;strong&gt;数学 + 神经网络 + 算法工程&lt;/strong&gt;的结合成果。下一步，我们可以探讨这些模型是否真的“具备智能”。&lt;/p&gt;
&lt;h2 id=&#34;大语言模型真的有智能吗&#34;&gt;大语言模型真的“有智能”吗？&lt;/h2&gt;
&lt;p&gt;看到这里，你可能已经开始有自己的判断：LLM（大语言模型）在生成文本时，是否体现出某种“智能”。&lt;/p&gt;
&lt;p&gt;就我个人而言，我并不认为 LLM 具有真正的推理能力，或者能够产生原创性的思想。但这并不意味着它们没有价值。&lt;/p&gt;
&lt;p&gt;凭借对上下文中 token 的精妙计算，LLM 能够识别出用户提示（prompt）中的模式，并将其与训练时学到的相似模式进行匹配。它生成的文本，虽然大多是从训练数据中“拼拼凑凑”而来，但拼接的方式非常巧妙，&lt;strong&gt;看起来像是“自己想出来的”&lt;/strong&gt;，而且往往很有用。&lt;/p&gt;
&lt;p&gt;不过，考虑到 LLM 存在**幻觉（hallucination）**的倾向（即生成内容看起来合理但却是虚假的），我不建议把 LLM 的输出直接交给最终用户使用，除非事先经过人工审核和验证。&lt;/p&gt;
&lt;p&gt;那么，未来几个月或几年中出现的更大规模的 LLM，有可能具备真正的智能吗？&lt;/p&gt;
&lt;p&gt;我认为，&lt;strong&gt;至少在目前 GPT 架构的框架下，还很难达到真正意义上的智能&lt;/strong&gt; —— 因为它还有很多根本性的限制。但谁知道呢？也许未来的某些创新，真的能带我们走得更远。&lt;/p&gt;
&lt;h2 id=&#34;写在最后&#34;&gt;写在最后&lt;/h2&gt;
&lt;p&gt;感谢你一路读到这里！&lt;/p&gt;
&lt;p&gt;我希望这篇文章能够激起你继续深入学习的兴趣，哪怕最后你真的要面对那些“可怕的数学”，因为如果你想彻底理解 LLM 的每一个细节，那是绕不过去的。&lt;/p&gt;
&lt;p&gt;如果你准备好面对这些挑战，我强烈推荐 Andrej Karpathy 的视频课程：&lt;a href=&#34;https://karpathy.ai/zero-to-hero.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Networks: Zero to Hero&lt;/a&gt;。它是理解神经网络从入门到精通的绝佳起点。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>什么是 AI Agent？简要介绍与构建指南</title>
      <link>https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/</link>
      <pubDate>Fri, 18 Apr 2025 09:30:08 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/</guid>
      <description>&lt;p&gt;&lt;strong&gt;下一件大事？&lt;/strong&gt; Gartner 认为 AI Agent 将引领未来。OpenAI、Nvidia 和 Microsoft 都在下注，就连在 AI 领域一直比较低调的 Salesforce 也开始布局。&lt;/p&gt;
&lt;p&gt;这一趋势确实正在快速起飞。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f1_hu_1ae994004d0f1acd.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f1_hu_87c6eeec2f368995.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f1_hu_c678857b510e3897.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f1_hu_1ae994004d0f1acd.webp&#34;
               width=&#34;760&#34;
               height=&#34;260&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;“AI Agents” 在 Google Trends 上的搜索趋势（来源：trends.google.com）&lt;/p&gt;
&lt;h2 id=&#34;什么是-ai-agent关键词是-agency&#34;&gt;什么是 AI Agent？关键词是 “Agency”&lt;/h2&gt;
&lt;p&gt;不同于传统的生成式 AI 系统，Agent 不只是回应用户输入，而是能够&lt;strong&gt;自主处理一个复杂流程&lt;/strong&gt;，比如处理一个保险理赔请求：理解邮件内容、图像、PDF 文件，从客户数据库提取信息，比对条款、与客户沟通并等待对方回复（哪怕是几天后）——整个过程中不丢失上下文。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;它能自主完成这些操作&lt;/strong&gt;，无需人类实时干预。&lt;/p&gt;
&lt;h2 id=&#34;咖啡机与咖啡师的比喻&#34;&gt;咖啡机与咖啡师的比喻&lt;/h2&gt;
&lt;p&gt;相比 Copilot 这类工具“辅助”员工，&lt;strong&gt;AI Agent 更像是一位可以独立上岗的“数字员工”&lt;/strong&gt;，可实现高程度的流程自动化。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;想象一下&lt;/strong&gt;，一个 AI 能够承担当前由人类员工或整个部门完成的复杂任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;策划、设计、执行、评估并优化一场 &lt;strong&gt;营销活动&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;与物流公司、客户和仓库沟通以&lt;strong&gt;追踪遗失的货物&lt;/strong&gt;，若找不到可发起索赔&lt;/li&gt;
&lt;li&gt;每天&lt;strong&gt;监控商标注册数据库&lt;/strong&gt;，发现潜在冲突后自动提起异议&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;汇总 ESG 报告所需数据&lt;/strong&gt;，主动向员工发起询问并校验信息准确性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前的 AI 模型只能在流程中“协助”，而不能主导。&lt;strong&gt;AI Agent 则能完成整个流程的执行&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f2_hu_8ec62ac21f787994.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f2_hu_bb358adfeab39d73.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f2_hu_c4e31b9a6ec541c5.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f2_hu_8ec62ac21f787994.webp&#34;
               width=&#34;760&#34;
               height=&#34;472&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;如上图所示，传统生成式 AI 协助团队完成流程（黄色），AI Agent 能从头到尾执行整个任务（橙色）。图片来源：Maximilian Vogel&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果把传统模型比作高端咖啡机，Agent 则是咖啡师&lt;/strong&gt;。咖啡师不仅能做咖啡，还能招呼客人、点单、收银、洗杯子，甚至打烊关店。而咖啡机永远无法独自运营一家咖啡馆。&lt;/p&gt;
&lt;h2 id=&#34;为什么-agent-能胜任这些任务&#34;&gt;为什么 Agent 能胜任这些任务？&lt;/h2&gt;
&lt;p&gt;Agent 擅长在复杂流程中&lt;strong&gt;掌控多个子流程&lt;/strong&gt;，能自主判断下一步应该执行什么任务。如果遇到问题，它知道该向谁请求帮助（缺咖啡豆就找老板，机器故障就联系售后）。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f3_hu_12a53b71f5ffffc0.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f3_hu_93b06a84a453144f.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f3_hu_66a3072064743abe.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f3_hu_12a53b71f5ffffc0.webp&#34;
               width=&#34;614&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;AI Agent 与传统生成式 AI 的比较。图片来源：Maximilian Vogel&lt;/p&gt;
&lt;h2 id=&#34;一位-ai-员工的构成解剖&#34;&gt;一位 AI 员工的构成解剖&lt;/h2&gt;
&lt;p&gt;现在开始动手构建一个 AI Agent，基于上述保险理赔流程，我们来看每一步的设计。&lt;/p&gt;
&lt;p&gt;我们的目标是构建业务架构与流程设计。由于篇幅原因，本篇不涉及具体代码实现。&lt;/p&gt;
&lt;h3 id=&#34;第一步分类并进入处理通道&#34;&gt;第一步：分类并进入处理通道&lt;/h3&gt;
&lt;p&gt;流程从客户发送邮件报案开始。Agent 需要先&lt;strong&gt;分析邮件内容，识别客户的意图&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;一旦分类完成，Agent 会将请求路由至正确的处理通道。通常这远不止是 function calling，而是涉及流程路径的选择与多个步骤的执行。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f4_hu_d0f313aa9cc43068.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f4_hu_9dfe7d5c84988c6f.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f4_hu_c2c5bae94275b01e.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f4_hu_d0f313aa9cc43068.webp&#34;
               width=&#34;659&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;AI Agent 的第一步：对邮件进行分类并分发至对应处理路径。图片来源：Maximilian Vogel&lt;/p&gt;
&lt;h3 id=&#34;第二步提取数据&#34;&gt;第二步：提取数据&lt;/h3&gt;
&lt;p&gt;下一步，Agent 将&lt;strong&gt;非结构化数据转化为结构化数据&lt;/strong&gt;，以实现安全、系统、规范的处理。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分类是将文本归入某个类别&lt;/li&gt;
&lt;li&gt;而抽取是“读懂”数据并提取其中的关键信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;模型并不会直接“复制粘贴”，而是生成带格式的结果，例如将“(718) 123–45678”转换为“+1 718 123 45678”。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f5_hu_7faa945123c7903b.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f5_hu_7e164f008af55a25.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f5_hu_cb21d17ec015c83c.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f5_hu_7faa945123c7903b.webp&#34;
               width=&#34;464&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;数据提取可以来自邮件文本、附件图片、PDF等。通常是多模型协作，包含 LLM、OCR 等模块。&lt;/p&gt;
&lt;p&gt;示例输入（非结构化）：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Hi,

I would like to report a damage and ask you to compensate me...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;输出（结构化 JSON）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;name&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Deepak&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;surname&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Jamal&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;address&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;123 Main Street, 10008 New York City, NY&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;phone&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;+1 718 123 45678&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;contract_no&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;HC12-223873923&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;claim_description&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;第三步调用外部系统并持久化上下文&#34;&gt;第三步：调用外部系统并持久化上下文&lt;/h3&gt;
&lt;p&gt;普通生成式 AI 通常只能在会话中处理信息，Agent 则&lt;strong&gt;需要访问并更新数据库与企业系统中的数据&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;查询合同号是否存在&lt;/li&gt;
&lt;li&gt;将处理状态写入客服工单系统&lt;/li&gt;
&lt;li&gt;向客户或外部方索要补充信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f6_hu_6c6c01f1d93bb1e.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f6_hu_48ff6a967b7f0b7c.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f6_hu_b3a9beacc1c63d04.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f6_hu_6c6c01f1d93bb1e.webp&#34;
               width=&#34;556&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;图示：调用系统服务并实现上下文持久化。图片来源：Maximilian Vogel&lt;/p&gt;
&lt;h3 id=&#34;第四步判断rag推理与置信度&#34;&gt;第四步：判断、RAG、推理与置信度&lt;/h3&gt;
&lt;p&gt;行政流程的核心是&lt;strong&gt;基于规则做判断&lt;/strong&gt;。这就需要引入 RAG（检索增强生成），通过向量数据库找出条款内容。&lt;/p&gt;
&lt;p&gt;在判断前，我们引导模型“思考过程”，即 prompt 要求其先解释自己的推理过程。&lt;/p&gt;
&lt;p&gt;好处包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给客户一个合理解释&lt;/li&gt;
&lt;li&gt;帮助数据科学家分析模型是否“瞎猜”&lt;/li&gt;
&lt;li&gt;判断模型的结果是否有理可循&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;设置“置信度阈值”也很关键：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若置信度低，交给人工处理&lt;/li&gt;
&lt;li&gt;若置信度高，可全自动处理&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;阈值的设置影响系统的安全性与自动化程度之间的平衡。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f7_hu_f3e280e4b369f178.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f7_hu_57aa48e3246d5f6.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f7_hu_f4f198bc45fc5cc7.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f7_hu_f3e280e4b369f178.webp&#34;
               width=&#34;740&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;AI Agent 的判断流程。图片来源：Maximilian Vogel&lt;/p&gt;
&lt;p&gt;至此，如果你实现了以上 2~3 个步骤，你就构建了一个初级 Agent。&lt;/p&gt;
&lt;p&gt;你可以使用 crewAI、langGraph、langFlow 等框架，也可以直接用 Python 编写。&lt;/p&gt;
&lt;p&gt;根据实践，这种 Agent 可承担理赔部门 70%~90% 的工作量。这是传统生成式 AI 无法完成的。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f8_hu_cd0e5d9766d6e81e.webp 400w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f8_hu_bc0673ee99bc0337.webp 760w,
               /blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f8_hu_9fffe8ffcbb56fd2.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/what-are-ai-agents-step-by-step-guide-to-build-your-own/f8_hu_cd0e5d9766d6e81e.webp&#34;
               width=&#34;760&#34;
               height=&#34;430&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;AI Agent 的三大法则（致敬 Asimov 的机器人三定律）。图片来源：Maximilian Vogel&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我的团队刚刚上线了一个大型物流系统，预计未来几个月我们将继续深耕 Agent 系统的开发。&lt;/p&gt;
&lt;p&gt;祝你成功打造属于自己的 AI Agent！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>平台工程师的 LLM 入门指南</title>
      <link>https://cloudnativecn.com/blog/a-gentle-introduction-to-llms-for-platform-engineers/</link>
      <pubDate>Tue, 15 Apr 2025 11:33:31 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/a-gentle-introduction-to-llms-for-platform-engineers/</guid>
      <description>&lt;p&gt;技术世界日新月异。如今最火的莫过于 AI。作为平台工程师，我们本身已经身处技术栈的洪流之中：容器、Kubernetes、Prometheus、Istio、ArgoCD、Zipkin、Backstage.io …… 技术名词一个接一个，每一个都复杂、抽象且需要深入理解。现在又来了个 AI，让人头大。大多数平台工程师根本没有时间或精力去琢磨什么是 LLM、大模型，更别说在系统中落地使用。&lt;/p&gt;
&lt;p&gt;但现实是：AI 正悄然渗透进平台工程的世界。我们终将需要理解和掌握它。本文尝试用通俗易懂的方式，帮助平台工程师快速建立起对 LLM（大语言模型）的基础认知，并思考它在云原生领域中的应用场景。&lt;/p&gt;
&lt;h2 id=&#34;1-ai-是智能助手而不是天外来物&#34;&gt;1. AI 是“智能助手”而不是“天外来物”&lt;/h2&gt;
&lt;p&gt;你可能用过 Siri，也可能在酒店网站上与机器人客服打过交道。大多数情况下，它们都让人失望——要么不理解你的问题，要么机械地回复固定答案。它们多数基于传统的机器学习或预设规则，无法真正理解你的意图。&lt;/p&gt;
&lt;p&gt;相比之下，现代的 LLM（如 ChatGPT）已经可以处理极为复杂的语言输入，甚至能根据上下文推理、总结信息，和人类进行近乎自然的对话。&lt;/p&gt;
&lt;p&gt;但问题来了：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对平台工程师来说，LLM 到底是什么？它跟传统 API、控制器、CI/CD 流水线有什么关系？&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;别急，我们从一个核心问题讲起——“它能做什么”。&lt;/p&gt;
&lt;h2 id=&#34;2-llm-能做什么像人一样理解文档和日志&#34;&gt;2. LLM 能做什么：像人一样理解文档和日志&lt;/h2&gt;
&lt;p&gt;设想一个企业内部的聊天助手，帮助员工快速了解公司的规范、流程、产品特点。当客户提出技术问题时，员工可以通过这个助手快速定位问题、给出答案。这种助手背后就是一个被企业文档、知识库、过往案例、甚至源码“喂养”过的 LLM。&lt;/p&gt;
&lt;p&gt;对比一下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;功能&lt;/th&gt;
          &lt;th&gt;人工&lt;/th&gt;
          &lt;th&gt;LLM&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;阅读全部文档&lt;/td&gt;
          &lt;td&gt;慢&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;理解概念&lt;/td&gt;
          &lt;td&gt;可&lt;/td&gt;
          &lt;td&gt;可&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;回答问题&lt;/td&gt;
          &lt;td&gt;慢&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;LLM 的强大之处，在于它可以“吞掉”TB 级别的数据，然后从中提炼出概念与模式。听起来是不是像搜索引擎？不，它远远超过了搜索引擎。&lt;/p&gt;
&lt;h2 id=&#34;3-不只是搜索是理解&#34;&gt;3. 不只是搜索，是“理解”&lt;/h2&gt;
&lt;p&gt;传统搜索引擎依赖关键词匹配，比如你搜索“database timeout”，它只会返回包含这些词的文档。如果真实错误日志写的是“SQL connection lost”，你就查不到了。&lt;/p&gt;
&lt;p&gt;而 LLM 能理解“database timeout”与“SQL连接丢失”、“查询超时”、“数据库网络延迟”之间的语义联系。它不仅能从日志、trace 和文档中抓出相关内容，还能像一个资深工程师一样，总结出可能原因。&lt;/p&gt;
&lt;p&gt;这才是 LLM 的本事：&lt;strong&gt;不仅能搜索，还能理解、总结、推理。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-使用自然语言交互甚至可以生成代码&#34;&gt;4. 使用自然语言交互（甚至可以生成代码）&lt;/h2&gt;
&lt;p&gt;LLM 可以像人类一样理解自然语言，还能用自然语言输出答案。例如：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;问：引擎故障灯亮了，启动时有咔哒声，怎么回事？
答：可能是电池电量不足或启动电机故障……（给出详细分析）&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;更惊人的是，它还能生成代码、撰写文档、总结聊天记录、处理用户请求……它甚至可以读懂老旧系统的接口文档，然后自动生成集成代码！&lt;/p&gt;
&lt;p&gt;对于平台工程师而言，LLM 可以：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;帮你总结应用日志&lt;/li&gt;
&lt;li&gt;快速生成 Kubernetes YAML 或 Terraform 模板&lt;/li&gt;
&lt;li&gt;自动生成 CI/CD 流水线步骤说明&lt;/li&gt;
&lt;li&gt;撰写插件或脚本（例如 ArgoCD 的 Plugin、Backstage 的 Template）&lt;/li&gt;
&lt;li&gt;甚至为 SRE 分析告警和异常根因&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5-如何接入-llm熟悉的-http-接口&#34;&gt;5. 如何接入 LLM？熟悉的 HTTP 接口！&lt;/h2&gt;
&lt;p&gt;最棒的是，LLM 通常通过 HTTP API 暴露服务。&lt;/p&gt;
&lt;p&gt;平台工程师早就熟悉这个套路了：写一个 HTTP 请求，传入 JSON，接收 JSON 响应。&lt;/p&gt;
&lt;p&gt;来看个例子，调用 OpenAI API 查询 Siri 是如何工作的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl https://api.openai.com/v1/chat/completions &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -H &lt;span class=&#34;s2&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -H &lt;span class=&#34;s2&#34;&gt;&amp;#34;Authorization: Bearer &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$OPENAI_API_KEY&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    &amp;#34;model&amp;#34;: &amp;#34;gpt-3.5-turbo&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    &amp;#34;messages&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;      {
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;content&amp;#34;: &amp;#34;Do you know how Siri works?&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;      }
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  }&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;返回内容如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;chatcmpl-Avpw5BwQ4HypBRJFpqg3pPeeqDRwS&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt-3.5-turbo-0125&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;choices&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;assistant&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Um... I mean... does it though?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;usage&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;prompt_tokens&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;completion_tokens&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;107&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;total_tokens&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;121&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;你会注意到几个要点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;请求是一个标准的 HTTP API 调用&lt;/li&gt;
&lt;li&gt;请求体是自然语言，响应也是自然语言&lt;/li&gt;
&lt;li&gt;响应中包含 token 数量（因为使用 LLM 通常按 token 计费）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，作为平台工程师，你可以用 API Gateway 做调用限流、配额管理、成本控制，还可以做安全网关。&lt;/p&gt;
&lt;h2 id=&#34;6-背后的原理其实很简单但也很神奇&#34;&gt;6. 背后的原理其实很简单（但也很神奇）&lt;/h2&gt;
&lt;p&gt;虽然 LLM 看起来很“神”，但它的核心原理其实很简单：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;接收一串单词（tokens），然后预测下一个最可能的词。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The cow jumped over the ___” → “moon”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;就是这么简单的过程，重复进行数百次，就组成了一个完整回答。&lt;/p&gt;
&lt;p&gt;这个过程背后依赖大量训练数据和昂贵的硬件，但核心机制就是概率预测。&lt;/p&gt;
&lt;p&gt;推荐阅读： 👉 &lt;a href=&#34;https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How LLMs work explained without math&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;7-注意事项不是银弹也有风险&#34;&gt;7. 注意事项：不是银弹，也有风险&lt;/h2&gt;
&lt;p&gt;LLM 带来了新的能力，也伴随着新的风险，尤其在平台工程中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;准确性&lt;/strong&gt;：LLM 可能自信满满地说错话，在合规或运维场景中可能带来严重问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据隐私&lt;/strong&gt;：若使用的是 SaaS 模型，输入的数据可能泄露（例如 OpenAI）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成本控制&lt;/strong&gt;：token 计费方式容易产生隐性费用，建议用网关管理配额&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;响应质量&lt;/strong&gt;：LLM 的输出不是文档原文，可能偏离主题或引入“幻觉”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;品牌风险&lt;/strong&gt;：若未设置过滤机制，LLM 输出可能引发不当或带偏见内容&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖过重&lt;/strong&gt;：部分用户过度依赖模型输出，忽略人工判断与验证&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合规问题&lt;/strong&gt;：如 GDPR、HIPAA 等法规限制使用 AI 处理敏感数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;建议设立审计机制、明确边界、设定使用准则。&lt;/p&gt;
&lt;h2 id=&#34;结语llm-是平台工程师的又一个工具&#34;&gt;结语：LLM 是平台工程师的又一个工具&lt;/h2&gt;
&lt;p&gt;LLM 不是什么魔法，它是一个模式识别系统，用海量数据训练而成，具备强大的语义理解和生成能力。&lt;/p&gt;
&lt;p&gt;对平台工程师而言，它就像：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;另一种“自动化”&lt;/li&gt;
&lt;li&gt;一种“超能运维助手”&lt;/li&gt;
&lt;li&gt;一种“文档理解引擎”&lt;/li&gt;
&lt;li&gt;一种“智能 CI/CD 脚本生成器”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你可以用它来增强现有平台的能力，提高团队效率，提升用户支持体验。
但你也需要理性对待它的局限，持续试验、迭代和评估其在你平台中的最佳用法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AI 正在来到平台工程的世界——拥抱它，不如先理解它。&lt;/p&gt;&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>MCP 到底是什么，为什么每个人都在谈论它？</title>
      <link>https://cloudnativecn.com/blog/what-the-heck-is-mcp-and-why-is-everyone-talking-about-it/</link>
      <pubDate>Mon, 14 Apr 2025 10:37:31 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/what-the-heck-is-mcp-and-why-is-everyone-talking-about-it/</guid>
      <description>&lt;p&gt;本文译自：&lt;a href=&#34;https://github.blog/ai-and-ml/llms/what-the-heck-is-mcp-and-why-is-everyone-talking-about-it/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;What the heck is MCP and why is everyone talking about it?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;在大模型（LLM）相关的讨论中，最近“&lt;strong&gt;MCP（模型上下文协议，Model Context Protocol）&lt;/strong&gt;”这个词频频出现，几乎成了热门话题。但真正讲清楚它是什么的，反而不多。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;一句话总结：MCP 是一个开放标准，用来连接大模型与外部的数据和工具。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;本文带你快速了解它的来龙去脉。&lt;/p&gt;
&lt;h2 id=&#34;llm-的上下文难题&#34;&gt;LLM 的上下文难题&lt;/h2&gt;
&lt;p&gt;大模型擅长生成内容，但一旦你问它一些训练数据之外的内容，它要么&lt;strong&gt;胡诌&lt;/strong&gt;（幻觉），要么说“我不知道”。&lt;/p&gt;
&lt;p&gt;这时，我们就需要在 Prompt 里提供“上下文信息”——无论是你的代码仓库、文档、数据源还是配置项，这些上下文对构建真正有用的 AI agent 来说是必不可少的。&lt;/p&gt;
&lt;p&gt;目前主流做法包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 Prompt 中尽可能精细地嵌入上下文&lt;/li&gt;
&lt;li&gt;借助额外工具注入上下文，比如 GitHub Copilot 的 &lt;a href=&#34;https://code.visualstudio.com/docs/copilot/workspace-context&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@workspace&lt;/a&gt;，会把代码库中的信息传递给 LLM&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这种方式虽然很酷，但&lt;strong&gt;实现复杂、跨 API 和服务集成时更是困难重重&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;解决方案mcpmodel-context-protocol&#34;&gt;解决方案：MCP（Model Context Protocol）&lt;/h2&gt;
&lt;p&gt;2023 年 11 月，Anthropic &lt;a href=&#34;https://www.anthropic.com/news/model-context-protocol&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;开源了 MCP 协议&lt;/a&gt;，它是一个用于连接大模型和工具的数据交换标准。&lt;/p&gt;
&lt;p&gt;MCP 就像“你睡觉的样子”——起初慢慢发展，后来突然爆火。随着越来越多的组织采纳 MCP，它的价值也在迅速上升。&lt;/p&gt;
&lt;p&gt;✨ &lt;strong&gt;MCP 是模型无关的（model-agnostic）&lt;/strong&gt;，意味着任何厂商、平台或个人开发者都可以实现自己的 MCP 兼容系统。这种开放性让它在 AI 工具生态中大受欢迎。&lt;/p&gt;
&lt;p&gt;从云原生社区的角度看，它类似于我们熟悉的 &lt;a href=&#34;https://microsoft.github.io/language-server-protocol/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language Server Protocol（LSP）&lt;/a&gt;。LSP 诞生于 2016 年，标准化了代码编辑器和语言之间的通信方式，使编辑器对语言的支持变得“开箱即用”。&lt;/p&gt;
&lt;p&gt;如今，&lt;strong&gt;MCP 正在复刻这一成功路径，只不过对象变成了 AI 工具链。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;mcp-意味着什么&#34;&gt;MCP 意味着什么？&lt;/h2&gt;
&lt;p&gt;它让：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大厂 🏢&lt;/li&gt;
&lt;li&gt;初创团队 🚀&lt;/li&gt;
&lt;li&gt;独立开发者 👩‍💻&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;都能在自己的系统中&lt;strong&gt;快速集成上下文能力&lt;/strong&gt;，从而构建功能更强大的 AI 应用。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;它降低了 AI 工具链的集成门槛，提升了开发体验和用户体验&lt;/strong&gt;，在开放标准领域，MCP 有望成为“新一代基础设施”。&lt;/p&gt;
&lt;h2 id=&#34;github-正在行动&#34;&gt;GitHub 正在行动&lt;/h2&gt;
&lt;p&gt;GitHub 不只是聊 MCP，我们也在贡献！&lt;/p&gt;
&lt;p&gt;我们最近发布了一个官方开源项目：&lt;a href=&#34;https://github.com/github/github-mcp-server&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub MCP Server&lt;/a&gt;，它可以与 GitHub API 无缝集成，开发者可以基于它构建自动化和上下文增强工具。&lt;/p&gt;
&lt;p&gt;📢 查看 &lt;a href=&#34;https://github.blog/changelog/2025-04-04-github-mcp-server-public-preview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;官方公告&lt;/a&gt;，或加入 &lt;a href=&#34;https://github.com/orgs/community/discussions/categories/announcements&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub 社区讨论&lt;/a&gt;。&lt;/p&gt;
&lt;h2 id=&#34;如何参与-mcp-社区&#34;&gt;如何参与 MCP 社区？&lt;/h2&gt;
&lt;p&gt;非常欢迎你加入 MCP 社区并贡献力量，下面是一些推荐资源：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://modelcontextprotocol.io/introduction&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MCP 官方文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/modelcontextprotocol/servers&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MCP 示例实现仓库&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://spec.modelcontextprotocol.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MCP 协议规范&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://code.visualstudio.com/api/language-extensions/language-server-extension-guide#why-language-server&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LSP 背景资料&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，也别忘了：MCP 已可与 Agent 模式结合使用。是时候动手打造你的 AI 工具链了！&lt;/p&gt;
&lt;h2 id=&#34;-云原生视角总结&#34;&gt;🚀 云原生视角总结&lt;/h2&gt;
&lt;p&gt;MCP 是一个符合云原生价值观的协议标准：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;开放&lt;/strong&gt;：人人可用，人人可贡献&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可组合&lt;/strong&gt;：可以集成到任何 AI 系统&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可移植&lt;/strong&gt;：不绑定任何平台或模型&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;生态驱动&lt;/strong&gt;：标准的普及带动工具和平台共同进化&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;它的崛起像当年的 Kubernetes、gRPC、LSP —— 正在为 AI 时代构建“可插拔”基础设施。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>了解 Llama 3：迄今最强大的免费开源大模型从概念到使用</title>
      <link>https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/</link>
      <pubDate>Mon, 01 Jul 2024 09:33:23 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/</guid>
      <description>&lt;p&gt;Meta 公司最近发布了 &lt;a href=&#34;https://ai.meta.com/blog/meta-llama-3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llama 3&lt;/a&gt;，这是其最新一代尖端开源大型语言模型（LLM）。基于其前身的基础之上，Llama 3 旨在提升 Llama 2 作为与 ChatGPT 竞争的重要开源产品的能力，如文章 &lt;a href=&#34;https://www.unite.ai/llama-2-a-deep-dive-into-the-open-source-challenger-to-chatgpt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llama 2: 深入探索开源挑战者 ChatGPT&lt;/a&gt; 中全面回顾的那样。&lt;/p&gt;
&lt;p&gt;在本文中，我们将讨论 Llama 3 背后的核心概念，探索其创新架构和训练过程，并提供关于如何负责任地访问、使用和部署这一开创性模型的实际指导。无论你是研究人员、开发者还是 AI 爱好者，这篇文章都将为你提供利用 Llama 3 为你的项目和应用赋能的知识和资源。&lt;/p&gt;
&lt;h2 id=&#34;llama-的演变从-llama-2-到-llama-3&#34;&gt;Llama 的演变：从 Llama 2 到 Llama 3&lt;/h2&gt;
&lt;p&gt;Meta 的 CEO，Mark Zuckerberg，在 &lt;a href=&#34;https://www.threads.net/@zuck/post/C56MFEKxl-x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Threads.net&lt;/a&gt; 上宣布了 Llama 3 的首次亮相，这是 Meta AI 开发的最新 AI 模型。这个尖端模型现在已开源，旨在提升 Meta 的各种产品，包括 Messenger 和 Instagram。Zuckerberg 强调，Llama 3 使 Meta AI 成为最先进的&lt;a href=&#34;https://about.fb.com/news/2024/04/meta-ai-assistant-built-with-llama-3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;免费可用的 AI 助手&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;在我们讨论 Llama 3 的具体细节之前，让我们简要回顾一下它的前身，Llama 2。Llama 2 于 2022 年推出，是开源 LLM 领域的一个重要里程碑，提供了一个强大而高效的模型，可以在消费者硬件上运行。&lt;/p&gt;
&lt;p&gt;然而，尽管 Llama 2 取得了显著的成就，但它也有其局限性。用户报告了一些问题，如错误拒绝（模型拒绝回答无害的提示）、有限的帮助性，以及在推理和代码生成等领域的改进空间。&lt;/p&gt;
&lt;p&gt;进入 Llama 3：Meta 对这些挑战和社区的反馈做出了回应。通过 Llama 3，Meta 设定了与当今市场上顶级专有模型相媲美的最佳开源模型的目标，同时也优先考虑了负责任的开发和部署实践。&lt;/p&gt;
&lt;h2 id=&#34;llama-3架构和训练&#34;&gt;Llama 3：架构和训练&lt;/h2&gt;
&lt;p&gt;Llama 3 的一项关键创新是其分词器，特点是显著扩展的词汇表，&lt;strong&gt;128,256 个 token&lt;/strong&gt;（从 Llama 2 的 32,000 个增加）。这更大的词汇表允许更有效的文本编码，无论是输入还是输出，可能导致更强的多语言能力和整体性能的提升。&lt;/p&gt;
&lt;p&gt;Llama 3 还采用了&lt;strong&gt;分组查询注意力&lt;/strong&gt;（GQA），这是一种提高可扩展性的有效表示技术，有助于模型更有效地处理更长的上下文。&lt;strong&gt;8B&lt;/strong&gt; 版本的 Llama 3 使用了 GQA，而&lt;strong&gt;8B&lt;/strong&gt; 和 &lt;strong&gt;70B&lt;/strong&gt; 模型都可以处理长达 &lt;strong&gt;8,192 个 token&lt;/strong&gt;的序列。&lt;/p&gt;
&lt;h3 id=&#34;训练数据和扩展&#34;&gt;训练数据和扩展&lt;/h3&gt;
&lt;p&gt;用于 Llama 3 的训练数据是其性能提升的关键因素。Meta 精心策划了一个包含超过 &lt;strong&gt;15 万亿&lt;/strong&gt; token 的庞大数据集，来自公开可获得的在线来源，是用于 Llama 2 的数据集的七倍。这个数据集还包括了超过 5% 的高质量非英语数据，涵盖了 &lt;strong&gt;30 多种语言&lt;/strong&gt;，为未来的多语言应用做准备。&lt;/p&gt;
&lt;p&gt;为了确保数据质量，Meta 采用了先进的过滤技术，包括启发式过滤器、NSFW 过滤器、语义去重和训练在 Llama 2 上预测数据质量的文本分类器。团队还进行了广泛的实验，以确定预训练的最佳数据来源组合，确保 Llama 3 在广泛的用例上表现良好，包括琐事、STEM、编码和历史知识。&lt;/p&gt;
&lt;p&gt;放大预训练是 Llama 3 开发的另一个关键方面。Meta 开发了缩放法则，使他们能够在实际训练之前预测其最大模型在关键任务上的性能，如代码生成。这些信息指导了关于数据组合和计算分配的决策，最终导致了更有效和有效的培训。&lt;/p&gt;
&lt;p&gt;Llama 3 最大的模型是在两个定制构建的 24,000 GPU 集群上训练的，利用数据并行、模型并行和流水线并行技术的组合。Meta 的先进训练堆栈自动化了错误检测、处理和维护，最大化了 GPU 的运行时间，使训练效率比 Llama 2 提高了大约三倍。&lt;/p&gt;
&lt;h3 id=&#34;指令微调和性能&#34;&gt;指令微调和性能&lt;/h3&gt;
&lt;p&gt;为了充分发挥 Llama 3 在聊天和对话应用中的潜力，Meta 创新了其指令微调方法。其方法结合了&lt;strong&gt;监督微调&lt;/strong&gt;（SFT）、拒绝抽样、&lt;strong&gt;近端政策优化&lt;/strong&gt;（PPO）和&lt;strong&gt;直接偏好优化&lt;/strong&gt;（DPO）。&lt;/p&gt;
&lt;p&gt;SFT 中使用的提示质量和在 PPO 和 DPO 中使用的偏好排名在对齐模型的性能中起着关键作用。Meta 的团队精心策划了这些数据，并对由人类注释者提供的注释进行了多轮质量保证。&lt;/p&gt;
&lt;p&gt;通过 PPO 和 DPO 对偏好排名进行训练还显著提高了 Llama 3 在推理和编码任务上的性能。Meta 发现，即使模型在直接回答推理问题时遇到困难，它仍然可能产生正确的推理迹象。通过偏好排名的训练，模型学会了如何从这些迹象中选择正确的答案。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-对比结果&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;对比结果&#34; srcset=&#34;
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu_5ad9f264a9af69b4.webp 400w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu_a77ca33dd81ec0b0.webp 760w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu_59f7cf344856f195.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu_5ad9f264a9af69b4.webp&#34;
               width=&#34;760&#34;
               height=&#34;617&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      对比结果
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;成果显而易见：Llama 3 在常见的行业基准测试中表现优于许多可用的开源聊天模型，为 LLM 的 8B 和 70B 参数级别建立了新的最佳性能。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu_e1de3054b3356480.webp 400w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu_461eac64dbc3387a.webp 760w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu_e254702f5c6690e7.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu_e1de3054b3356480.webp&#34;
               width=&#34;760&#34;
               height=&#34;467&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;负责任的开发和安全考虑&#34;&gt;负责任的开发和安全考虑&lt;/h2&gt;
&lt;p&gt;在追求尖端性能的同时，Meta 也优先考虑了负责任的开发和部署实践，用于 Llama 3。该公司采用了系统级方法，将 Llama 3 模型视为更广泛生态系统的一部分，使开发者能够设计和定制模型以满足其特定用例和安全要求。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu_9a63cf184dd8a9e9.webp 400w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu_1000360f1179428e.webp 760w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu_67479b81a44cbdcc.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu_9a63cf184dd8a9e9.webp&#34;
               width=&#34;760&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Meta 进行了广泛的红队演习，执行了对抗评估，并实施了安全缓解技术，以降低其指令调优模型中的残余风险。然而，该公司承认可能仍会存在残余风险，并建议开发者在其特定用例的背景下评估这些风险。&lt;/p&gt;
&lt;p&gt;为支持负责任的部署，Meta 更新了其负责任使用指南，为开发者提供了一个全面的资源，以实施模型和系统级安全最佳实践，用于他们的应用。该指南涵盖了内容审查、风险评估和使用安全工具（如 Llama Guard 2 和 Code Shield）等主题。&lt;/p&gt;
&lt;p&gt;Llama Guard 2，基于 MLCommons 分类法构建，旨在对 LLM 输入（提示）和响应进行分类，检测可能被视为不安全或有害的内容。CyberSecEval 2 在其前身的基础上增加了措施，以防止模型的代码解释器被滥用、攻击性网络安全能力和对提示注入攻击的易感性。&lt;/p&gt;
&lt;p&gt;Code Shield 是 Llama 3 新推出的一个介绍，增加了推断时间的不安全代码过滤，减轻了不安全代码建议、代码解释器滥用和安全命令执行等风险。&lt;/p&gt;
&lt;h2 id=&#34;访问和使用-llama-3&#34;&gt;访问和使用 Llama 3&lt;/h2&gt;
&lt;p&gt;随着 Meta AI 的 Llama 3 发布，已推出了几种开源工具，可在各种操作系统上进行本地部署，包括 Mac、Windows 和 Linux。本节详细介绍了三个值得注意的工具：Ollama、Open WebUI 和 LM Studio，每个工具都提供了利用 Llama 3 功能的独特功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;：适用于 Mac、Linux 和 Windows，&lt;a href=&#34;https://ollama.com/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ollama&lt;/a&gt; 简化了在个人计算机上操作 Llama 3 和其他大型语言模型的过程，即使是那些硬件较弱的设备也是如此。它包括一个包管理器，便于模型管理，并支持跨平台的下载和运行模型的命令。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open WebUI with Docker&lt;/strong&gt;：这个工具提供了一个用户友好的、基于 &lt;a href=&#34;https://docs.docker.com/desktop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker&lt;/a&gt; 的界面，兼容 Mac、Linux 和 Windows。它与 Ollama 注册表中的模型无缝集成，允许用户在本地 Web 界面内部署和交互，例如 Llama 3。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;：面向 Mac、Linux 和 Windows 的用户，&lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LM Studio&lt;/a&gt; 支持一系列模型，基于 llama.cpp 项目构建。它提供了一个聊天界面，便于直接与各种模型进行交互，包括 Llama 3 8B Instruct 模型。&lt;/p&gt;
&lt;p&gt;这些工具确保用户可以在个人设备上高效利用 Llama 3，满足技术技能和需求的不同范围。每个平台都提供了设置和模型交互的分步过程，使先进的人工智能更加易于开发者和爱好者接触。&lt;/p&gt;
&lt;h2 id=&#34;大规模部署-llama-3&#34;&gt;大规模部署 Llama 3&lt;/h2&gt;
&lt;p&gt;除了直接提供模型权重外，Meta 还与各种云提供商、模型 API 服务和硬件平台合作，实现 Llama 3 的无缝部署。&lt;/p&gt;
&lt;p&gt;Llama 3 的一大优势是其改进的 token 效率，这要归功于新的分词器。基准测试显示，与 Llama 2 相比，Llama 3 需要的 token 减少了 &lt;strong&gt;15%&lt;/strong&gt;，从而实现了更快、更经济的推断。&lt;/p&gt;
&lt;p&gt;Grouped Query Attention (GQA) 的整合在 Llama 3 的 8B 版本中有助于保持与 Llama 2 的 7B 版本相当的推断效率，尽管参数数量增加了。&lt;/p&gt;
&lt;p&gt;为简化部署流程，Meta 提供了 Llama Recipes 代码库，其中包含开源代码和微调、部署、模型评估等示例。这个代码库为开发者提供了一个宝贵的资源，帮助他们利用 Llama 3 的能力来应用到他们的应用中。&lt;/p&gt;
&lt;p&gt;对于那些有兴趣探索 Llama 3 性能的人来说，Meta 已经将其最新模型整合到 Meta AI 中，这是一个以 Llama 3 技术构建的领先人工智能助手。用户可以通过各种 Meta 应用程序，如 Facebook、Instagram、WhatsApp、Messenger 和 Web 与 Meta AI 进行交互，以完成任务、学习、创造和与他们关心的事物建立联系。&lt;/p&gt;
&lt;h2 id=&#34;llama-3-接下来会怎样&#34;&gt;Llama 3 接下来会怎样？&lt;/h2&gt;
&lt;p&gt;尽管 8B 和 70B 模型标志着 Llama 3 发布的开始，但 Meta 对这款开创性 LLM 的未来有雄心勃勃的计划。&lt;/p&gt;
&lt;p&gt;在未来几个月，我们可以期待看到新功能的引入，包括多模态（能够处理和生成不同数据模态，如图像和视频）、多语言支持（支持多种语言）和更长的上下文窗口，以提高在需要广泛上下文的任务上的性能。&lt;/p&gt;
&lt;p&gt;此外，Meta 计划发布更大的模型大小，包括目前正在训练中的超过 4000 亿参数的模型，这些模型在性能和能力方面展现出了有前途的趋势。&lt;/p&gt;
&lt;p&gt;为了进一步推进该领域的发展，Meta 还将发布关于 Llama 3 的详细研究论文，与广泛的 AI 社区分享其发现和见解。&lt;/p&gt;
&lt;p&gt;作为即将推出的内容的预览，Meta 分享了一些其最大 LLM 模型在各种基准上的早期性能快照。尽管这些结果是基于早期检查点的，并且可能会发生变化，但它们提供了一个激动人心的展望，展示了 Llama 3 的未来潜力。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;Llama 3 代表了开源大型语言模型演变的一个重要里程碑，推动了性能、能力和负责任开发实践的边界。凭借其创新架构、庞大的训练数据集和尖端的微调技术，Llama 3 为 LLM 的 8B 和 70B 参数级别建立了新的最佳性能基准。&lt;/p&gt;
&lt;p&gt;然而，Llama 3 不仅仅是一个强大的语言模型；它还体现了 Meta 致力于培养一个开放和负责任的 AI 生态系统的承诺。通过提供全面的资源、安全工具和最佳实践，Meta 授权开发者充分利用 Llama 3 的潜力，同时确保根据其特定用例和受众的需求实现负责任的部署。&lt;/p&gt;
&lt;p&gt;随着 Llama 3 之旅的继续，随着新的能力、模型大小和研究发现的出现，AI 社区热切期待从这款开创性 LLM 中涌现出的创新应用和突破。&lt;/p&gt;
&lt;p&gt;无论你是一名推动自然语言处理边界的研究人员、一名构建下一代智能应用的开发者还是对最新进展感到好奇的 AI 爱好者，Llama 3 都承诺成为你工具箱中的强大工具，开启新的大门并解锁一系列可能性。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>给初学生成式 AI（GenAI）的开发人员的 7 条最佳实践</title>
      <link>https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/</link>
      <pubDate>Wed, 20 Dec 2023 11:30:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/</guid>
      <description>&lt;h2 id=&#34;编者按&#34;&gt;编者按&lt;/h2&gt;
&lt;p&gt;本文译自：&lt;a href=&#34;https://thenewstack.io/7-best-practices-for-developers-getting-started-with-genai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thenewstack.io/7-best-practices-for-developers-getting-started-with-genai/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;编辑评论：这是一篇非常有价值的文章，向开发者展示了生成式 AI 的潜力和应用。生成式 AI 是一种利用大型语言模型来生成和转换文本的技术，它可以帮助开发者解决一些复杂的问题，如代码生成，文档编写，内容创作等。生成式 AI 也是一种云原生的技术，它需要大量的计算资源和数据，以及高效的部署和管理方式。文章提供了一些实用的工具和平台，如 GitHub Copilot，Bard，ChatGPT 等，让开发者可以轻松地尝试和使用生成式 AI。文章还给出了一些注意事项和建议，如保护数据隐私，验证输出质量，避免滥用等，让开发者可以负责任地使用生成式 AI。我认为这篇文章是一个很好的入门指南，让开发者可以了解和利用生成式 AI 来打造创新的云原生应用。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;正文&#34;&gt;正文&lt;/h2&gt;
&lt;p&gt;通过一点经验，你可以使用 GenAI 解决一些相当困难的问题，就像学习任何新技术一样，最好的方法就是动手实践。&lt;/p&gt;
&lt;p&gt;随着可访问的生成式人工智能进入主流，以及由此产生的通过简单语言转化整个人类知识的能力，每个企业都在竭力将人工智能整合到其技术体系中。对于开发人员来说，压力很大，但也有着令人兴奋的无限可能性。&lt;/p&gt;
&lt;p&gt;如果你有一些经验，你可以使用 GenAI 解决一些相当困难的问题，就像学习自 HTML 诞生以来的每一项新技术一样。让我们看看你可以采取的七个步骤，以开始建立 GenAI 的基础，并最终逐步发展成一个完全运作、可扩展的应用程序。&lt;/p&gt;
&lt;h2 id=&#34;1-玩转现有的-genai-工具&#34;&gt;1. 玩转现有的 GenAI 工具&lt;/h2&gt;
&lt;p&gt;入门 GenAI 的最佳方法是实践，而且门槛非常低。市场上现在有许多免费选项，比如 Bard、ChatGPT、Bing 和 Anthropic，有很多可以学习的选择。&lt;/p&gt;
&lt;p&gt;尝试使用 GenAI 工具和代码生成解决方案进行实验（并鼓励你的团队进行实验），例如&lt;a href=&#34;https://github.com/features/copilot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Copilot&lt;/a&gt;，它集成到每个流行的 IDE 中，充当一对程序员。Copilot 提供程序员建议，帮助解决代码问题，并生成整个函数，使学习和适应&lt;a href=&#34;https://us.resources.cio.com/resources/the-data-streaming-platform-key-to-ai-initiatives-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GenAI&lt;/a&gt;变得更快更容易。&lt;/p&gt;
&lt;p&gt;当你首次使用这些现成的工具时，要小心使用专有或敏感的公司数据，即使只是提供给工具一个提示也要小心。Gen AI 供应商可能会存储并使用你的数据用于将来的训练运行，这是公司数据政策和信息安全协议的重大违规行为。确保你及时直接地向你的团队传达这一黄金规则。&lt;/p&gt;
&lt;h2 id=&#34;2-了解从-genai-中可以获得什么&#34;&gt;2. 了解从 GenAI 中可以获得什么&lt;/h2&gt;
&lt;p&gt;一旦你开始尝试 GenAI，你将很快了解到不同提示会产生什么类型的输出。大多数 GenAI 工具可以生成各种格式的文本，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成&lt;/strong&gt;新的故事、想法、文章或任意长度的文本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;转换&lt;/strong&gt;现有文本为不同格式，如 JSON、Markdown 或 CSV。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;翻译&lt;/strong&gt;文本成不同语言。&lt;/li&gt;
&lt;li&gt;以聊天的方式&lt;strong&gt;对话&lt;/strong&gt;来回。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;审查&lt;/strong&gt;文本以展示特定元素。&lt;/li&gt;
&lt;li&gt;将长篇内容&lt;strong&gt;汇总&lt;/strong&gt;以获取洞察。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分析&lt;/strong&gt;文本的情感。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;任何人都可以生成这些类型的生成文本结果，无需编程技能。只需键入提示，文本就会产生。然而，大型语言模型（LLM）经过的培训越多，即它摄取的语言碎片越多，随着时间的推移，它在生成、更改和分析文本方面就会变得更加准确。&lt;/p&gt;
&lt;h2 id=&#34;3-学习提示工程&#34;&gt;3. 学习提示工程&lt;/h2&gt;
&lt;p&gt;部署 GenAI 的良好方法之一是掌握编写提示的技巧，这既是一门艺术又是一门科学。虽然提示工程师是一个实际的职位描述，但它也是任何希望提高他们使用 AI 的人的好绰号。优秀的提示工程师知道如何开发、完善和优化文本提示，以获得最佳结果并提高整个 AI 系统的性能。&lt;/p&gt;
&lt;p&gt;提示工程不需要特定的学位或背景，但从事这项工作的人需要擅长清晰解释事物。这是重要的，因为所有可用的 LLM 都是无状态的，这意味着没有长期记忆，每次交互只存在于小会话中。&lt;/p&gt;
&lt;p&gt;在提示工程中，以下三个因素变得重要：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;上下文&lt;/strong&gt;：你提出的问题、聊天历史记录和你设置的参数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;知识&lt;/strong&gt;：LLM 已经接受的培训内容以及你通过提示提供的新信息的结合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;形式&lt;/strong&gt;：你期望以何种方式生成信息的语气。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上下文、知识和形式的结合塑造了 GenAI 的大量知识存储成为你希望获得的响应类型。&lt;/p&gt;
&lt;h2 id=&#34;4-探索其他-genai-提示方法&#34;&gt;4. 探索其他 GenAI 提示方法&lt;/h2&gt;
&lt;p&gt;到目前为止，我们一直在谈论零-shot 提示，这基本上意味着提出一个带有一些上下文的问题。如果你从这种方法中没有得到期望的结果，还有四种提示 GenAI 的方法。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;单次提示&lt;/strong&gt;：提供你正在寻找的输出类型的示例。如果你想要特定类型的格式，例如[标题]和[4 个要点]，这将特别有用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;少量提示&lt;/strong&gt;：这类似于单次提示，但你会提供三到五个示例而不仅仅是一个。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“让我们一步一步地思考”&lt;/strong&gt;：这种技巧对 LLM 和对人都同样有效。如果你有一个包含多个部分的复杂问题，请在末尾输入此短语，等待 LLM 分解问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;思路链提示&lt;/strong&gt;：对于涉及复杂算术或其他推理任务的问题，思路链提示会指示工具“展示其工作方式”并解释其如何得出答案。以下是可能的示例：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu_e85bc3796c80d8fc.webp 400w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu_93db0c122a322182.webp 760w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu_fdcd36c8d3dade6e.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu_e85bc3796c80d8fc.webp&#34;
               width=&#34;760&#34;
               height=&#34;402&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;5-查看其他-genai-工作示例&#34;&gt;5. 查看其他 GenAI 工作示例&lt;/h2&gt;
&lt;p&gt;一旦你熟悉了 GenAI 工具并了解如何编写出色的提示，&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main/examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;请查看 OpenAI 发布的一些示例&lt;/a&gt;，了解其他人正在做什么以及可能的其他可能性。随着你的实验，你将更加熟悉聊天界面，并学会如何对其进行微调，以便熟练地缩小响应范围，甚至将响应转换为 CSV 文件或其他类型的表格。&lt;/p&gt;
&lt;p&gt;考虑如何将你的 GenAI 知识应用于你的业务，以简化困难或重复性任务，生成创意并使信息易于让更广泛的受众访问。你可以想象出哪些新的用例？以前不可能的东西现在成为可能了吗？&lt;/p&gt;
&lt;h2 id=&#34;6-集成第三方-genai-工具和-api&#34;&gt;6. 集成第三方 GenAI 工具和 API&lt;/h2&gt;
&lt;p&gt;考虑使用 ChatGPT、Bard 和 Claude 2 等 API 通过 API 使用 LLMs 的角色。这些工具都提供了强大的 API，并有支持文档，因此入门门槛很低。大多数这些 API 是基于使用量的，因此更容易玩弄。&lt;/p&gt;
&lt;p&gt;通常情况下，通过语义搜索和由向量数据库支持的嵌入来将自定义或私有数据集成到 LLM 提示中，你还可以集成自定义或私有数据。通常称为 RAG（检索增强生成）。&lt;/p&gt;
&lt;p&gt;分解这两个术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义搜索&lt;/strong&gt;：使用词嵌入比较查询的含义与其索引中文档的含义，即使没有完全匹配的单词也能获得更相关的结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;嵌入&lt;/strong&gt;：将对象（如单词、句子或整个文档）的数值表示转化为多维空间。这使得评估不同实体之间的关系成为可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下是这可能看起来的一个示例：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu_2b2bd505873cf48b.webp 400w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu_e013df8a800715dd.webp 760w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu_362c1405721698b1.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu_2b2bd505873cf48b.webp&#34;
               width=&#34;760&#34;
               height=&#34;608&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这幅图展示了“猫”和“狗”的概念比它们与“人”或“蜘蛛”的概念更接近，“车辆汽车”则是最远的，是概念中最不相关的。（&lt;a href=&#34;https://www.confluent.io/blog/chatgpt-and-streaming-data-for-real-time-generative-ai/#connecting-knowledge-base-to-gpt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里有更多关于如何使用语义搜索和嵌入的信息&lt;/a&gt;。）&lt;/p&gt;
&lt;h2 id=&#34;7-从头开始训练自己的模型&#34;&gt;7. 从头开始训练自己的模型&lt;/h2&gt;
&lt;p&gt;这最后的建议实际上不太像建议，更像是一个“可选的下一步”。训练自己的 GenAI 模型并不适合每个人，但如果你：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有独特而有价值的知识库。&lt;/li&gt;
&lt;li&gt;想要执行商业 LLM 无法完成的某些任务。&lt;/li&gt;
&lt;li&gt;发现商业 LLM 的推理成本在商业上没有意义。&lt;/li&gt;
&lt;li&gt;有特定的安全要求，需要托管自己的 LLM 数据，并且不愿通过第三方 API 传递数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练自己的模型的一种方法是使用开源模型，例如 Llama 2、Mosaic MPT-7B、Falcon 或 Vicuna，其中许多还提供了商业使用许可证。这些通常根据它们具有的参数数量进行标记：7B、13B、40B 等。 “B”代表模型的参数数目，以及它可以处理和存储的信息量。数字越高，模型就越复杂和复杂，但训练和运行成本也越高。如果你的用例不复杂，并且如果你计划在性能相当强大的现代笔记本电脑上运行模型，那么具有较低参数的模型是开始的最佳且最经济的方法。&lt;/p&gt;
&lt;p&gt;中大型组织可能会选择从头开始构建和训练一个 LLM 模型。这是一条非常昂贵、资源密集且耗时的 AI 之路。你需要难以招聘的技术人才，并具备长时间迭代的机会，因此对大多数组织来说，这条路线不现实。&lt;/p&gt;
&lt;h3 id=&#34;微调-llm&#34;&gt;微调 LLM&lt;/h3&gt;
&lt;p&gt;一些组织选择中间路径：微调基本开源 LLM 以实现模型预训练能力之外的特定功能。如果你希望以你品牌独特的声音创建虚拟助手或基于真实客户购买构建的推荐系统，那么这是一个很好的选择。这些模型会随着你纳入排名靠前的用户交互而不断地训练自己。事实上，&lt;a href=&#34;https://voicebot.ai/2023/08/23/openai-brings-fine-tuning-to-gpt-3-5-turbo-and-gpt-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open AI 报告&lt;/a&gt;，使用此模型，可以将提示长度缩短多达 90%，同时保持性能不变。此外，Open AI 的商业 API 的最新增强功能使其与驱动 ChatGPT 和 Bing AI 的模型一样强大和易于访问。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>微软零成本收购 OpenAI？——OpenAI 的错位与微软的收益</title>
      <link>https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/</link>
      <pubDate>Tue, 21 Nov 2023 14:40:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/</guid>
      <description>&lt;p&gt;上个周末的 OpenAI“宫斗”大戏相信大家都知晓了，今天正巧看到 Ben Thompson 的这篇总结文章，感觉不错，分享给大家。&lt;/p&gt;
&lt;p&gt;关于作者：Ben Thompson（本·汤普森）是一位知名的科技和商业评论家，他是 Stratechery（《战略》）博客的创始人和作者。Stratechery 是一家专注于科技产业和商业策略分析的网站，汤普森在博客中深入剖析科技公司、产品、行业趋势和商业模式。他的文章通常涵盖了互联网、数字媒体、云计算、人工智能等领域的重要话题。&lt;/p&gt;
&lt;p&gt;Ben Thompson 因其深刻的洞察力和独特的分析方法而闻名，他的文章常常引发行业内外的广泛讨论。他的分析不仅关注技术本身，还关注技术如何塑造和影响商业和社会。由于其博客的高质量内容，他已经建立了一支忠实的读者群，并成为了科技产业和商业领域的重要声音之一。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;下文译自：&lt;a href=&#34;https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;摘要：
这篇文章分析了 OpenAI 的使命与商业模式之间的不一致，以及 Microsoft 如何从中受益。文章认为，OpenAI 的 GPT-3 模型是一种强大的通用人工智能，但是它的许可协议限制了它的应用范围。Microsoft 作为 OpenAI 的合作伙伴和投资者，拥有 GPT-3 的独家许可权，可以将其用于自己的产品和服务，从而获得竞争优势。&lt;/p&gt;
&lt;p&gt;正如您所预期的那样，我已经在脑海中和页面上撰写了几个版本的这篇文章，因为&lt;a href=&#34;https://twitter.com/benthompson/status/1726514608234746003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;我职业生涯中最不同寻常的周末&lt;/a&gt;已经开始。简要总结如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;周五，时任 CEO Sam Altman 被 OpenAI 的董事会解雇；接着，OpenAI 总裁 Greg Brockman 被免去董事会职务，并随后辞职。&lt;/li&gt;
&lt;li&gt;周末期间，有传闻称 Altman 正在谈判回归，然后 OpenAI 聘请了前 Twitch CEO Emmett Shear 担任 CEO。&lt;/li&gt;
&lt;li&gt;最后，在周日深夜，&lt;a href=&#34;https://twitter.com/satyanadella/status/1726509045803336122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satya Nadella 通过推文宣布&lt;/a&gt;，Altman 和 Brockman 以及他们的“同事”将加入 Microsoft。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;很显然，对于 Microsoft 来说，这是一个非凡的成果。该公司已经获得了&lt;a href=&#34;https://www.wsj.com/articles/microsoft-and-openai-forge-awkward-partnership-as-techs-new-power-couple-3092de51&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;所有 OpenAI 知识产权的永久许可证&lt;/a&gt;（&lt;a href=&#34;https://openai.com/our-structure#:~:text=the%20board%20determines%20when%20we%27ve%20attained%20AGI.%20Again%2C%20by%20AGI%20we%20mean%20a%20highly%20autonomous%20system%20that%20outperforms%20humans%20at%20most%20economically%20valuable%20work.%20Such%20a%20system%20is%20excluded%20from%20IP%20licenses%20and%20other%20commercial%20terms%20with%20Microsoft%2C%20which%20only%20apply%20to%20pre%2DAGI%20technology.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;除通用人工智能之外&lt;/a&gt;)，包括源代码和模型权重；问题是，如果 OpenAI 遭受了人才流失的威胁，Altman 和 Brockman 被解雇，它是否有足够的才能来利用那些知识产权。实际上，他们将拥有足够多的人才，似乎很有可能流向 Microsoft；你可以这样说，Microsoft 刚刚以零风险和零成本收购了 OpenAI。&lt;/p&gt;
&lt;p&gt;注：微软与 OpenAI 的最初协议还禁止微软基于 OpenAI 技术单独开发通用人工智能（AGI）；据我了解，这一条款已在最近的协议中被删除&lt;/p&gt;
&lt;p&gt;与此同时，对于 OpenAI 来说，这是一个损失，因为它在金钱和计算资源上依赖于总部位于雷德蒙德的 Microsoft 公司：OpenAI 员工在人工智能方面的工作要么因为永久许可证属于 Microsoft，要么因为员工加入了 Altman 的团队而成为 Microsoft 的直接财产。OpenAI 的王牌是 ChatGPT，它正在朝着技术的至高境界迈进——一个大规模的消费者平台——但是如果周末的报道可信，OpenAI 的董事会可能已经对 ChapGPT 给公司带来的激励产生了疑虑（后文详述）。&lt;/p&gt;
&lt;p&gt;然而，最大的损失是一个必然的损失：即除了盈利公司，任何其他组织形式都不是正确的公司组织方式。&lt;/p&gt;
&lt;h2 id=&#34;openai-的非营利模式&#34;&gt;OpenAI 的非营利模式&lt;/h2&gt;
&lt;p&gt;OpenAI 成立于 2015 年，是一家被称为“非营利智能研究公司”的组织。从他们的&lt;a href=&#34;https://openai.com/blog/introducing-openai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;最初博客文章&lt;/a&gt;中可以得知：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;OpenAI 是一家非营利人工智能研究公司。我们的目标是以最有可能造福整个人类的方式推进数字智能，不受需要产生财务回报的限制。由于我们的研究不受财务义务的制约，我们可以更好地专注于对人类产生积极影响。我们认为人工智能应该是个体人类意愿的延伸，在自由的精神中尽可能广泛和均匀地分布。这个冒险的结果是不确定的，工作是困难的，但我们认为目标和结构是正确的。我们希望这对领域最优秀的人来说是最重要的。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;对于 OpenAI 的创始人，特别是 Altman 和 Elon Musk 的动机，我曾经有过相当怀疑；我在一篇&lt;a href=&#34;https://stratechery.com/2015/openai-artificial-intelligence-and-data-data-and-recruiting/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;每日更新&lt;/a&gt;中写到：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Elon Musk 和 Sam Altman 分别领导着特斯拉和 YCombinator 等组织，这些组织看起来很像我刚刚描述的那两个受到 Google 和 Facebook 数据优势威胁的公司的例子，他们与 OpenAI 做到了这一点，还有使整个事情成为非营利性组织的额外动机；我说“动机”是因为成为非营利性组织几乎肯定更多地与我在开头强调的那句话有关：“我们希望这对领域中最优秀的人来说最重要。”换句话说，OpenAI 也许没有最好的数据，但至少它有一个可能帮助理想主义研究人员晚上睡得更香的使命结构。OpenAI 可能有助于平衡特斯拉和 YCombinator 的竞争局势，我猜我们应该相信这是一个愉快的巧合。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;无论 Altman 和 Musk 的动机如何，将 OpenAI 建立为非营利性组织的决定并不仅仅是口头上的；该公司是一个 501(c)(3) 组织；您可以在&lt;a href=&#34;https://projects.propublica.org/nonprofits/organizations/810861541&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;查看他们的年度 IRS 申报。Form 990 上的第一个问题要求组织“简要描述组织的使命或最重要的活动”；&lt;a href=&#34;https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2016 年的第一份申报&lt;/a&gt;中写道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;OpenAI 的目标是以最有可能使整个人类受益的方式推进数字智能，不受产生财务回报的需求限制。我们认为人工智能技术将有助于塑造 21 世纪，我们希望帮助世界构建安全的人工智能技术，并确保人工智能的利益尽可能广泛均衡地分布。我们试图作为更大社区的一部分构建人工智能，并希望在此过程中公开分享我们的计划和能力。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;两年后，“公开分享我们的计划和能力”的承诺消失了；三年后，“推进数字智能”的目标被替换为“构建通用人工智能”。&lt;/p&gt;
&lt;p&gt;2018 年，根据今年早些时候的一份 Semafor 报告，马斯克试图接管该公司，但被拒绝了；他离开了董事会，并且更为关键的是，停止了对 OpenAI 的运营支持。这导致了第二个关键背景信息：面对需要支付大量计算资源的需求，现在坚决掌控 OpenAI 的 Altman 创建了 OpenAI Global, LLC，这是一家有上限的营利性公司，微软是少数股东。这是 OpenAI 当前结构的图像来自&lt;a href=&#34;https://openai.com/our-structure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;他们的网站&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-openai-的公司结构&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;OpenAI 的公司结构&#34; srcset=&#34;
               /blog/openais-misalignment-and-microsofts-gain/openai-2_hu_ca45077af8128757.webp 400w,
               /blog/openais-misalignment-and-microsofts-gain/openai-2_hu_a777acc1672bdfda.webp 760w,
               /blog/openais-misalignment-and-microsofts-gain/openai-2_hu_7d634035d5ee96b3.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/openai-2_hu_ca45077af8128757.webp&#34;
               width=&#34;640&#34;
               height=&#34;464&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      OpenAI 的公司结构
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;OpenAI Global 可以筹集资金，而且对于其投资者来说至关重要的是，它可以盈利，但它仍然在非营利性组织和其使命的监督下运营；OpenAI Global 的经营协议规定：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该公司的存在是为了推进 OpenAI, Inc.确保开发安全的人工通用智能并使其造福全人类的使命。该公司对这一使命以及 OpenAI, Inc.宪章中提出的原则的责任优先于产生利润的任何义务。该公司可能永远不会盈利，也没有义务这样做。该公司可以自由重新投资公司的全部或部分现金流于研究和开发活动和/或相关费用，而无需向成员承担任何义务。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;chatgpt-族群&#34;&gt;ChatGPT 族群&lt;/h3&gt;
&lt;p&gt;第三个关键背景信息是最为人熟知的，也推动了这些雄心壮志达到了新的高度：ChatGPT 在 2022 年 11 月底发布，震撼了整个世界。如今，ChatGPT 拥有超过 1 亿的每周用户和超过 10 亿美元的收入；它还从根本上改变了几乎每个大公司和政府关于人工智能的讨论。&lt;/p&gt;
&lt;p&gt;但对我来说，最引人注目的是我上面提到的可能性，即 ChatGPT 成为一个新的重要消费者科技公司的基础，这是最有价值和最难建立的公司类型。我在今年早些时候在&lt;a href=&#34;https://stratechery.com/2023/the-accidental-consumer-tech-company-chatgpt-meta-and-product-market-fit-aggregation-and-apis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《偶然的消费者科技公司》&lt;/a&gt;中写道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在涉及有意义的消费者科技公司时，实际上产品是最重要的。消费者产品的关键在于高效的客户获取，这意味着口碑和/或网络效应；ChatGPT 实际上没有后者（是的，它会收到反馈），但它拥有大量的前者。事实上，ChatGPT 的出现最让我想到的产品是谷歌：它简直比市场上任何其他产品都要好，这意味着它来自一对大学生并不重要（起源故事有些相似！）。此外，就像谷歌一样，与扎克伯格对硬件的痴迷相反，ChatGPT 非常出色，人们会想方设法使用它。甚至没有一个应用程序！然而，仅仅四个月过去，已经有一个平台了。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;我所指的平台是&lt;a href=&#34;https://stratechery.com/2023/chatgpt-learns-computing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT 插件&lt;/a&gt;；这是一个令人着迷的概念，其用户界面不太完善，而在八个月后的&lt;a href=&#34;https://stratechery.com/2023/the-openai-keynote/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI 首个开发者日&lt;/a&gt;上，公司宣布了 GPTs，这是他们试图成为一个平台的第二次尝试。与此同时，据报道，Altman 正在探索 OpenAI 监管范围之外的新公司，以构建芯片和硬件，显然没有向董事会报备。这些因素的某种组合，或者可能尚未报道的其他因素，是董事会的最后一根稻草，由首席科学家 Ilya Sutskever 领导，他们在周末罢免了 Altman。&lt;a href=&#34;https://www.theatlantic.com/technology/archive/2023/11/sam-altman-open-ai-chatgpt-chaos/676050/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Atlantic 报道&lt;/a&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Altman 在上周五被 OpenAI 董事会解雇，这是公司两派意识形态的权力斗争的顶点，一派来自硅谷的技术乐观主义，充满了快速商业化的活力；另一方则深受人工智能代表对人类构成存在威胁的担忧，认为必须极度谨慎地控制。多年来，这两派成功地共存，虽然也出现了一些波折。&lt;/p&gt;
&lt;p&gt;根据现任和前员工的说法，几乎一年前的今天，正是 ChatGPT 的发布导致了 OpenAI 陷入全球关注的关键时刻。从外部看，ChatGPT 看起来像有史以来最成功的产品推出之一。它比历史上任何其他消费者应用程序都增长得更快，似乎单凭一己之力重新定义了数百万人对自动化的威胁和机遇的理解。但它使 OpenAI 朝着截然相反的方向发展，加剧了已经存在的意识形态分歧。ChatGPT 为盈利而创造产品的竞争加速了，同时给公司的基础设施和专注于评估和减轻技术风险的员工带来了前所未有的压力。这加剧了 OpenAI 派系之间已经紧张的关系，Altman 在 2019 年的员工电子邮件中称之为“族群”。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Altman 的“族群”——将 OpenAI 变得更像传统科技公司的一方——对于科技界的人们，包括我自己，肯定更加熟悉。我甚至在我的文章中关于开发者日演讲的段落中谈到了 OpenAI 的过渡，但不幸的是，我编辑掉了。以下是我写的内容：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;大约在这个时候，我再次开始哀叹&lt;a href=&#34;https://stratechery.com/2022/dall-e-open-to-all-openai-and-openness-openai-opportunities-and-threats/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI 奇怪的公司结构&lt;/a&gt;。作为长期关注硅谷的观察者，看到 OpenAI 追随传统的创业公司道路是令人愉快的：公司显然处于迅速扩张阶段，产品经理突然被认为是有用的，因为他们处于找到并交付低悬果实的甜蜜点，而这对于一个尚未拥有时间或壕沟来容忍王国建设和功能蔓延的实体来说是有益的。&lt;/p&gt;
&lt;p&gt;让我犹豫不决的是，目标不是上市，然后退休到游艇上，并向那些在消除极度富有的内疚感方面做得更好的事业提供资金。赚钱并回应股东的要求会控制更多救世主的冲动；当我听说 Altman 不拥有 OpenAI 的任何股权时，这使我更加紧张而不是宽慰。或者也许是因为我不会有 S-1 或 10-K 要分析。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;微软与董事会的对立&#34;&gt;微软与董事会的对立&lt;/h3&gt;
&lt;p&gt;在整个周末，科技界 Twitter 上的讨论大部分集中在董事会为何会烧掉如此多的价值上感到震惊。首先，Altman 是硅谷最有关联的高管之一，是一位多产的筹款人和交易谈判者；其次，一些 OpenAI 员工已经辞职，预计未来几天会有更多人辞职。OpenAI 以前可能有两个派系；可以合理地假设未来只会有一个，由新任 CEO Shear 领导，他将人工智能末日的概率定为&lt;a href=&#34;https://twitter.com/rowancheung/status/1726473420299534491&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;5% 至 50% 之间&lt;/a&gt;，并提倡&lt;a href=&#34;https://twitter.com/eshear/status/1703178063306203397&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;大幅减缓发展&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;然而，事实上是这样的：无论您是否同意 Sutskever/Shear 派的观点，董事会的宪章和责任不是为了赚钱。这不是一家受益人公司，其对股东具有信托责任；事实上，正如我上面所述，OpenAI 的宪章明确规定了它是“无需生成财务回报”。从这个角度来看，董事会实际上正在履行其职责，尽管这似乎有些违反直觉：在董事会认为 Altman 及其派别并没有“构建造福于人类的通用人工智能”的情况下，他们有权解雇他；他们这么做了。&lt;/p&gt;
&lt;p&gt;这涉及到了我对该公司非营利性地位的担忧的讽刺之处：我曾担心 Altman 没有受到赚钱的需要的限制，或者担心由某人负责，而他没有对结果有财务利益，而事实上正是这些因素让他失去了工作。更广泛地说，我的批评不够全面，因为在商业分析的案例中，对无约束权力的哲学关切相形见绌——至少在 OpenAI 成为与之进行交易的根本不稳定的实体方面如此。当然，这涉及到了微软，作为一直支持 Satya Nadella 领导的人，我不得不承认我对公司与 OpenAI 合作的分析存在不足之处。&lt;/p&gt;
&lt;p&gt;微软已经押注了其未来的大部分，这超越了金钱，微软拥有大量的金钱，其中很多尚未支付（或以 Azure 积分形式授予）；OpenAI 的技术已经内建到了微软的许多产品中，从 Windows 到 Office，甚至有些大多数人从未听说过的产品（我看到了你，Dynamics CRM 迷！）。微软还在为 OpenAI 量身定制的基础设施上进行大规模投资，纳德拉一直在宣传专业化的财务优势，而且刚刚发布了一款专门用于运行 OpenAI 模型的定制芯片。现在看来，微软将大量的承诺交给了一个非追求利润的实体，因此不受微软作为投资者和收入驱动因素的约束，这似乎是荒谬的。&lt;/p&gt;
&lt;p&gt;或者说，直到纳德拉在太平洋时间晚上 11:53 发推文如下：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-satya-nadella-发文宣布-sam-altman-加入微软&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Satya Nadella 发文宣布 Sam Altman 加入微软&#34; srcset=&#34;
               /blog/openais-misalignment-and-microsofts-gain/openai-1_hu_f6c353236e412f3d.webp 400w,
               /blog/openais-misalignment-and-microsofts-gain/openai-1_hu_52ed93ec2480f9af.webp 760w,
               /blog/openais-misalignment-and-microsofts-gain/openai-1_hu_b34d6e7c6d83e9a9.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/openai-1_hu_f6c353236e412f3d.webp&#34;
               width=&#34;640&#34;
               height=&#34;358&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Satya Nadella 发文宣布 Sam Altman 加入微软
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对我刚才提出的关于微软错误决定与非营利组织合作的论点的反驳是关于人工智能开发的现实，特别是对大量计算资源的需求。正是对这种计算资源的需求导致 OpenAI，其自身禁止进行传统的风险投资交易，向微软交出了知识产权。换句话说，尽管董事会可能拥有非营利公司的宪章，以及一种令人钦佩的愿意行动并坚守信念，但他们最终没有筹码，因为他们不是一家拥有资本以真正独立的公司。最终的结果是，一个以安全开发人工智能为宗旨的实体基本上已经将其所有工作，以及可能很快的时间内的一大部分人才，移交给了地球上最大的盈利实体之一。或者用与人工智能相关的框架来说，OpenAI 的结构最终与实现其所述使命不符。试图通过命令组织激励措施根本无法考虑在动态情况下可能发挥作用的所有可能情况和变量；出于良好的原因，收获自身利益一直是对齐个人和公司的最佳方式。&lt;/p&gt;
&lt;h3 id=&#34;关于-altman-的疑问&#34;&gt;关于 Altman 的疑问&lt;/h3&gt;
&lt;p&gt;董事会的行动还有一个角度值得承认：这很可能是有正当理由的。我支持&lt;a href=&#34;https://www.newcomer.co/p/give-openais-board-some-time-the&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eric Newcomer 在他的同名 Substack 上发表的深思熟虑的专栏文章&lt;/a&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在其&lt;a href=&#34;https://openai.com/blog/openai-announces-leadership-transition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;声明&lt;/a&gt;中，董事会表示他们得出了结论，认为 Altman“在与董事会的沟通中没有始终坦诚”。我们不应该因为糟糕的公开信息传递让我们忽视一个事实，那就是 Altman 失去了董事会的信任，而董事会本应该证明 OpenAI 的诚信&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;据我了解，一些董事会成员真诚地认为 Altman 在与他们的沟通中不诚实，不可靠，消息来源告诉我。董事会的一些成员认为他们无法监督公司，因为他们无法相信 Altman 所说的话。然而，非营利董事会的存在是 OpenAI 的&lt;a href=&#34;https://x.com/martin_casado/status/1723112508234539270?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;信誉&lt;/a&gt;的一个关键理由。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Newcomer 指出了我上面提到的董事会的宪章，Anthropic 的创始人觉得有必要首先离开 OpenAI，Musk 对 Altman 的敌意，以及 Altman 在离开 YCombinator 时仍然&lt;a href=&#34;https://www.newcomer.co/p/odds-and-ends?nthPub=1251&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;有些模糊和未解释清楚的离职&lt;/a&gt;。Newcomer 总结道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我相信写这封警示信不会让我在硅谷的许多角落受欢迎。但我认为我们应该放慢脚步，获取更多的事实。如果 OpenAI 将我们引向人工通用智能或接近的领域，我们将希望花更多的时间来思考我们想要由谁引领我们去那里&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Altman 拥有很大的权力，以及非营利组织的外衣，以及超过他更混杂的私人声誉的公开形象。他失去了董事会的信任。我们应该认真对待这一点。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;也许我因为上面提到的对微软分析的疏忽而感到有些谦卑，更不用说对周末深夜的命运逆转感到震惊了，但我要指出，&lt;a href=&#34;https://stratechery.com/2023/attenuating-innovation-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;我已经明确了立场&lt;/a&gt;，反对 AI 末日论者和呼吁监管的呼声；为此，我对确认本周末事件背后驱动力的假设性叙述感到警惕。并且，我要指出，我仍然担心那些试图在没有自己切身利益的情况下控制令人难以置信的能力的高管的哲学问题。&lt;/p&gt;
&lt;p&gt;为此，像 Altman 这样的初创生态系统固定成员加入微软当然令人惊讶：微软是唯一保留对 OpenAI 知识产权的地方，可以将其与有效无限的资金和 GPU 访问相结合，这无疑增加了权力控制人工智能是 Altman 主要动机的叙述的可信度。&lt;/p&gt;
&lt;h3 id=&#34;改变后的格局&#34;&gt;改变后的格局&lt;/h3&gt;
&lt;p&gt;显然，Altman 和 Microsoft 掌握了人工智能的主导权。微软拥有知识产权，并很快将拥有足够的团队，可以将其与资金和基础设施相结合，同时摆脱了他们与 OpenAI 之前合作中固有的协调问题（当然，他们仍然是 OpenAI 的合作伙伴！）。&lt;/p&gt;
&lt;p&gt;我也辩论了一段时间，外部公司建立在 Azure 的 API 上比建立在 OpenAI 的 API 上更有意义；微软天生就是一个开发平台，而 OpenAI 虽然有趣和令人兴奋，但很可能会克隆您的功能或淘汰旧的 API。现在，选择更加明显了。从微软的角度来看，这消除了企业客户避免使用 Azure 的主要原因之一，因为他们依赖 OpenAI；微软现在拥有完整的技术栈。&lt;/p&gt;
&lt;p&gt;与此同时，谷歌可能需要进行一些重大变革；公司的最新模型 Gemini 已经延迟推出，其云业务因支出转向人工智能而放缓，这正是公司所希望的完全相反的结果。公司的创始人和股东还能容忍公司进展太慢的看法多久，特别是与微软展示的灵活性和愿意承担风险的态度相比？&lt;/p&gt;
&lt;p&gt;这留给了 Anthropic，12 小时前看起来像是大赢家，现在作为一个独立实体感觉越来越脆弱。该公司已经与谷歌和亚马逊达成了合作协议，但现在面临着一个竞争对手微软，其拥有实际上无限的资金和 GPU 访问权限；很难摆脱这样一种感觉，即将其作为 AWS 的一部分是有道理的（是的，B 公司可以被收购，比非营利组织更容易）。&lt;/p&gt;
&lt;p&gt;然而，最终，可以提出这样的论点，即实际上并没有发生太大变化：很长一段时间以来，已经明显&lt;a href=&#34;https://stratechery.com/2023/ai-and-the-big-five/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI 至少在短期和中期内&lt;/a&gt;是一种持续的创新，而不是一种颠覆性的创新，也就是说，它主要会使最大的公司受益并部署。成本如此之高，以至于其他人很难获得资金，甚至在考虑渠道和客户获取问题之前。如果有一家公司有望加入 Big Five 的行列，那就是 OpenAI，多亏了 ChatGPT，但现在似乎不太可能了（但不是不可能）。最终，这是纳德拉的洞察力：如果您很大，赢得胜利的关键不是像初创公司那样发明，而是利用自己的规模来收购或快速跟随它们；如果您可以以 0 美元的低价格做到这一点，那就更好了。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
