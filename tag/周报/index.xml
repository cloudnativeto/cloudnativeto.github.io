<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>周报 | 云原生社区（中国）</title>
    <link>https://cloudnativecn.com/tag/%E5%91%A8%E6%8A%A5/</link>
      <atom:link href="https://cloudnativecn.com/tag/%E5%91%A8%E6%8A%A5/index.xml" rel="self" type="application/rss+xml" />
    <description>周报</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Tue, 07 May 2019 15:12:53 +0800</lastBuildDate>
    <image>
      <url>https://cloudnativecn.com/media/sharing.png</url>
      <title>周报</title>
      <link>https://cloudnativecn.com/tag/%E5%91%A8%E6%8A%A5/</link>
    </image>
    
    <item>
      <title>云原生生态周报（Cloud Native Weekly）第 3 期</title>
      <link>https://cloudnativecn.com/blog/cloud-native-weekly-03/</link>
      <pubDate>Tue, 07 May 2019 15:12:53 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/cloud-native-weekly-03/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;摘要：&lt;/em&gt; Docker Hub 遭入侵，19 万账号被泄露；Java 8 终于开始提供良好的容器支持；Snyk 年度安全报告出炉，容器安全问题形势空前严峻。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;业界要闻&#34;&gt;业界要闻&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cnbeta.com/articles/tech/841873.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Docker Hub 遭入侵，19 万账号被泄露&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;:&lt;/strong&gt; 4 月 25 日 Docker 官方邮件曝露，因为 Hub 的一个数据库收到非授权访问，影响了约 19 万用户的用户名和哈希后的密码，以及用户自动构建的 Github 和 Bitbucket Token。Docker 公司建议用户修改其登录密码。如果您在公有云上的应用依赖于来自 Docker Hub 的镜像，我们强烈建议您登录容器服务控制台更新相应的 docker login 信息或 kubernetes secret。此外，阿里云容器镜像服务&lt;a href=&#34;https://promotion.aliyun.com/ntms/act/acree.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;企业版&lt;/a&gt;提供网络访问控制、独享 OSS Bucket 加密存储等安全加固功能，最大程度保障您的镜像仓库的安全。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://blog.softwaremill.com/docker-support-in-new-java-8-finally-fd595df0ca54&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Java 8 终于开始提供良好的容器支持&lt;/strong&gt;&lt;/a&gt;**：**长久以来，容器 和 Java 就像一对“欢喜冤家”。一方面，容器技术的“不可变基础设施”特性为开发者带来了无比宝贵的依赖与环境一致性保证；但另一方面，Linux 容器通过 Cgroups 对应用进行资源限制的方式跟所有依赖于 JVM 进行资源分配的编程语言都产生了本质的冲突。而就在上周，最近发布的 OpenJDK 镜像 &lt;strong&gt;openjdk:8u212-jdk&lt;/strong&gt; 终于能够让 Java 8 运行时在容器里面为应用分配出合理的 CPU 数目和堆栈大小了。自此，发布 Java 容器应用的痛苦经历，可能要一去不复返了。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Snyk 年度安全报告出炉，容器安全问题形势空前严峻：&lt;strong&gt;知名开源安全厂商 Snyk 在年初发布了 2019 年度安全报告。报告中指出：“随着容器技术在 2019 年继续在 IT 环境中爆发式增长，针对容器安全的威胁正在迅猛增加，&lt;strong&gt;任何一家企业现在都必须比以往更加重视&lt;/strong&gt;&lt;a href=&#34;https://help.aliyun.com/document_detail/60751.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;容器镜像安全&lt;/strong&gt;&lt;/a&gt;&lt;/strong&gt;，并将此作为企业的首要任务&lt;/strong&gt;”。报告详情，请&lt;a href=&#34;https://snyk.io/opensourcesecurity-2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;点击此处查看全文&lt;/a&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;上游重要进展&#34;&gt;上游重要进展&lt;/h2&gt;
&lt;p&gt;Kubernetes 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;**Kubernetes 集群联邦 v1（Federation v1）正式宣布废弃。**K8s 社区近日宣布将 Federation v1 代码库正式废弃。Federation v1 即 Kubernetes 项目原“集群联邦”特性，旨在通过一个统一的入口管理多个 Kubernetes 集群。&lt;strong&gt;但是，这个特性逐步被设计成了在多个 Kubernetes 集群之上构建一个“Federation 层”的方式来实现&lt;/strong&gt;，从而背离了 Kubernetes 项目的设计初衷。最终，在 RedHat、CoreOS、Google 等多位社区成员的推动下，社区开始全面拥抱 &lt;a href=&#34;https://github.com/kubernetes-sigs/federation-v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Federation v2&lt;/a&gt;：&lt;strong&gt;一个完全旁路控制、以 K8s API 为核心的多集群管理方案。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/kubernetes/sig-release/tree/master/releases/release-1.15&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Kubernetes 1.15 进入发布节奏&lt;/strong&gt; &lt;/a&gt;K8s 1.15 发布进入日程，5 月 30 日即将 Code Freeze（即：不接受任何功能性 PR）。&lt;/li&gt;
&lt;li&gt;[**&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/958&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KEP] Ephemeral Containers KEP 合并，进入编码阶段**&lt;/a&gt;。
Ephemeral container 旨在通过在 Pod 里启动一个临时容器的方式，来为用户提供对 Pod 和容器应用进行 debug 和 trouble shooting 的能力。**这种通过“容器设计模式”而非 SSH 等传统手段解决运维难题的思路，对于“不可变基础设施”的重要性不言而喻，**阿里巴巴在“全站云化”过程中也采用了同样的设计来解决类似问题。在上游完成该功能的编码实现后，会通过 kubectl debug 命令方便用户直接使用。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Knative 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;**Knative 逐步弃用原 Build 项目。**按照计划，Tektoncd/Pipeline 子项目已经在 v0.2.0 时移除了对 Build 的依赖。最近，Knative Serving v1beta1 也移除了对 Build 的依赖，目前，社区已经开始制定弃用 Build 的确切方式并通知到 knative 开发者社区。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/knative/eventing/issues/930&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;knative 正在考虑为事件触发（Trigger）引入更高级的规则和策略。&lt;/strong&gt;&lt;/a&gt; 社区正在就 Advanced Filtering 设计一个 提案。该提案提议基于 &lt;a href=&#34;https://github.com/google/cel-spec/blob/9cdb3682ba04109d2e03d9b048986bae113bf36f/doc/intro.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CEL&lt;/a&gt; （Google 维护的一种表达式语言）来进行事件的过滤。具体来说，Trigger 的 filter 字段会增加一个 Spec 字段，然后在 Spec 字段下使用 CEL 语法完成对事件的过滤规则定义。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Istio/Envoy 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://istio.io/about/notes/1.1.4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Istio 1.1.4 本周正式发布&lt;/strong&gt;&lt;/a&gt;，其中一个重要的功能是更改了 Pilot 的默认行为，对出口流量的控制变化。除了之前通过 Service Entry 与配置特定范围 IP 段来支持访问外部服务，新版本中通过设置环境变量 PILOT_ENABLE_FALLTHROUGH_ROUTE 允许 Envoy 代理将请求传递给未在网格内部配置的服务。更多可以参考&lt;a href=&#34;https://yq.aliyun.com/articles/655489?source_type=cnvol_429_wenzhang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio 流量管理实践&lt;/a&gt;系列文章。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/envoyproxy/envoy/issues/6614&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Envoy 正通过 ORCA 改善负载均衡的精准度&lt;/strong&gt;&lt;/a&gt;。
目前 Envoy 可以用于进行负载均衡决策的信息主要是权重和连接数等信息，为了能让 Envoy 的负载均衡更加精准需要为 Envoy 提供更多的决策因素。比如本地和远程机器的负载情况、CPU、内存等信息，更复杂的还可以利用应用程序特定的指标信息来进行决策，比如队列长度。而 ORCA 的目的是定义 Envoy 和上游代理之间传递这些信息的标准。该功能的提出者希望 ORCA 可以成为 Universal Data Plane API (UDPA)。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/envoyproxy/envoy/pull/5910&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Envoy 正引入 RPC 去代替共享内存机制以便提高统计模块的的扩展性&lt;/strong&gt;&lt;/a&gt;。
Envoy 当下通过共享内存的方式来保存 stats 数据的这种方式存在很多局限性，比如需要限制 stats 使用固定的内存大小，当有大量集群的时候没办法扩展。这给他升级 stats 子系统的架构带来了不少的阻碍。因此他希望可以通过将 stats 数据以堆内存的方式来保存，然后通过 RPC 在新老进程中传递。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/envoyproxy/envoy/pull/6552&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Envoy 正在 xDS 协议中增加 VHDS 协议减小动态路由信息的更新粒度&lt;/strong&gt;&lt;/a&gt;。
现状是，Envoy 中的路由配置是通过 RDS 来动态更新的，但是 RDS 的粒度太粗了，包含了一个 Listener 下所有的路由配置信息。由于一个 Listener 下面可能会有多个服务，每一个服务对应一个 Virtual Host，因此在更新路由的时候，如果只是其中一个 Virtual Host 更新了，那么会导致所有的路由配置都重新下发而导致通讯负荷偏重。引入 VHDS 就是为了只下发变化的路由信息，从而将更新的路由配置信息量缩小。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Containerd 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/containerd/containerd/pull/3148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Non-root&lt;/strong&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/containerd/containerd/pull/3148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;用户运行&lt;/strong&gt;&lt;/a&gt;&lt;a href=&#34;https://github.com/containerd/containerd/pull/3148&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; &lt;strong&gt;containerd&lt;/strong&gt;&lt;/a&gt;&lt;strong&gt;：&lt;/strong&gt; 近日，社区正在尝试实现无需 root 权限就可以运行 containerd 的能力。在这种场景下，用户可以提前准备好容器所需要的 rootfs，但是 containerd 服务端在清理容器时依然会尝试去 unmount rootfs，对于没有 root 权限的 containerd 进程而言，该步骤必定会失败（mount 操作必须要有 root 权限）。目前 Pivotal 的工程师正在解决这个问题，这种 non-root 模式可以为解决云上安全问题提供新的思路，&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;开源项目推荐&#34;&gt;开源项目推荐&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;本周，我们向您推荐&lt;/strong&gt; &lt;a href=&#34;https://github.com/ilhaan/kubeCDN&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;kubeCDN&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;项目&lt;/strong&gt;。
kubeCDN 项目是一个基于 Kubernetes 实现的自托管 CDN 方案，只要将它部署在分布在不同地域（Region）的 Kubernetes 集群上，你就拥有了一个跨地域进行内容分发的 CDN 网络。而更重要的是，通过 kubeCDN，用户不再需要第三方的内容分发网络，从而重新控制了原本就属于自己的从服务器到用户设备的数据流。kubeCDN 目前只是一个个人项目**，但是这里体现出来的思想确实至关重要的：在不久的未来，每一朵云、每一个数据中心里都会布满 Kubernetes 项目，这将会成为未来云时代基础设施的“第一假设”。** 推荐你阅读 &lt;a href=&#34;https://www.infoq.cn/article/trfu-uB4FPhAB4uLvL4R?utm_source=tuicool&amp;amp;utm_medium=referral&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InfoQ 的解读文章&lt;/a&gt;来进一步了解 kubeCND。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;本周阅读推荐&#34;&gt;本周阅读推荐&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;《Knative 入门——构建基于 Kubernetes 的现代化 Serviceless 应用》中文版，这是一本 O’Reilly 出品的免费电子书，已经由 servicemesher 社区组织完成翻译。提供 &lt;a href=&#34;http://www.servicemesher.com/getting-started-with-knative/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;在线阅读&lt;/a&gt; 和 &lt;a href=&#34;http://t.cn/EaB8g6d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF 下载&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;信通院发起的云原生产业联盟出具《云原生技术实践白皮书》，白皮书系统性地梳理了云原生概念、关键技术、应用场景、发展趋势及实践案例。&lt;a href=&#34;https://files.alicdn.com/tpsservice/dd44ce32c783473b595382cad5857ef5.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF 链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;《&lt;a href=&#34;https://www.infoq.cn/article/7642QHo6vmZvQxFw9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;阿里云 PB 级 Kubernetes 日志平台建设实践&lt;/a&gt;》Kubernetes 近两年来发展十分迅速，已经成为容器编排领域的事实标准，但是 Kubernetes 中日志采集相对困难。本文来自 InfoQ 记者的采访，文中谈及了如何让使用者专注在“分析”上，远离琐碎的工作。&lt;/li&gt;
&lt;li&gt;《&lt;a href=&#34;https://programmaticponderings.com/2019/04/17/istio-observability-with-go-grpc-and-protocol-buffers-based-microservices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Istio Observability with Go, gRPC, and Protocol Buffers-based Microservices&lt;/a&gt;》，这是一篇很长的博文，介绍可以与 Istio 相适配的观测性组件，用实际的例子演示了如何对以 Go 语言、Protobuf 和 gRPC 为基础的微服务框架进行全面的观测。如果你还对 Prometheus、Grafana、Jaeger 和 Kiali 这几个组件感到既熟悉又陌生，并且好奇如何把它们组合在一起使用来提升微服务的可观测性，这个博客的内容应该会对你很有帮助。&lt;/li&gt;
&lt;li&gt;《&lt;a href=&#34;https://www.infoq.cn/article/hhk37_UC1FgJFCQyIk7c&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;云原生的新思考：为什么说容器已经无处不在了？&lt;/a&gt;》这篇文章在对云原生技术总结的同时，对未来应用趋势走向进行了展望。“云原生不但可以很好的支持互联网应用，也在深刻影响着新的计算架构、新的智能数据应用。以容器、服务网格、微服务、Serverless 为代表的云原生技术，带来一种全新的方式来构建应用。”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;名词解释：KEP - Kubernetes Enhancement Proposal，即 Kubernetes 上游设计文档&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本周报由阿里巴巴容器平台联合蚂蚁金服共同发布&lt;/p&gt;
&lt;p&gt;本周作者：张磊，临石，浔鸣，天千，至简，傅伟，汤志敏，王夕宁
责任编辑：木环&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>云原生生态周报（Cloud Native Weekly）第 2 期</title>
      <link>https://cloudnativecn.com/blog/cloud-native-weekly-02/</link>
      <pubDate>Tue, 23 Apr 2019 10:44:45 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/cloud-native-weekly-02/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/cloud-native-weekly-02/006tNc79ly1g2cdd5x4mfj31uo0m8di3_hu16923444954572749290.webp 400w,
               /blog/cloud-native-weekly-02/006tNc79ly1g2cdd5x4mfj31uo0m8di3_hu5463796587061588974.webp 760w,
               /blog/cloud-native-weekly-02/006tNc79ly1g2cdd5x4mfj31uo0m8di3_hu3484099937068859590.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/cloud-native-weekly-02/006tNc79ly1g2cdd5x4mfj31uo0m8di3_hu16923444954572749290.webp&#34;
               width=&#34;760&#34;
               height=&#34;253&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;本周报由阿里巴巴容器平台联合蚂蚁金服共同发布&lt;/p&gt;
&lt;p&gt;本周作者：傅伟，敖小剑，张磊，临石，南异，心贵，王夕宁，长虑&lt;/p&gt;
&lt;p&gt;责任编辑：木环&lt;/p&gt;
&lt;h2 id=&#34;业界要闻&#34;&gt;业界要闻&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.tuicool.com/articles/Jfaeqy2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes External Secrets&lt;/a&gt;  近日，世界上最大的域名托管公司 Godaddy 公司，正式宣布并详细解读了其开源的 K8s 外部 Secrets 管理项目： &lt;a href=&#34;https://github.com/godaddy/kubernetes-external-secrets&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes External Secrets&lt;/a&gt;，简称 KES。这个项目定义了 ExternalSecrets API，让开发者可以在 K8s 内部以和使用内部 Secret 相似的方式使用外部系统提供的 Secrets，大大简化了开发者为了让应用获取外部 Secrets 所需要的工作量。从安全的角度，这个方案降低了使用外部 Secret 时候的攻击面（外部 Secret 是通过一个 K8s API 暴露的，而不是之前的每个应用自己实现），也降低了应用在适配外部 Secret 时候的难度。另外，&lt;a href=&#34;https://github.com/AliyunContainerService/ack-kms-plugin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes KMS plugin 开源插件&lt;/a&gt; ，采用信封加密的方式与密钥管理能力结合，对进行 K8s secret 的存储加密。建议安全相关技术人员重点关注。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/y2V3PwOK5qbdmjFsuNTGkg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CNCF 官方宣布&lt;/a&gt;为中国开发者提供免费的云原生技术公开课。这些课程将专注于云原生技术堆栈，包括技术深度探索与动手实验课程，旨在帮助和指导中国开发人员在生产环境中使用云原生技术，并了解其用例和优势。此前，著名社区 Stackoverflow 发布了 2019 年开发者调研报告，报告有近九万人参与，Top 3 最受热爱开发平台是分别是 Linux（83.1%）、Docker（77.8%）和 Kubernetes（76.8%）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;上游重要进展&#34;&gt;上游重要进展&lt;/h2&gt;
&lt;p&gt;Kubernetes 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[重要性能优化] 2X performance improvement on both required and preferred PodAffinity. (&lt;a href=&#34;https://github.com/kubernetes/kubernetes/pull/76243&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#76243&lt;/a&gt;, &lt;a href=&#34;https://github.com/Huang-Wei&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@Huang-Wei&lt;/a&gt;) 这是一个重要的性能优化。这个提交将 PodAffinity 调度的效率实现了两倍的提高。&lt;strong&gt;要知道，PodAffinity/Anti-affinity 调度规则是目前 K8s 默认调度器最大的性能瓶颈点，这次修复很值得关注。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;[重要安全增强] &lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/944&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KEP: Node-Scoped DaemonSet&lt;/a&gt;: K8s 项目现在提供一种叫 Node-Scoped DaemonSet。这种 DaemonSet 的独特之处，在于它拥有、并且只能拥有自己所在的节点 kubelet 相同的权限，并且遵循同 kubelet 相同的鉴权流程。**这种设计，避免了以往 DaemonSet 权限泛滥的问题（比如：我们现在就可以让 DaemonSet 只能访问到属于该 Node 的 API 资源）。**这个特性发布后，DaemonSet 一直以来都 是 K8s 集群里最优先被黑客们关照的尴尬局面有望从根本上得到缓解。&lt;/li&gt;
&lt;li&gt;[重要功能补丁] &lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/953&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KEP: Add kubelet support lxcfs&lt;/a&gt;: 一直以来，容器里面通过 /proc 文件系统查看 CPU、内存等信息不准确都是一个让人头疼的问题，而挂载 lxcfs 则是这个问题的最常见解决办法。&lt;strong&gt;现在，K8s 上游正在提议将 lxcfs 作为默认支持&lt;/strong&gt;，如果得以合并的话，那对于开发者和运维人员来说，都是个喜闻乐见的事情。&lt;strong&gt;不过，lxcfs 本身是一个需要额外安装的软件，很可能会成为这个阻碍设计的 blocker。&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Knative 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[Serving v1beta1 API proposal(https://docs.google.com/presentation/d/10wuLMFXyol731WKuO5x7lalWrH0A6YVHa4exIERQaQ8/edit#slide=id.p)&lt;strong&gt;Knative serving API 准备升级到 v1beta1 版本&lt;/strong&gt;，其目标之一是使用标准的 PodSpec，以便更方便的从 K8s Deployment 迁移过来。这个版本和 v1alpha1 对比主要变更有：去掉了 runLatest，缺省默认就是 runLatest；Release 模式可以通过配置 traffic 实现，可以指定各个版本的流量比例；取消 Manual 模式；提供 revision 生成名字的控制；停用 Serving 内置的 Build。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/knative/eventing/issues/918&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Triggers don&amp;rsquo;t use Istio VirtualServices&lt;/a&gt;：Knative Eventing 原有的实现，依赖于 Istio 的 VirtualService 来重写 Host Header，使得接下来 Broker 可以通过 Host  Header 来识别此 Event 是发给哪个 Trigger 的。而最新的做法，则是通过  URL 来进行区分（比如：http://foo-broker-filter-1da3a.default.svc.cluster.local/my-trigger 代表此事件是发送给 my-trigger 的)，&lt;strong&gt;从而解除了 Trigger 对 Istio  VirtualService 的依赖&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/knative/eventing/issues/294&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Remove Istio as a dependency&lt;/a&gt;：除了上述解耦之外，Knative Eventing Channel 和 Bus 的绑定目前也是通过 istio 的 VirtualService 实现的。在这个新的实现方案中，Provisioners 直接把  Bus 的 主机名写到 channel 的状态当中，就不再需要 Istio VirtualService 来充当 Proxy 了。&lt;strong&gt;这些提交，都在透出这样一个事实：Knative 正在逐步减少对 Istio 的各种依赖，这对于一个真正、适用于更广泛场景的 Serverless 基础设施来说，是非常重要的。&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Istio 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;[重要安全增强]最近在 Envoy 代理中发现了两个安全漏洞（CVE 2019-9900 和 CVE 2019-9901）。这些漏洞现在已经在 Envoy 版本 1.9.1 中修补，并且相应地嵌入在 Istio 1.1.2 和 Istio 1.0.7 中的 Envoy 版本中。由于 Envoy 是 Istio 不可分割的一部分，因此建议用户立即更新 Istio 以降低这些漏洞带来的安全风险。&lt;/li&gt;
&lt;li&gt;[性能提升] Istio 1.1 中的新增强功能可以提高应用程序性能和服务管理效率，从而实现扩展，Pilot CPU 使用率降低了 90％, 内存使用率降低 50％。业界已有尝试在 Kubernetes 中使用 Pilot 实现服务的流量管理，对应用服务提供多版本管理、灵活的流量治理策略，以支持多种灰度发布场景。可以参考&lt;a href=&#34;https://yq.aliyun.com/articles/667297?source_type=cnvol_422_wenzhang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;通过 Istio 管理应用的灰度发布&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;containerd 项目&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/containerd/containerd/issues/3198&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;runc v2 shim &lt;/a&gt;&lt;a href=&#34;https://github.com/containerd/containerd/issues/3198&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;支持&lt;/a&gt;&lt;a href=&#34;https://github.com/containerd/containerd/issues/3198&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; cgroup &lt;/a&gt;&lt;a href=&#34;https://github.com/containerd/containerd/issues/3198&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;设置&lt;/a&gt;：containerd 目前支持多个容器使用&lt;a href=&#34;https://github.com/containerd/containerd/pull/3004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;同一个&lt;/a&gt;&lt;a href=&#34;https://github.com/containerd/containerd/pull/3004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt; containerd-shim &lt;/a&gt;&lt;a href=&#34;https://github.com/containerd/containerd/pull/3004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;来管理&lt;/a&gt; - 一个 Pod 就可以使用一个 containerd-shim 来管理一组容器，减少 containerd-shim 对系统资源的开销。但是目前新的 shim v2 没有提供配置 Cgroup 接口，这个功能会在 1.3 Release 中解决。有了这个能力之后，上层应用就可以将 containerd-shim 资源控制也纳入 Pod 资源管理体系中，严格控制系统服务占用的资源大小。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/containerd/containerd/issues/3210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;containerd 插件 ID 管理&lt;/a&gt;：containerd 允许开发者将自定义的组件注册到服务里，提供了可插拔的能力。但是当前 containerd 插件的管理是假设 ID 是唯一，这会导致相同 ID 的插件加载结果不可预测。当前该问题还在讨论中，计划在 1.3 Release 中解决。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;本周云原生最佳实践&#34;&gt;本周云原生最佳实践&lt;/h2&gt;
&lt;p&gt;传统富容器运维模式如何云原生化？&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在很多企业当中长期以来都在使用富容器模式，即：在业务容器里安装 systemd、sshd、监控进程等系统进程，模拟一个虚拟机的行为。这种运维方式固然方便业务迁入，但是也跟云原生理念中的“不可变基础设施”产生了本质冲突。比如：容器里的内容被操作人员频繁变化给升级、发布带来了众多运维隐患；富容器模式导致开发人员其实并不了解容器概念，在容器里随机位置写日志甚至用户数据等高风险的行为屡见不鲜。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;来自阿里巴巴“全站云化”的实践&lt;/strong&gt;：&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;将富容器容器运行时替换为支持 CRI 体系的标准容器运行时比如 containerd 等。目前阿里已经将 PouchContainer 全面升级为 containerd 发行版。&lt;/li&gt;
&lt;li&gt;把富容器里面的耦合在一起进程、服务进行拆分，变成一个 Pod 里的多个容器，下面是“全站云化”采用的拆分方法：   (1)  业务容器：运行业务主进程，允许 exec 方式进入；(2)  运维 Sidecar 容器：日志收集、debugger、运维辅助进程等；(3)  业务辅助容器：Service Mesh 的 agent&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;开源项目推荐&#34;&gt;开源项目推荐&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;本周我们向您推荐&lt;a href=&#34;https://spiffe.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPIFFE 项目&lt;/a&gt;。SPIFFE，从运维人员的第一感觉而言，是解决证书下发问题的。以往的安全体系更注重自然人的身份认证，而在 SPIFFE 里面所有的运行实体都有身份。一个案例就是 K8s 上的每个 pod 都配置相应的身份，对于多云和混合云的安全角度讲，SPIFFE 的好处在于不被供应商的安全认证体系绑定，可以达到跨云/跨域的身份认证，从而确保安全。下面是我们搜集的一些关于 SPIFFE 的不错的公开资料，有兴趣可以去了解：
&lt;ul&gt;
&lt;li&gt;项目的主要发起人 Evan 的&lt;a href=&#34;https://v.qq.com/x/page/t07113umnwq.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;演讲&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SPIRE 是 SPIFFE 的实现，和 Service Mesh 结合详见&lt;a href=&#34;https://segmentfault.com/a/1190000018432444&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这篇文章&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.aqniu.com/learn/39145.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SPIRE&lt;/a&gt;&lt;a href=&#34;https://www.aqniu.com/learn/39145.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;的零信任安全机制&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;本周阅读推荐&#34;&gt;本周阅读推荐&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;http://www.servicemesher.com/blog/knative-whittling-down-the-code/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative：精简代码之道&lt;/a&gt;，作者 Brian McClain | 译者 孙海洲。这篇文章用循序渐进的例子对“什么是 Knative”做出了很好的回答。如果你现在对 Knative 的认识还停留在三张分别叫做 Build，Serving 和 Eventing 的插图的话，那可能阅读一下这篇文章会让你对它们的理解更加形象。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://yq.aliyun.com/articles/695315?source_type=cnvol_422_wenzhang&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spark in action on Kubernetes - 存储篇&lt;/a&gt;，by Alibaba 莫源。存储永远是大数据计算的核心之一，随着新计算场景的不断涌现和硬件技术的飞速发展，存储的适配关系到大规模计算的成本、性能、稳定性等核心竞争要素。本文继上面分析 K8s 中的 Spark Operator 之后，从硬件限制、计算成本和存储成本几个角度，讨论了云原生时代来临后存储如何解决&lt;strong&gt;成本低、存得多、读写快&lt;/strong&gt;这几个挑战，详细介绍了阿里云上相关产品在不同场景下的表现，并总结了不同场景下适用的存储解决方案以及选择的原因。如果你是 K8s 和大数据方面的开发者和使用者，这是一篇你不应该错过的博客，可以快速的帮你梳理当前技术下存储的场景和典型解决方案。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>云原生生态周报（Cloud Native Weekly）第 1 期</title>
      <link>https://cloudnativecn.com/blog/cloud-native-weekly-01/</link>
      <pubDate>Tue, 16 Apr 2019 19:35:39 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/cloud-native-weekly-01/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;本周作者：张磊 临石 禅鸣 至简 宋净超&lt;/p&gt;
&lt;p&gt;编辑：木环&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这是 Cloud Native 周报第一期。&lt;/p&gt;
&lt;h2 id=&#34;业界要闻&#34;&gt;业界要闻&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在上周于旧金山举办的 Google Cloud Next 2019 大会上，Google Cloud 正式发布了：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/run/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run&lt;/a&gt;。这是一个跟 Microsoft Azure ACI，AWS Fargate 类似的容器实例服务。但与 ACI 和 Fargate 基于虚拟机技术栈的实现不同，Google 的 Cloud Run 服务则是基于 &lt;a href=&#34;https://github.com/knative/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Knative&lt;/a&gt; 这个 Kubernetes 原生的 Serverless 基础设施项目完成的。这也是业界第一个基于 Knative + Kubernetes + gVisor 体系的 Serverless 服务。此外，&lt;a href=&#34;https://cloud.google.com/run/pricing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cloud Run 的计费模型&lt;/a&gt;也颇具创新性：它不像 Fargate 那样完全按请求数目计费，而是将所有并发的请求算在一个计费单位内，这有望大大减低用户需要支付的成本。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cloud.google.com/traffic-director/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Traffic Director&lt;/a&gt;。一个与 AWS App Mesh 对标的 Service Mesh 产品。Traffic Director 通过 xDS 协议与数据平面的 Envoy 进行通讯，可分别与 Google Cloud 的&lt;a href=&#34;https://cloud.google.com/compute/docs/instance-groups/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIG&lt;/a&gt;和&lt;a href=&#34;https://cloud.google.com/load-balancing/docs/negs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NEG&lt;/a&gt;两款产品结合去提供 Service Mesh 的能力。Traffic Director 的功能与开源 Istio 项目中的 Pilot-discovery 相似，也复用了 Istio 的不少技术实现（比如，通过 iptables 完成流量透明拦截）。Traffic Director 支持全球负载均衡、集中式的集群健康检查、流量驱动的自动扩缩容等功能，帮助客户在全球部署与管理高弹性的无状态应用。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;关于 Google Cloud Next 上其他一些比较有意思的发布，你可以阅读 &lt;a href=&#34;https://techcrunch.com/2019/04/10/the-6-most-important-announcements-from-google-cloud-next-2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TechCrunch 上的这篇文章&lt;/a&gt;来进一步了解。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;上游重要进展&#34;&gt;上游重要进展&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/900&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KEP: Rebase K8s images to distroless&lt;/a&gt;  Kubernetes 即将使用 gcr.io/distroless/static:latest 作为 K8s 核心镜像和 addon 镜像的统一 base 镜像。优势如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使得镜像体积更小，也更加安全。&lt;/li&gt;
&lt;li&gt;极大的减少冗余的 K8s image 的数量。&lt;/li&gt;
&lt;li&gt;通过对底层镜像的统一管理，可以使得 K8s image 更加安全（比如 CVE 的防护），更易于维护。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/906/files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kustomize: Generator and Transformer Plugins&lt;/a&gt; 将 Kustomize 进行解耦，各项功能由各个 plugin 进行实现，现有的基础功能会作为内置插件。这意味着 Kubernetes 在向基于 Kustomize 的原生应用管理能力上又迈出了坚实的一步。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/830/files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Port Troubleshooting Running Pods proposal to KEP&lt;/a&gt; 为 kubectl 添加一个 debug 命令，开发者可以用这个命令来和特定 pod 中的所有容器进行交互和访问。这里的关键设计，在于 Kubernetes 巧妙的利用了 sidecar 容器实现了对应用的非侵入式的 debug，非常值得学习。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/887/files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;keps: sig-node: initial pod overhead proposal&lt;/a&gt; 这个 KEP（Kubernetes Enhancement Proposal）设计了一套机制，使 Pod 能够对系统层面的 额外资源消耗（overhead）进行审计。这里的 overhead 主要包括以下两个部分。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;系统组件比如 kubelet，Docker, Linux Kernel，以及 Fluentd 等日志组件带来的 额外资源消耗&lt;/li&gt;
&lt;li&gt;沙箱容器运行时（Sandbox container runtime，比如 KataContainers）本身因为虚拟化和独立 Guest Kernel 所带来的额外的资源消耗&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/enhancements/pull/909/files&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RuntimeClass scheduling] native scheduler support, ready to implement&lt;/a&gt; 在这个设计中，Kubernetes RuntimeClass 的信息会被 Kubernetes 直接转义成 Toleration/Taint 信息从而使用 Kubernetes 的默认调度器即可处理。这个设计实现后，Kubernetes 就有了根据应用的需求，自主选择使用 KataContainers 还是 Docker 来运行应用的能力，值得期待。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;开源项目推荐&#34;&gt;开源项目推荐&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/kubecost/introducing-kubecost-a-better-approach-to-kubernetes-cost-monitoring-b5450c3ae940&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubecost: 让你的 Kubernetes 服务花费一目了然&lt;/a&gt; 本周，我们强烈推荐你了解一下这个名叫 &lt;a href=&#34;https://github.com/kubecost&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubecost&lt;/a&gt; 的开源项目。它能够按照 Kubernetes 的原生 API 比如 Pod，Deployment，Service，Namespace 等概念逐层监控并详细的计算和展现出每一层上你的真实花费。更重要的是，无论你下层用的是 AWS 还是 GCP，Kubecost 内置的成本模型都可以应对自如。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;本周阅读推荐&#34;&gt;本周阅读推荐&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;技术博文：&lt;a href=&#34;https://leebriggs.co.uk/blog/2019/04/13/the-fargate-illusion.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《Fargate 幻象》&lt;/a&gt;by Lee Briggs。众所周知，Fargate 是 AWS 目前主推的容器实例服务产品。但是，Fargate 这种产品形态，是不是就是开发者想要的云产品的未来呢？本周，推荐你阅读一篇深入剖析 Fargate 服务的技术博文《Fargate 幻象》。这篇文章不仅能带你理解关于 Fargate 服务的方方面面，也能从一位开发者的角度，跟你聊聊作者眼中到底什么才是 Kubernetes 最具吸引力的“魔法”所在。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;技术博文：&lt;a href=&#34;https://itnext.io/benchmark-results-of-kubernetes-network-plugins-cni-over-10gbit-s-network-updated-april-2019-4a9886efe9c4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《Benchmark results of Kubernetes netwokr plugins (CNI) over 10Gbit/s network》&lt;/a&gt;，by Alexis Ducastel。这个系列博客专注对 K8s 不同 CNI 网络插件的性能测试，上一篇博客发布于 2018 年 11 月，随着 K8s 1.14 的发布，作者对 up-to-date 的网络插件的性能重新进行了对比。对比的 CNI 包括：Calico v3.3，Canal v3.3，Cilium 1.3.0，Flannel 0.10.0，Kube-router 0.2.1，Romana 2.0.2，WeavNet 2.4.1；对比的内容包括安装难度（Installation）、安全、性能和资源消耗。测试结果不出意外的说明了没有一个 CNI 是所有方面的全能冠军，如何根据自身的需求选择合适的 CNI 方案？阅读这篇文章也需要可以给你很多启发。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;技术博文：&lt;a href=&#34;https://crate.io/a/infrastructure-as-code-part-one/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《Infrastructure as Code, Part One》&lt;/a&gt;，by Emily Woods。Infrastructure as Code（IaC）是时下非常火热的概念，然而究竟什么是 IaC，谁应该去关心它，它能解决什么痛点，不同的人有不同的答案。这篇博客从一个常见的升级失败展开，讨论我们需要什么样的集群和应用管理方式，集群管理者和应用开发者究竟以什么样的方式共享知识才能更加高效的协作，并描述 IaC 的实践应该如何展开。这篇文章对每一个软件工程师都会有帮助。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;技术博文：&lt;a href=&#34;http://www.servicemesher.com/blog/data-plane-setup/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《Istio Sidecar 注入过程解密》&lt;/a&gt;by Manish Chugtu，崔秀龙 译。Sidecar 模式是 Istio 项目工作的核心依赖，也是 Kubernetes 项目“容器设计模式”的最重要的一种。那么你是否会好奇，Istio 中 Istio Sidecar 注入到底是如何完成的呢？相信这篇精心翻译的博文一定能帮你一解究竟。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;技术博文：&lt;a href=&#34;http://www.servicemesher.com/blog/istio-cni-note/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《Istio 学习笔记：Istio CNI 插件》&lt;/a&gt;by 陈鹏。当前实现将用户 Pod 流量转发到 proxy 的默认方式是使用 privileged 权限的 istio-init 这个 InitContainer 来做的（运行脚本写入 iptables），而 Istio CNI 插件的主要设计目标是消除这个 privileged 权限的 InitContainer，换成利用 k8s CNI 机制来实现相同功能的替代方案。你是否好奇过，这个改进到底是如何实现的？这篇文章，三言两语之间就能为你解释清楚。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
