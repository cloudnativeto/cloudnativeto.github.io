<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DeepFlow | 云原生社区（中国）</title>
    <link>https://cloudnative.to/tag/deepflow/</link>
      <atom:link href="https://cloudnative.to/tag/deepflow/index.xml" rel="self" type="application/rss+xml" />
    <description>DeepFlow</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Mon, 26 Dec 2022 12:00:00 +0800</lastBuildDate>
    <image>
      <url>https://cloudnative.to/media/sharing.png</url>
      <title>DeepFlow</title>
      <link>https://cloudnative.to/tag/deepflow/</link>
    </image>
    
    <item>
      <title>Kubernetes 服务异常排障过程全解密</title>
      <link>https://cloudnative.to/blog/k8s-service-exception-troubleshooting/</link>
      <pubDate>Mon, 26 Dec 2022 12:00:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/k8s-service-exception-troubleshooting/</guid>
      <description>&lt;p&gt;Kubernetes（K8s）是一个用于大规模运行分布式应用和服务的开源容器编排平台。K8s 让应用发布更加快速安全，让应用部署也更加灵活，但在带来这些便利性的同时，也给应用排障增加了 K8s 平台层面的复杂度，本篇文章将以常见的服务异常入手，来详细拆解 K8s 服务访问方式，以及如何利用现有的可观测体系来对 k8s 平台和应用服务进行快速排障。&lt;/p&gt;
&lt;h2 id=&#34;服务的访问方式&#34;&gt;服务的访问方式&lt;/h2&gt;
&lt;p&gt;开启 K8s 服务异常排障过程前，须对 K8s 服务的访问路径有一个全面的了解，下面我们先介绍目前常用的 K8s 服务访问方式（不同云原生平台实现方式可能基于部署方案、性能优化等情况会存在一些差异，但是如要运维 K8s 服务，则需要在一开始就对访问方式有一个了解）。&lt;/p&gt;
&lt;p&gt;方式一：&lt;strong&gt;集群内客户端通过 ClusterIP 访问集群内服务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-集群内客户端通过-clusterip-访问集群内服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;集群内客户端通过 ClusterIP 访问集群内服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_74752a4738cb903161b3e9059f811845.webp 400w,
               /blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_20ab70f82fa40d7f8ed546bd45c88ff7.webp 760w,
               /blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/1_huf5e4a27a00f9ae05b04b133384cfda67_160684_74752a4738cb903161b3e9059f811845.webp&#34;
               width=&#34;760&#34;
               height=&#34;571&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      集群内客户端通过 ClusterIP 访问集群内服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;从&lt;code&gt;访问逻辑拓扑&lt;/code&gt;来分析，集群内客户端 POD 访问的是集群内服务的 svc_name，然后在 svc 层进行 DNAT，将请求转发到对应的后端 POD。这个过程对应的&lt;code&gt;访问实现拓扑&lt;/code&gt;则要复杂不少：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step 1: client_pod 根据 DNS 配置，请求 DNS 服务器解析 svc_name，DNS 服务器会返回 svc_name 对应的 ClusterIP&lt;/li&gt;
&lt;li&gt;step 2: client_pod 请求 ClusterIP，Node 根据 kube-proxy 配置的 IPVS/IPTABLES 完成 DNAT&lt;/li&gt;
&lt;li&gt;step 3: 根据 DNAT 的结果，Node 将请求转发给对应的 server_pod，server_pod 可能与 client_pod 在同一个 Node，也可能在不同 Node，此差异主要体现在网络转发层面&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;方式二：&lt;strong&gt;集群外客户端通过 NodePort 访问集群内服务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-集群外客户端通过-nodeport-访问集群内服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;集群外客户端通过 NodePort 访问集群内服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_ada1825890c0d6a3d993f602f2d8b1dc.webp 400w,
               /blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_63f54ca416490ad2058e2c1a75722f73.webp 760w,
               /blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/2_hu452ee717f29cee2db389021a6e48abec_126582_ada1825890c0d6a3d993f602f2d8b1dc.webp&#34;
               width=&#34;760&#34;
               height=&#34;573&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      集群外客户端通过 NodePort 访问集群内服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;相比&lt;code&gt;方式一&lt;/code&gt;，&lt;code&gt;访问逻辑拓扑&lt;/code&gt;上 client 访问的区别是从 svc_name 替换为 nodeip:port。&lt;code&gt;访问实现拓扑&lt;/code&gt;主要过程如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step 1: client 直接请求 svc 对外暴露的 nodeip:port，如果是 LoadBalance 类型的服务，在此之前还会访问 LB（因为并不是 K8s 服务的中的特别能力，所以此处并无特别说明），请求转发到对应的 Node 上，Node 也会根据kube-proxy 配置的 IPVS/IPTABLES 完成 DNAT&lt;/li&gt;
&lt;li&gt;step 2: 与&lt;code&gt;方式一&lt;/code&gt;的 step 3 一致&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;方式三：&lt;strong&gt;集群外客户端通过 Ingress 访问集群内服务&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-集群外客户端通过-ingress-访问集群内服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;集群外客户端通过 Ingress 访问集群内服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_5c683e9e1e1718050d385c2ea7332915.webp 400w,
               /blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_a863ea2c1db8840377006529a194fcb9.webp 760w,
               /blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/3_hu7e9716df07c89790997c877b77a3ce7b_214154_5c683e9e1e1718050d385c2ea7332915.webp&#34;
               width=&#34;760&#34;
               height=&#34;642&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      集群外客户端通过 Ingress 访问集群内服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;code&gt;方式三&lt;/code&gt;相比前两种方式，引入了 Ingress 的概念，因此复杂度增加了非常多。&lt;code&gt;访问逻辑拓扑&lt;/code&gt;中外部 client 可以直接请求 url 而不是 ip 了，请求 url 会先到达 Ingress，由 Ingress 完成反向代理，转发给后端的 svc，svc 再完成 DNAT 转发给后端 POD。&lt;code&gt;访问实现拓扑&lt;/code&gt;会根据 ingress-controller 部署形式不同而有差异，ingress-controller &lt;strong&gt;非&lt;/strong&gt; hostnetwork 部署下文简称&lt;code&gt;部署模式一&lt;/code&gt;，ingress-controller hostnetwork 部署下文简称&lt;code&gt;部署模式二&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;step 1：外部 client 请求 DNS 服务器解析 url 的域名，DNS 服务会返回 ingress-controller 服务所在的 IP (如果前面有挂 LB，则访问的是 LB 的 IP，否则返回的是 ingress-controller 服务的 nodeip:port)&lt;/li&gt;
&lt;li&gt;step 2：此时&lt;code&gt;部署模式一&lt;/code&gt;，则需要按&lt;code&gt;方式二&lt;/code&gt;访问 ingress-controller（为避免画图过于复杂，未画出 ingress_controller_pod 分布在不同 Node 场景）；&lt;code&gt;部署模式二&lt;/code&gt;，请求到达 Node 后，则直接转给 ingress_controller_pod&lt;/li&gt;
&lt;li&gt;step 3：此为&lt;code&gt;部署模式一&lt;/code&gt;特有的步骤，参考&lt;code&gt;方式二&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;step 4/5/6：请求经过 ingress_controller_pod 后，已经确定需要转发的后端 svc，则按&lt;code&gt;方式一&lt;/code&gt;进行转发即可（为避免画图过于复杂，未画出 server_pod 分布在同 Node 场景）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;服务异常的排障思路&#34;&gt;服务异常的排障思路&lt;/h2&gt;
&lt;p&gt;了解服务的访问方式后，在遇到服务异常时，基于一套整体的排障思路来开展工作，更能事半功倍，接下来详细聊聊如何进行排障。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-服务异常的排障思路&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;服务异常的排障思路&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_bf5388560f752cb1b71d829d571b650e.webp 400w,
               /blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_becdd3b11f3af151d1def1c3428dcd17.webp 760w,
               /blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/4_hu2cb2a0c74d5544e3df3caead3ff0a368_31237_bf5388560f752cb1b71d829d571b650e.webp&#34;
               width=&#34;760&#34;
               height=&#34;470&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      服务异常的排障思路
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 1：&lt;strong&gt;确定 Node/POD 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;首先我们可以通过 Prometheus 提供的 Dashboard 来检查部署的实例的基础状态，通过直接查看应用的 CPU/内存/带宽等指标，确认目前部署实例的负载都在正常范围内。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-node&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;node&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_054fa3090e8dcfb2c67198fba2f27887.webp 400w,
               /blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_4fba6a1ef4df6d454806077a9b2dae72.webp 760w,
               /blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/5_hu3ac141d4e82bf3796bedcec107ced75c_169547_054fa3090e8dcfb2c67198fba2f27887.webp&#34;
               width=&#34;760&#34;
               height=&#34;375&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      node
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-pod&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;pod&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_c233bb5e096030d0498fd6ced0416705.webp 400w,
               /blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_fe5388917eb11cc9d44f8229577808df.webp 760w,
               /blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/6_hu990d3f2c141fd6f9c2270a56b6d54af3_292662_c233bb5e096030d0498fd6ced0416705.webp&#34;
               width=&#34;760&#34;
               height=&#34;334&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      pod
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 2：&lt;strong&gt;确定后端服务是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;然后我们可以通过 DeepFlow 提供的 Dashboard 查看后端服务的黄金指标：请求、异常、时延，以快速判断目前的服务是否在正常运行。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-后端服务&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;后端服务&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_16ffa651fcfa699abd1ec8da06be592c.webp 400w,
               /blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_96b25080b78279f7b10c2ed91efbf4fe.webp 760w,
               /blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/7_hu91e0f6d740bce7065cf9d25304d6922d_197431_16ffa651fcfa699abd1ec8da06be592c.webp&#34;
               width=&#34;760&#34;
               height=&#34;375&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      后端服务
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 3：&lt;strong&gt;确定 DNS 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;从前面&lt;strong&gt;服务的访问方式&lt;/strong&gt;一节可知，仅方式一/方式三的访问过程经过 DNS 服务，因此只有这两种场景才需要检查 DNS 服务是否异常，方式一和三都需要检查集群内的 DNS 服务是否异常，方式三相比方式一还需要检查 client 访问的集群外的 DNS 服务是否异常。对于 CoreDNS 本身，我们可以使用 Prometheus 提供的 Dashboard 来进行排障，对于排查应用服务访问 DNS 异常，我们可以使用 DeepFlow 提供的 Dashboard 查看 DNS 服务的请求、异常、时延指标。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-coredns&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;coredns&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_5296faef1387c2836bf8555e5a9adf1d.webp 400w,
               /blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_41ff233d1182b87668c74f92ec5ce803.webp 760w,
               /blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/8_hu58ed82a18ff2640bafe550048445dea7_229812_5296faef1387c2836bf8555e5a9adf1d.webp&#34;
               width=&#34;760&#34;
               height=&#34;405&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      coredns
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-dns&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;DNS&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_ae9a164663d73a94ba6a966bc16dd812.webp 400w,
               /blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_266cb7c6de06e5ee1d69e40e0f580231.webp 760w,
               /blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/9_hubcd4fb641810ec198e414b0df5f6a33d_312083_ae9a164663d73a94ba6a966bc16dd812.webp&#34;
               width=&#34;760&#34;
               height=&#34;416&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      DNS
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;如 DNS 服务无异常，则可直接用 ClusterIP 访问服务，如果能正常访问，那可以确定是 DNS 的问题，并且这个问题很大可能就是配置错误。&lt;/p&gt;
&lt;p&gt;step 4：&lt;strong&gt;确定 SVC 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;因为 SVC 的功能实际是 kube-proxy 同步配置到 IPVS/IPTABLES 来实现的，所以我们可以参考 &lt;strong&gt;step 1&lt;/strong&gt; 的排查步骤，把 kube-proxy 视作应用 POD，通过 Prometheus 提供的 Dashboard 查看 kube-proxy 是否正常。&lt;/p&gt;
&lt;p&gt;如果能确定应用服务运行正常，可以尝试直接访问后端 POD，如果能正常访问，则可以继续分析 SVC 问题，很大可能是 IPVS/IPTABLES 配置错误。&lt;/p&gt;
&lt;p&gt;step 5：&lt;strong&gt;确定 Ingress 是否异常&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在&lt;strong&gt;服务访问方式&lt;/strong&gt;中方式三的场景下，如果需要检查 Ingress 的状态，可以查看基于 ingress-controller 服务的状态/负载/请求日志等构建的 Dashboard。这一类 Dashboard 除了 Prometheus/DeepFlow 有提供之外，各个 API 网关的厂商也有提供，可以用 DeepFlow + 厂商提供的 Dashboard 结合进行分析，厂商会更关注网关本身的分析，DeepFlow 则更关注全链路分析，快速定位问题点。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-ingress&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;ingress&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_07b820b9533958df09528a1c5dca577d.webp 400w,
               /blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_d917b9a51ee64f68baa20caf40dc8a38.webp 760w,
               /blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/10_hu9614c7e93e3fc6c761946b1c966264e3_527189_07b820b9533958df09528a1c5dca577d.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      ingress
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;step 6：&lt;strong&gt;追踪访问路径异常点&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;上述排障过程，都是独立的一个个点检查，如果都没问题，则可以去追踪报障的某一次访问路径是否有异常。如果能直接定位访问路径，确认问题点就会变得更简单。比如我们发现访问路径如果存在断路，则分析断路位置即可；如果追踪的是时延高的问题，则分析追踪到的每一段路径的时延即可。访问路径需要能覆盖从应用-&amp;gt;系统-&amp;gt;网络各个层面，目前提供这样全链路追踪能力的组件不多，可以使用 DeepFlow 自动化的分布式追踪能力来进行排查。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-追踪访问路径&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;追踪访问路径&#34; srcset=&#34;
               /blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_3c8ae998802f92c7fd3484880512762c.webp 400w,
               /blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_8bf0f8fdd120c1f9476a466ec255a8f4.webp 760w,
               /blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/k8s-service-exception-troubleshooting/11_hu2a274b323a344e5e9205d3737fdabeb3_45404_3c8ae998802f92c7fd3484880512762c.webp&#34;
               width=&#34;760&#34;
               height=&#34;130&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      追踪访问路径
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;什么是-deepflow&#34;&gt;什么是 DeepFlow&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/deepflowys/deepflow&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow&lt;/a&gt; 是一款开源的高度自动化的可观测性平台，是为云原生应用开发者建设可观测性能力而量身打造的全栈、全链路、高性能数据引擎。DeepFlow 使用 eBPF、WASM、OpenTelemetry 等新技术，创新的实现了 AutoTracing、AutoMetrics、AutoTagging、SmartEncoding 等核心机制，帮助开发者提升埋点插码的自动化水平，降低可观测性平台的运维复杂度。利用 DeepFlow 的可编程能力和开放接口，开发者可以快速将其融入到自己的可观测性技术栈中。&lt;/p&gt;
&lt;p&gt;GitHub 地址：https://github.com/deepflowys/deepflow&lt;/p&gt;
&lt;p&gt;访问 &lt;a href=&#34;https://deepflow.yunshan.net/docs/zh/install/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepFlow Demo&lt;/a&gt;，体验高度自动化的可观测性新时代。&lt;/p&gt;
&lt;h2 id=&#34;参考文档&#34;&gt;参考文档&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://deepflow.yunshan.net/docs/zh/about/overview/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://deepflow.yunshan.net/docs/zh/about/overview/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/1860-node-exporter-full/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/1860-node-exporter-full/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/15661-1-k8s-for-prometheus-dashboard-20211010/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/15661-1-k8s-for-prometheus-dashboard-20211010/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/9614-nginx-ingress-controller/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/9614-nginx-ingress-controller/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://grafana.com/grafana/dashboards/14981-coredns/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://grafana.com/grafana/dashboards/14981-coredns/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/service/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nginx.com/products/nginx-ingress-controller/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.nginx.com/products/nginx-ingress-controller/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/haproxytech/kubernetes-ingress#readme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/haproxytech/kubernetes-ingress#readme&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/services-networking/service-topology/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kubernetes.io/docs/concepts/services-networking/service-topology/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mp.weixin.qq.com/s/mp5coRHPAdx5nIfcCnPFhw&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://mp.weixin.qq.com/s/mp5coRHPAdx5nIfcCnPFhw&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
