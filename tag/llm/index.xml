<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM | 云原生社区（中国）</title>
    <link>https://cloudnativecn.com/tag/llm/</link>
      <atom:link href="https://cloudnativecn.com/tag/llm/index.xml" rel="self" type="application/rss+xml" />
    <description>LLM</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Mon, 21 Apr 2025 16:40:00 +0800</lastBuildDate>
    <image>
      <url>https://cloudnativecn.com/media/sharing.png</url>
      <title>LLM</title>
      <link>https://cloudnativecn.com/tag/llm/</link>
    </image>
    
    <item>
      <title>大语言模型是怎么工作的？通俗解释版</title>
      <link>https://cloudnativecn.com/blog/how-llms-work-explained-without-math/</link>
      <pubDate>Mon, 21 Apr 2025 16:40:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/how-llms-work-explained-without-math/</guid>
      <description>&lt;p&gt;我相信你也会同意，现在已经无法忽视&lt;strong&gt;生成式 AI&lt;/strong&gt;（Generative AI，简称 GenAI）了。关于大语言模型（Large Language Models，LLMs）的新闻铺天盖地。你很可能已经用过 &lt;a href=&#34;https://chat.openai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT&lt;/a&gt;，甚至一直把它开着当助手用。&lt;/p&gt;
&lt;p&gt;但很多人心中有一个基本疑问：这些模型看上去“很聪明”，这种“聪明”到底是从哪儿来的？&lt;/p&gt;
&lt;p&gt;这篇文章就是想用简单的方式、尽量不涉及复杂数学，来解释文本生成模型是如何工作的，帮助你把它们当作&lt;strong&gt;计算机算法&lt;/strong&gt;来理解，而不是神奇魔法。&lt;/p&gt;
&lt;h2 id=&#34;大语言模型到底在做什么&#34;&gt;大语言模型到底在做什么？&lt;/h2&gt;
&lt;p&gt;我们先来澄清一个很多人对大语言模型的误解。很多人以为这些模型是“会聊天”、“会答题”的，但实际上，它们唯一真正擅长的事情是：&lt;/p&gt;
&lt;p&gt;👉 &lt;strong&gt;给它一段文字，它就根据上下文“猜”下一个词（准确说是“Token”）是什么。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以，我们可以从“Token”这个核心概念开始揭开 LLM 的神秘面纱。&lt;/p&gt;
&lt;h3 id=&#34;什么是-token&#34;&gt;什么是 Token？&lt;/h3&gt;
&lt;p&gt;Token（词元）是大语言模型处理文本的最小单位。&lt;/p&gt;
&lt;p&gt;你可以大致把它当成是“词”，但实际上，Token 既可以是一个字母、一个词，也可以是一段词根、甚至是空格或标点。LLM 的目标是&lt;strong&gt;尽可能高效地表示文本&lt;/strong&gt;，所以不会总是按单词来切分。&lt;/p&gt;
&lt;p&gt;一个语言模型的全部 Token 列表就叫它的“词汇表”（vocabulary）。这个词汇表是通过一种叫做 &lt;a href=&#34;https://en.wikipedia.org/wiki/Byte_pair_encoding&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BPE（字节对编码）&lt;/a&gt; 的算法，从大量语料中训练出来的。&lt;/p&gt;
&lt;p&gt;举个例子，开源的 &lt;a href=&#34;https://github.com/openai/gpt-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-2&lt;/a&gt; 模型就使用了一个包含 &lt;strong&gt;50,257 个 Token&lt;/strong&gt; 的词汇表。&lt;/p&gt;
&lt;p&gt;每个 Token 都有一个唯一编号，模型会用一个叫做 &lt;strong&gt;tokenizer（分词器）&lt;/strong&gt; 的工具，把你输入的文字转换成 Token 编号列表。你也可以在 Python 中尝试这个过程：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;pip install tiktoken
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后在 Python 中运行以下代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;tiktoken&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tiktoken&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encoding_for_model&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt-2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;The quick brown fox jumps over the lazy dog.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;# 输出: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290, 13]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;464&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# &amp;#39;The&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2068&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# &amp;#39; quick&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;     &lt;span class=&#34;c1&#34;&gt;# &amp;#39;.&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;可以看到：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Token &lt;code&gt;464&lt;/code&gt; 表示 &amp;ldquo;The&amp;rdquo;&lt;/li&gt;
&lt;li&gt;Token &lt;code&gt;2068&lt;/code&gt; 表示 &amp;quot; quick&amp;quot;（注意前导空格也包括进去了）&lt;/li&gt;
&lt;li&gt;Token &lt;code&gt;13&lt;/code&gt; 表示句号 &amp;ldquo;.&amp;rdquo;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于分词是算法决定的，有时候你会发现一些奇怪的现象：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;The&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# [464]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;the&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;    &lt;span class=&#34;c1&#34;&gt;# [1169]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39; the&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;   &lt;span class=&#34;c1&#34;&gt;# [262]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;同一个词，根据是否有大小写、是否有前导空格，会被编码成不同的 Token。&lt;/p&gt;
&lt;p&gt;此外，&lt;strong&gt;使用频率低的词&lt;/strong&gt;不会单独占用一个 Token，而是被拆成多个 Token。例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;encode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Payment&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;      &lt;span class=&#34;c1&#34;&gt;# [19197, 434]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;19197&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;        &lt;span class=&#34;c1&#34;&gt;# &amp;#39;Pay&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;encoding&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;decode&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;434&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;          &lt;span class=&#34;c1&#34;&gt;# &amp;#39;ment&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;预测下一个-token&#34;&gt;预测下一个 Token&lt;/h3&gt;
&lt;p&gt;正如前面提到的，语言模型的任务就是&lt;strong&gt;预测下一个 Token 是什么&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;假设你已经输入了：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;The&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; quick&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; brown&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; fox&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;然后运行伪代码：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_token_predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;The&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; quick&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; brown&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; fox&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;])&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这一步会返回一份&lt;strong&gt;概率分布表&lt;/strong&gt;，告诉你在这个上下文下，每个 Token 作为下一个词的概率是多少。&lt;/p&gt;
&lt;p&gt;以 GPT-2 为例，这个返回值就是一个包含 &lt;strong&gt;50,257 个浮点数&lt;/strong&gt; 的列表，每个数表示对应 Token 出现在下一个位置的概率。&lt;/p&gt;
&lt;p&gt;你可以想象，像 &amp;ldquo;jumps&amp;rdquo; 这样的词很可能被赋予较高概率，而像 &amp;ldquo;potato&amp;rdquo; 这样的无关词概率则接近 0。&lt;/p&gt;
&lt;p&gt;为了能做出这种预测，模型必须经过一个&lt;strong&gt;训练过程&lt;/strong&gt;：它读了大量的文本，从中学习“哪些词通常跟随哪些词”。&lt;/p&gt;
&lt;p&gt;最终它构建了一个复杂的数据结构，能根据输入 Token 序列预测下一个 Token 的概率。&lt;/p&gt;
&lt;p&gt;这和你原来想象的一样吗？现在你是不是开始觉得，这其实并没有那么神秘了？&lt;/p&gt;
&lt;h3 id=&#34;生成一段完整的文字&#34;&gt;生成一段完整的文字&lt;/h3&gt;
&lt;p&gt;因为语言模型每次只能预测一个下一个 token（词元），所以如果我们想要它生成一整段句子，唯一的办法就是&lt;strong&gt;通过循环多次调用模型&lt;/strong&gt;，每次生成一个新的 token，直到长度足够。&lt;/p&gt;
&lt;p&gt;每一次循环中，模型都会根据当前的输入预测下一个 token 的概率分布，然后从中选择一个 token，添加到输入序列的末尾，再继续下一轮预测。这就像接力一样，模型每次生成一个词，然后拿这个词继续接着往下写。&lt;/p&gt;
&lt;p&gt;我们来看一段更完整的伪代码（Python 风格）：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;generate_text&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;num_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hyperparameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tokenize&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;i&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;num_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;get_token_predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;next_token&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;select_next_token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hyperparameters&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;append&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;next_token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;generate_text()&lt;/code&gt; 函数接受一个用户的输入提示（prompt），比如一句话或者一个问题。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tokenize()&lt;/code&gt; 是一个辅助函数，它会用如 &lt;code&gt;tiktoken&lt;/code&gt; 这样的工具把文本转成 Token 编号的列表。&lt;/li&gt;
&lt;li&gt;在 &lt;code&gt;for&lt;/code&gt; 循环中，每轮会调用 &lt;code&gt;get_token_predictions()&lt;/code&gt;，也就是实际调用 AI 模型，让它预测下一个 token 的概率。&lt;/li&gt;
&lt;li&gt;然后通过 &lt;code&gt;select_next_token()&lt;/code&gt; 这个函数，从这些概率中选出一个 token 来作为输出。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这个选 token 的函数可以用多种策略：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;最简单的是&lt;strong&gt;选择概率最高的那个&lt;/strong&gt;（在机器学习中称为“贪婪选择”/greedy selection）；&lt;/li&gt;
&lt;li&gt;更聪明的方法是&lt;strong&gt;用随机性来加点变化&lt;/strong&gt;：根据概率分布用随机数决定哪个 token 被选中，这样同一个 prompt 可以生成不一样的内容，增加“创造力”。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了进一步控制生成结果的风格，我们可以用一些**超参数（hyperparameters）**来影响 token 的选择过程。这些超参数是通过参数传给 &lt;code&gt;generate_text()&lt;/code&gt; 的：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;比如 &lt;code&gt;temperature&lt;/code&gt;（温度）参数：它会影响模型对“冷门”词的选择倾向。温度越高，概率分布会“拉平”，选择一些低概率 token 的可能性就会增加，生成结果就更有想象力。&lt;/li&gt;
&lt;li&gt;还有 &lt;code&gt;top_p&lt;/code&gt; 和 &lt;code&gt;top_k&lt;/code&gt;，用于控制模型考虑的“最可能的几个 token”，从中再做选择。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一旦选定了下一个 token，就会进入下一轮循环，把这个新 token 加入输入中，继续预测下一个 token，直到生成足够长度的内容。&lt;/p&gt;
&lt;p&gt;注意：这个过程不懂得“句子”或“段落”的概念，它只是一个词接一个词地往下预测。所以生成的文本可能会在句子中间突然结束。为了解决这个问题，&lt;code&gt;num_tokens&lt;/code&gt; 参数可以设置为“最多生成几个 token”，而不是固定数量。你还可以设置当生成出句号 token（如 &lt;code&gt;.&lt;/code&gt;）时就自动结束。&lt;/p&gt;
&lt;p&gt;如果你读到这里并理解了上述内容，恭喜你！你已经掌握了 LLM 的基本工作机制。如果你还想更进一步了解，那么接下来我们会进入稍微技术一点的内容（但依然尽量避免复杂数学）。&lt;/p&gt;
&lt;h2 id=&#34;模型是如何训练的&#34;&gt;模型是如何训练的？&lt;/h2&gt;
&lt;p&gt;谈到模型是怎么训练出来的，其实很难完全避开数学。不过这里我会用一个非常简单直观的例子来帮助你理解这个过程。&lt;/p&gt;
&lt;p&gt;因为语言模型的核心任务是“预测接下来可能出现的 token”，所以最基础的训练方式，就是去统计在训练语料中连续出现的 token 对（也就是“前一个词+下一个词”的组合），并建立一个概率表。&lt;/p&gt;
&lt;p&gt;我们从一个简单的例子开始：假设模型的词汇表中只有以下 5 个 token：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;I&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;you&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;like&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;apples&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;bananas&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;为了简化，我们这里不考虑空格和标点符号。&lt;/p&gt;
&lt;p&gt;训练语料由三句话组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I like apples&lt;/li&gt;
&lt;li&gt;I like bananas&lt;/li&gt;
&lt;li&gt;you like bananas&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;现在我们可以构建一个 5×5 的表格，行表示“前一个 token”，列表示“后一个 token”。我们统计每个组合在语料中出现的次数：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;I&lt;/th&gt;
          &lt;th&gt;you&lt;/th&gt;
          &lt;th&gt;like&lt;/th&gt;
          &lt;th&gt;apples&lt;/th&gt;
          &lt;th&gt;bananas&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;I&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;you&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;like&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;1&lt;/td&gt;
          &lt;td&gt;2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;apples&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;bananas&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;这个表表示：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;“I like” 出现了 2 次；&lt;/li&gt;
&lt;li&gt;“you like” 出现了 1 次；&lt;/li&gt;
&lt;li&gt;“like apples” 出现了 1 次；&lt;/li&gt;
&lt;li&gt;“like bananas” 出现了 2 次。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来我们把这张“频次表”转换成“概率表”。方法是：每一行中出现的次数加总后，按比例转换成概率。&lt;/p&gt;
&lt;p&gt;例如，&amp;ldquo;like&amp;rdquo; 这一行有两种后续词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;apples&amp;rdquo; 出现 1 次；&lt;/li&gt;
&lt;li&gt;&amp;ldquo;bananas&amp;rdquo; 出现 2 次；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总共 3 次，所以：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;apples&amp;rdquo; 的概率是 1/3 ≈ 33.3%；&lt;/li&gt;
&lt;li&gt;&amp;ldquo;bananas&amp;rdquo; 的概率是 2/3 ≈ 66.7%。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;于是最终的概率表如下（空格表示概率为 0%）：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;I&lt;/th&gt;
          &lt;th&gt;you&lt;/th&gt;
          &lt;th&gt;like&lt;/th&gt;
          &lt;th&gt;apples&lt;/th&gt;
          &lt;th&gt;bananas&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;I&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;100%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;you&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;100%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;like&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;33.3%&lt;/td&gt;
          &lt;td&gt;66.7%&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;apples&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;bananas&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;25%&lt;/td&gt;
          &lt;td&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;你可能注意到了，&amp;ldquo;apples&amp;rdquo; 和 &amp;ldquo;bananas&amp;rdquo; 的那两行看上去很奇怪 —— 因为在我们的训练数据中，它们后面没有再跟任何词，这就是训练数据中的“空洞”。&lt;/p&gt;
&lt;p&gt;为了让模型不会在遇到这些 token 时“卡住”，我采用了一种折中办法：给每个可能的下一个 token 都分配了相等的概率（即平均分配）。虽然这样做不一定准确，但至少能保证模型不会直接“失效”。&lt;/p&gt;
&lt;h3 id=&#34;关于训练数据空洞的思考&#34;&gt;关于“训练数据空洞”的思考&lt;/h3&gt;
&lt;p&gt;这个例子中的“空洞”很明显，在真实的大模型中可能不容易察觉。但即使是使用了超大规模语料的数据集，也还是可能存在一些训练不足的区域。&lt;/p&gt;
&lt;p&gt;这些“稀疏区域”的预测质量通常比较差，虽然看起来语句没问题，但可能会出现事实错误、逻辑不通等现象 —— 这就是所谓的 &lt;strong&gt;幻觉（hallucination）&lt;/strong&gt;，即模型生成的内容“像真的，但其实是假的”。&lt;/p&gt;
&lt;h3 id=&#34;模拟实现如何用这张概率表生成预测&#34;&gt;模拟实现：如何用这张概率表生成预测？&lt;/h3&gt;
&lt;p&gt;现在你已经有了这个概率表，那么语言模型内部的 &lt;code&gt;get_token_predictions()&lt;/code&gt; 函数其实就可以非常简单地实现了：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;get_token_predictions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;input_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;last_token&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;input_tokens&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;probabilities_table&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;last_token&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;你传入一串 token（比如用户的 prompt），模型只会关注最后一个 token，然后返回它在概率表中对应的一行。&lt;/p&gt;
&lt;p&gt;例如输入：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s1&#34;&gt;&amp;#39;you&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39;like&amp;#39;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;函数就会读取 &amp;ldquo;like&amp;rdquo; 这一行，告诉你：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;apples&amp;rdquo; 的概率是 33.3%&lt;/li&gt;
&lt;li&gt;&amp;ldquo;bananas&amp;rdquo; 的概率是 66.7%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么在上一个伪代码中 &lt;code&gt;select_next_token()&lt;/code&gt; 函数就有 1/3 的概率选中 &amp;ldquo;apples&amp;rdquo;，2/3 的概率选中 &amp;ldquo;bananas&amp;rdquo;。&lt;/p&gt;
&lt;p&gt;如果模型选择了 &amp;ldquo;apples&amp;rdquo;，生成的句子就是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-text&#34; data-lang=&#34;text&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;you like apples
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这是一句&lt;strong&gt;训练数据中没有&lt;/strong&gt;的句子，但它却是非常合理的。模型仅凭过去学到的词序关系，就能“合成”出一个看似全新的输出。&lt;/p&gt;
&lt;p&gt;你现在应该能理解：模型的“创造力”其实是通过把已学过的词汇和模式&lt;strong&gt;巧妙地拼接在一起&lt;/strong&gt;而来，这种方式虽然没有真正的“思考”，但效果却很惊艳。&lt;/p&gt;
&lt;h3 id=&#34;上下文窗口context-window&#34;&gt;上下文窗口（Context Window）&lt;/h3&gt;
&lt;p&gt;在上一节中，我用来训练迷你语言模型的方法，属于一种叫做&lt;a href=&#34;https://zh.wikipedia.org/wiki/%e9%a9%ac%e5%b0%94%e5%8f%af%e5%a4%ab%e9%93%be&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;马尔可夫链（Markov chain）&lt;/a&gt;的技术。&lt;/p&gt;
&lt;p&gt;这种方法的一个核心问题是：它**只考虑上一个 token（词元）**来做预测。也就是说，在选择下一个 token 时，之前出现的所有内容都会被“遗忘”。因此，我们可以说这种方法的上下文窗口是 &lt;strong&gt;1 个 token&lt;/strong&gt; —— 实在太小了。&lt;/p&gt;
&lt;p&gt;当上下文窗口这么小，模型就像失忆症一样，每次预测都只能看到最近的一个词，很难保持上下文的一致性或连贯性。&lt;/p&gt;
&lt;h3 id=&#34;如果增大上下文窗口呢&#34;&gt;如果增大上下文窗口呢？&lt;/h3&gt;
&lt;p&gt;一个办法是：&lt;strong&gt;扩展概率表，使用更长的 token 序列作为上下文&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;比如：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若使用 2 个 token 做上下文（窗口大小为 2），就需要在概率表中添加所有可能的两词组合；&lt;/li&gt;
&lt;li&gt;用我前面举的例子（5 个 token），就需要再添加 5×5=25 行，再加上原本的 5 行，一共 30 行；&lt;/li&gt;
&lt;li&gt;模型训练时要学会每三个 token 的组合关系，才能预测下一个 token；&lt;/li&gt;
&lt;li&gt;运行时，每次都取最后两个 token 作为上下文，用来查找对应的概率分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不过，&lt;strong&gt;2 个 token 作为上下文依然远远不够&lt;/strong&gt;。想要生成更自然、上下文更一致的文本，我们需要更长的上下文窗口。否则，模型无法让新生成的 token 和之前的内容保持一致或有逻辑联系。&lt;/p&gt;
&lt;p&gt;那我们需要多大窗口呢？&lt;/p&gt;
&lt;p&gt;如果窗口从 2 提高到 3，那么就需要 5×5×5 = 125 个组合，表格就要增加 125 行。但效果仍然很有限。&lt;/p&gt;
&lt;h3 id=&#34;如果用马尔可夫链来实现-gpt-2-的上下文窗口&#34;&gt;如果用马尔可夫链来实现 GPT-2 的上下文窗口？&lt;/h3&gt;
&lt;p&gt;GPT-2 的上下文窗口是 &lt;strong&gt;1024 个 token&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果我们用马尔可夫链来实现这样的模型，就意味着我们要准备一个超大的概率表，每一行都代表一串最多 1024 个 token 的组合。&lt;/p&gt;
&lt;p&gt;举个例子，如果词汇表中只有 5 个 token，那么可能的 1024 长度的组合数量是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;pow&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1024&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;55626846462680034577255817933310101605480399.&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;..&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;后面还有上百位&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这是一个&lt;strong&gt;天文数字级别的行数&lt;/strong&gt;，光是存这个表都需要超出地球所有硬盘的总容量！&lt;/p&gt;
&lt;p&gt;而且这还只是考虑长度为 1024 的组合。实际上我们还要支持长度为 1、2、3……1023 的组合，因为用户的输入可能并没有那么长。那整个表的规模会更大，完全不可行。&lt;/p&gt;
&lt;h3 id=&#34;马尔可夫链方法的极限&#34;&gt;马尔可夫链方法的极限&lt;/h3&gt;
&lt;p&gt;随着模型的发展：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GPT-3 的上下文窗口提升到 2048；&lt;/li&gt;
&lt;li&gt;GPT-3.5 提升到了 4096；&lt;/li&gt;
&lt;li&gt;GPT-4 起步是 8192，后来又提升到 32K，甚至是 128K（没错，是 12.8 万个 token）；&lt;/li&gt;
&lt;li&gt;最新的研究中，甚至出现了支持 &lt;strong&gt;上百万 token 上下文窗口&lt;/strong&gt; 的模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这意味着模型已经能够理解和关联更大段落之间的语义，生成的内容也更加连贯、一致。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;马尔可夫链&lt;/strong&gt;是一种很棒的思维工具，它让我们理解“预测下一个 token”的基本逻辑。但它&lt;strong&gt;不适合构建真正的大语言模型&lt;/strong&gt;，因为它在上下文长度、存储空间和可扩展性方面有根本性的瓶颈。&lt;/p&gt;
&lt;p&gt;为了处理大规模上下文，我们需要更智能、更高效的机制 —— 而这正是神经网络和 Transformer 架构真正擅长的领域。&lt;/p&gt;
&lt;h3 id=&#34;从马尔可夫链到神经网络&#34;&gt;从马尔可夫链到神经网络&lt;/h3&gt;
&lt;p&gt;显然，我们已经无法继续依赖“概率表”的方式了 —— 因为如果想支持较大的上下文窗口，那么这张表将变得大得离谱，根本无法存在于内存中。&lt;/p&gt;
&lt;p&gt;那我们该怎么办？&lt;/p&gt;
&lt;p&gt;答案是：&lt;strong&gt;用一个函数代替表格&lt;/strong&gt;。这个函数不再存储所有可能性，而是通过算法“算”出一个近似的概率分布 —— 这正是&lt;strong&gt;神经网络&lt;/strong&gt;擅长的事情。&lt;/p&gt;
&lt;p&gt;神经网络是一种特殊类型的函数，它：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;接收一些输入（比如 token）；&lt;/li&gt;
&lt;li&gt;对这些输入进行复杂的计算；&lt;/li&gt;
&lt;li&gt;输出一个结果 —— 在语言模型中，就是下一个 token 的概率分布。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为什么说它“特殊”？&lt;/p&gt;
&lt;p&gt;因为神经网络的计算行为不光由代码控制，还依赖于大量&lt;strong&gt;外部定义的参数（parameters）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;这些参数在一开始是未知的、随机的，所以模型一开始的输出几乎毫无用处。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;训练的过程&lt;/strong&gt;，就是找到那些&lt;strong&gt;最适合的参数组合&lt;/strong&gt;，使得神经网络的预测结果尽可能地贴近训练数据中的真实情况。&lt;/p&gt;
&lt;p&gt;训练过程会反复进行多轮，每一轮对参数进行一点点微调，这个过程叫做&lt;a href=&#34;https://en.wikipedia.org/wiki/Backpropagation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;反向传播（Backpropagation）&lt;/a&gt;，它本质上是一个数学优化过程（本文不会深入讨论）。&lt;/p&gt;
&lt;p&gt;每次参数更新之后，模型会再次评估它在训练数据上的表现，再基于反馈继续调整参数。这个循环会持续进行，直到模型能在训练集上做出合理的预测为止。&lt;/p&gt;
&lt;p&gt;为了帮助你理解神经网络的规模，我们来看几个数据：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;GPT-2&lt;/strong&gt;：约 15 亿个参数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-3&lt;/strong&gt;：1750 亿个参数&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT-4&lt;/strong&gt;：据说达到了 1.76 万亿个参数！&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练这种规模的神经网络，即使使用现代最强的计算资源，也需要花费数周甚至数月。&lt;/p&gt;
&lt;p&gt;由于神经网络中隐藏了如此多的参数，而且这些参数全是通过自动化迭代得出的，&lt;strong&gt;连开发者自己都很难完全理解模型内部是如何“思考”的&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;一个训练完成的 LLM 更像是一个&lt;strong&gt;黑盒&lt;/strong&gt; —— 你看不到它的内部“逻辑”，只能观察它的输入输出。就连训练它的人也很难解释它为什么会给出某个答案。&lt;/p&gt;
&lt;h3 id=&#34;层layerstransformer-和注意力机制attention&#34;&gt;层（Layers）、Transformer 和注意力机制（Attention）&lt;/h3&gt;
&lt;p&gt;你也许会好奇：这些神经网络到底在内部做了些什么？它们是如何“把一堆 token 输入，经过参数的计算，然后输出合理的下一个 token 的概率”的？&lt;/p&gt;
&lt;p&gt;这就涉及到神经网络的“分层计算”结构。&lt;/p&gt;
&lt;p&gt;一个神经网络由多个称为**层（layer）**的计算模块组成。其基本流程是：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;第一层接收输入；&lt;/li&gt;
&lt;li&gt;对输入做一次变换（数学运算）；&lt;/li&gt;
&lt;li&gt;把变换后的结果传给下一层；&lt;/li&gt;
&lt;li&gt;重复这个过程，直到最后一层生成输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;每一层都像一个“加工车间”，把输入“改造”一下，然后再传递下去。&lt;/p&gt;
&lt;p&gt;机器学习研究者会设计不同类型的层，用来处理不同的数据类型，比如图像或文本。有些层是通用的，有些则是专门为处理“token 化后的文本”设计的。&lt;/p&gt;
&lt;p&gt;目前在大语言模型中，最常用的神经网络架构叫做 &lt;strong&gt;Transformer&lt;/strong&gt;。这种架构非常适合处理序列数据（例如自然语言中的单词序列），它的核心机制是：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;注意力机制（Attention）&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Attention 能帮助模型理解“在上下文中，哪些 token 比较重要”，并基于这种理解生成更合理的下一个词的概率分布。&lt;/p&gt;
&lt;p&gt;最早，这种机制是用在机器翻译中的，用来判断“源语言的句子中，哪些词在翻译中最关键”。它可以模拟人类阅读时“聚焦重点词语”的过程。&lt;/p&gt;
&lt;p&gt;今天，Attention 已成为 Transformer 架构的核心能力，使得 LLM 能够从长上下文中捕捉信息，从而生成连贯、逻辑一致的文本。&lt;/p&gt;
&lt;p&gt;如果你已经理解了这一部分，那么你就已经初步了解了大语言模型“背后的魔法”其实是&lt;strong&gt;数学 + 神经网络 + 算法工程&lt;/strong&gt;的结合成果。下一步，我们可以探讨这些模型是否真的“具备智能”。&lt;/p&gt;
&lt;h2 id=&#34;大语言模型真的有智能吗&#34;&gt;大语言模型真的“有智能”吗？&lt;/h2&gt;
&lt;p&gt;看到这里，你可能已经开始有自己的判断：LLM（大语言模型）在生成文本时，是否体现出某种“智能”。&lt;/p&gt;
&lt;p&gt;就我个人而言，我并不认为 LLM 具有真正的推理能力，或者能够产生原创性的思想。但这并不意味着它们没有价值。&lt;/p&gt;
&lt;p&gt;凭借对上下文中 token 的精妙计算，LLM 能够识别出用户提示（prompt）中的模式，并将其与训练时学到的相似模式进行匹配。它生成的文本，虽然大多是从训练数据中“拼拼凑凑”而来，但拼接的方式非常巧妙，&lt;strong&gt;看起来像是“自己想出来的”&lt;/strong&gt;，而且往往很有用。&lt;/p&gt;
&lt;p&gt;不过，考虑到 LLM 存在**幻觉（hallucination）**的倾向（即生成内容看起来合理但却是虚假的），我不建议把 LLM 的输出直接交给最终用户使用，除非事先经过人工审核和验证。&lt;/p&gt;
&lt;p&gt;那么，未来几个月或几年中出现的更大规模的 LLM，有可能具备真正的智能吗？&lt;/p&gt;
&lt;p&gt;我认为，&lt;strong&gt;至少在目前 GPT 架构的框架下，还很难达到真正意义上的智能&lt;/strong&gt; —— 因为它还有很多根本性的限制。但谁知道呢？也许未来的某些创新，真的能带我们走得更远。&lt;/p&gt;
&lt;h2 id=&#34;写在最后&#34;&gt;写在最后&lt;/h2&gt;
&lt;p&gt;感谢你一路读到这里！&lt;/p&gt;
&lt;p&gt;我希望这篇文章能够激起你继续深入学习的兴趣，哪怕最后你真的要面对那些“可怕的数学”，因为如果你想彻底理解 LLM 的每一个细节，那是绕不过去的。&lt;/p&gt;
&lt;p&gt;如果你准备好面对这些挑战，我强烈推荐 Andrej Karpathy 的视频课程：&lt;a href=&#34;https://karpathy.ai/zero-to-hero.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Networks: Zero to Hero&lt;/a&gt;。它是理解神经网络从入门到精通的绝佳起点。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>平台工程师的 LLM 入门指南</title>
      <link>https://cloudnativecn.com/blog/a-gentle-introduction-to-llms-for-platform-engineers/</link>
      <pubDate>Tue, 15 Apr 2025 11:33:31 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/a-gentle-introduction-to-llms-for-platform-engineers/</guid>
      <description>&lt;p&gt;技术世界日新月异。如今最火的莫过于 AI。作为平台工程师，我们本身已经身处技术栈的洪流之中：容器、Kubernetes、Prometheus、Istio、ArgoCD、Zipkin、Backstage.io …… 技术名词一个接一个，每一个都复杂、抽象且需要深入理解。现在又来了个 AI，让人头大。大多数平台工程师根本没有时间或精力去琢磨什么是 LLM、大模型，更别说在系统中落地使用。&lt;/p&gt;
&lt;p&gt;但现实是：AI 正悄然渗透进平台工程的世界。我们终将需要理解和掌握它。本文尝试用通俗易懂的方式，帮助平台工程师快速建立起对 LLM（大语言模型）的基础认知，并思考它在云原生领域中的应用场景。&lt;/p&gt;
&lt;h2 id=&#34;1-ai-是智能助手而不是天外来物&#34;&gt;1. AI 是“智能助手”而不是“天外来物”&lt;/h2&gt;
&lt;p&gt;你可能用过 Siri，也可能在酒店网站上与机器人客服打过交道。大多数情况下，它们都让人失望——要么不理解你的问题，要么机械地回复固定答案。它们多数基于传统的机器学习或预设规则，无法真正理解你的意图。&lt;/p&gt;
&lt;p&gt;相比之下，现代的 LLM（如 ChatGPT）已经可以处理极为复杂的语言输入，甚至能根据上下文推理、总结信息，和人类进行近乎自然的对话。&lt;/p&gt;
&lt;p&gt;但问题来了：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对平台工程师来说，LLM 到底是什么？它跟传统 API、控制器、CI/CD 流水线有什么关系？&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;别急，我们从一个核心问题讲起——“它能做什么”。&lt;/p&gt;
&lt;h2 id=&#34;2-llm-能做什么像人一样理解文档和日志&#34;&gt;2. LLM 能做什么：像人一样理解文档和日志&lt;/h2&gt;
&lt;p&gt;设想一个企业内部的聊天助手，帮助员工快速了解公司的规范、流程、产品特点。当客户提出技术问题时，员工可以通过这个助手快速定位问题、给出答案。这种助手背后就是一个被企业文档、知识库、过往案例、甚至源码“喂养”过的 LLM。&lt;/p&gt;
&lt;p&gt;对比一下：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;功能&lt;/th&gt;
          &lt;th&gt;人工&lt;/th&gt;
          &lt;th&gt;LLM&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;阅读全部文档&lt;/td&gt;
          &lt;td&gt;慢&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;理解概念&lt;/td&gt;
          &lt;td&gt;可&lt;/td&gt;
          &lt;td&gt;可&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;回答问题&lt;/td&gt;
          &lt;td&gt;慢&lt;/td&gt;
          &lt;td&gt;快&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;LLM 的强大之处，在于它可以“吞掉”TB 级别的数据，然后从中提炼出概念与模式。听起来是不是像搜索引擎？不，它远远超过了搜索引擎。&lt;/p&gt;
&lt;h2 id=&#34;3-不只是搜索是理解&#34;&gt;3. 不只是搜索，是“理解”&lt;/h2&gt;
&lt;p&gt;传统搜索引擎依赖关键词匹配，比如你搜索“database timeout”，它只会返回包含这些词的文档。如果真实错误日志写的是“SQL connection lost”，你就查不到了。&lt;/p&gt;
&lt;p&gt;而 LLM 能理解“database timeout”与“SQL连接丢失”、“查询超时”、“数据库网络延迟”之间的语义联系。它不仅能从日志、trace 和文档中抓出相关内容，还能像一个资深工程师一样，总结出可能原因。&lt;/p&gt;
&lt;p&gt;这才是 LLM 的本事：&lt;strong&gt;不仅能搜索，还能理解、总结、推理。&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-使用自然语言交互甚至可以生成代码&#34;&gt;4. 使用自然语言交互（甚至可以生成代码）&lt;/h2&gt;
&lt;p&gt;LLM 可以像人类一样理解自然语言，还能用自然语言输出答案。例如：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;问：引擎故障灯亮了，启动时有咔哒声，怎么回事？
答：可能是电池电量不足或启动电机故障……（给出详细分析）&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;更惊人的是，它还能生成代码、撰写文档、总结聊天记录、处理用户请求……它甚至可以读懂老旧系统的接口文档，然后自动生成集成代码！&lt;/p&gt;
&lt;p&gt;对于平台工程师而言，LLM 可以：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;帮你总结应用日志&lt;/li&gt;
&lt;li&gt;快速生成 Kubernetes YAML 或 Terraform 模板&lt;/li&gt;
&lt;li&gt;自动生成 CI/CD 流水线步骤说明&lt;/li&gt;
&lt;li&gt;撰写插件或脚本（例如 ArgoCD 的 Plugin、Backstage 的 Template）&lt;/li&gt;
&lt;li&gt;甚至为 SRE 分析告警和异常根因&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;5-如何接入-llm熟悉的-http-接口&#34;&gt;5. 如何接入 LLM？熟悉的 HTTP 接口！&lt;/h2&gt;
&lt;p&gt;最棒的是，LLM 通常通过 HTTP API 暴露服务。&lt;/p&gt;
&lt;p&gt;平台工程师早就熟悉这个套路了：写一个 HTTP 请求，传入 JSON，接收 JSON 响应。&lt;/p&gt;
&lt;p&gt;来看个例子，调用 OpenAI API 查询 Siri 是如何工作的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;curl https://api.openai.com/v1/chat/completions &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -H &lt;span class=&#34;s2&#34;&gt;&amp;#34;Content-Type: application/json&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -H &lt;span class=&#34;s2&#34;&gt;&amp;#34;Authorization: Bearer &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$OPENAI_API_KEY&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt; &lt;span class=&#34;se&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;se&#34;&gt;&lt;/span&gt;  -d &lt;span class=&#34;s1&#34;&gt;&amp;#39;{
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    &amp;#34;model&amp;#34;: &amp;#34;gpt-3.5-turbo&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    &amp;#34;messages&amp;#34;: [
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;      {
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;,
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;        &amp;#34;content&amp;#34;: &amp;#34;Do you know how Siri works?&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;      }
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;    ]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s1&#34;&gt;  }&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;返回内容如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;id&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;chatcmpl-Avpw5BwQ4HypBRJFpqg3pPeeqDRwS&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;model&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;gpt-3.5-turbo-0125&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;choices&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;nt&#34;&gt;&amp;#34;message&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;role&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;assistant&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;        &lt;span class=&#34;nt&#34;&gt;&amp;#34;content&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Um... I mean... does it though?&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;      &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;],&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;usage&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;prompt_tokens&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;completion_tokens&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;107&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;nt&#34;&gt;&amp;#34;total_tokens&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;121&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;你会注意到几个要点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;请求是一个标准的 HTTP API 调用&lt;/li&gt;
&lt;li&gt;请求体是自然语言，响应也是自然语言&lt;/li&gt;
&lt;li&gt;响应中包含 token 数量（因为使用 LLM 通常按 token 计费）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，作为平台工程师，你可以用 API Gateway 做调用限流、配额管理、成本控制，还可以做安全网关。&lt;/p&gt;
&lt;h2 id=&#34;6-背后的原理其实很简单但也很神奇&#34;&gt;6. 背后的原理其实很简单（但也很神奇）&lt;/h2&gt;
&lt;p&gt;虽然 LLM 看起来很“神”，但它的核心原理其实很简单：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;接收一串单词（tokens），然后预测下一个最可能的词。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The cow jumped over the ___” → “moon”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;就是这么简单的过程，重复进行数百次，就组成了一个完整回答。&lt;/p&gt;
&lt;p&gt;这个过程背后依赖大量训练数据和昂贵的硬件，但核心机制就是概率预测。&lt;/p&gt;
&lt;p&gt;推荐阅读： 👉 &lt;a href=&#34;https://blog.miguelgrinberg.com/post/how-llms-work-explained-without-math&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How LLMs work explained without math&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;7-注意事项不是银弹也有风险&#34;&gt;7. 注意事项：不是银弹，也有风险&lt;/h2&gt;
&lt;p&gt;LLM 带来了新的能力，也伴随着新的风险，尤其在平台工程中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;准确性&lt;/strong&gt;：LLM 可能自信满满地说错话，在合规或运维场景中可能带来严重问题&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据隐私&lt;/strong&gt;：若使用的是 SaaS 模型，输入的数据可能泄露（例如 OpenAI）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;成本控制&lt;/strong&gt;：token 计费方式容易产生隐性费用，建议用网关管理配额&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;响应质量&lt;/strong&gt;：LLM 的输出不是文档原文，可能偏离主题或引入“幻觉”&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;品牌风险&lt;/strong&gt;：若未设置过滤机制，LLM 输出可能引发不当或带偏见内容&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;依赖过重&lt;/strong&gt;：部分用户过度依赖模型输出，忽略人工判断与验证&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;合规问题&lt;/strong&gt;：如 GDPR、HIPAA 等法规限制使用 AI 处理敏感数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;建议设立审计机制、明确边界、设定使用准则。&lt;/p&gt;
&lt;h2 id=&#34;结语llm-是平台工程师的又一个工具&#34;&gt;结语：LLM 是平台工程师的又一个工具&lt;/h2&gt;
&lt;p&gt;LLM 不是什么魔法，它是一个模式识别系统，用海量数据训练而成，具备强大的语义理解和生成能力。&lt;/p&gt;
&lt;p&gt;对平台工程师而言，它就像：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;另一种“自动化”&lt;/li&gt;
&lt;li&gt;一种“超能运维助手”&lt;/li&gt;
&lt;li&gt;一种“文档理解引擎”&lt;/li&gt;
&lt;li&gt;一种“智能 CI/CD 脚本生成器”&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你可以用它来增强现有平台的能力，提高团队效率，提升用户支持体验。
但你也需要理性对待它的局限，持续试验、迭代和评估其在你平台中的最佳用法。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;AI 正在来到平台工程的世界——拥抱它，不如先理解它。&lt;/p&gt;&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>
