<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI | 云原生社区（中国）</title>
    <link>https://cloudnative.to/tag/ai/</link>
      <atom:link href="https://cloudnative.to/tag/ai/index.xml" rel="self" type="application/rss+xml" />
    <description>AI</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Wed, 20 Dec 2023 11:30:00 +0800</lastBuildDate>
    <image>
      <url>https://cloudnative.to/media/sharing.png</url>
      <title>AI</title>
      <link>https://cloudnative.to/tag/ai/</link>
    </image>
    
    <item>
      <title>给初学生成式 AI（GenAI）的开发人员的 7 条最佳实践</title>
      <link>https://cloudnative.to/blog/7-best-practices-for-developers-getting-started-with-genai/</link>
      <pubDate>Wed, 20 Dec 2023 11:30:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/7-best-practices-for-developers-getting-started-with-genai/</guid>
      <description>&lt;h2 id=&#34;编者按&#34;&gt;编者按&lt;/h2&gt;
&lt;p&gt;本文译自：&lt;a href=&#34;https://thenewstack.io/7-best-practices-for-developers-getting-started-with-genai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thenewstack.io/7-best-practices-for-developers-getting-started-with-genai/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;编辑评论：这是一篇非常有价值的文章，向开发者展示了生成式 AI 的潜力和应用。生成式 AI 是一种利用大型语言模型来生成和转换文本的技术，它可以帮助开发者解决一些复杂的问题，如代码生成，文档编写，内容创作等。生成式 AI 也是一种云原生的技术，它需要大量的计算资源和数据，以及高效的部署和管理方式。文章提供了一些实用的工具和平台，如 GitHub Copilot，Bard，ChatGPT 等，让开发者可以轻松地尝试和使用生成式 AI。文章还给出了一些注意事项和建议，如保护数据隐私，验证输出质量，避免滥用等，让开发者可以负责任地使用生成式 AI。我认为这篇文章是一个很好的入门指南，让开发者可以了解和利用生成式 AI 来打造创新的云原生应用。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;正文&#34;&gt;正文&lt;/h2&gt;
&lt;p&gt;通过一点经验，你可以使用 GenAI 解决一些相当困难的问题，就像学习任何新技术一样，最好的方法就是动手实践。&lt;/p&gt;
&lt;p&gt;随着可访问的生成式人工智能进入主流，以及由此产生的通过简单语言转化整个人类知识的能力，每个企业都在竭力将人工智能整合到其技术体系中。对于开发人员来说，压力很大，但也有着令人兴奋的无限可能性。&lt;/p&gt;
&lt;p&gt;如果你有一些经验，你可以使用 GenAI 解决一些相当困难的问题，就像学习自 HTML 诞生以来的每一项新技术一样。让我们看看你可以采取的七个步骤，以开始建立 GenAI 的基础，并最终逐步发展成一个完全运作、可扩展的应用程序。&lt;/p&gt;
&lt;h2 id=&#34;1-玩转现有的-genai-工具&#34;&gt;1. 玩转现有的 GenAI 工具&lt;/h2&gt;
&lt;p&gt;入门 GenAI 的最佳方法是实践，而且门槛非常低。市场上现在有许多免费选项，比如 Bard、ChatGPT、Bing 和 Anthropic，有很多可以学习的选择。&lt;/p&gt;
&lt;p&gt;尝试使用 GenAI 工具和代码生成解决方案进行实验（并鼓励你的团队进行实验），例如&lt;a href=&#34;https://github.com/features/copilot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Copilot&lt;/a&gt;，它集成到每个流行的 IDE 中，充当一对程序员。Copilot 提供程序员建议，帮助解决代码问题，并生成整个函数，使学习和适应&lt;a href=&#34;https://us.resources.cio.com/resources/the-data-streaming-platform-key-to-ai-initiatives-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GenAI&lt;/a&gt;变得更快更容易。&lt;/p&gt;
&lt;p&gt;当你首次使用这些现成的工具时，要小心使用专有或敏感的公司数据，即使只是提供给工具一个提示也要小心。Gen AI 供应商可能会存储并使用你的数据用于将来的训练运行，这是公司数据政策和信息安全协议的重大违规行为。确保你及时直接地向你的团队传达这一黄金规则。&lt;/p&gt;
&lt;h2 id=&#34;2-了解从-genai-中可以获得什么&#34;&gt;2. 了解从 GenAI 中可以获得什么&lt;/h2&gt;
&lt;p&gt;一旦你开始尝试 GenAI，你将很快了解到不同提示会产生什么类型的输出。大多数 GenAI 工具可以生成各种格式的文本，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成&lt;/strong&gt;新的故事、想法、文章或任意长度的文本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;转换&lt;/strong&gt;现有文本为不同格式，如 JSON、Markdown 或 CSV。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;翻译&lt;/strong&gt;文本成不同语言。&lt;/li&gt;
&lt;li&gt;以聊天的方式&lt;strong&gt;对话&lt;/strong&gt;来回。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;审查&lt;/strong&gt;文本以展示特定元素。&lt;/li&gt;
&lt;li&gt;将长篇内容&lt;strong&gt;汇总&lt;/strong&gt;以获取洞察。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分析&lt;/strong&gt;文本的情感。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;任何人都可以生成这些类型的生成文本结果，无需编程技能。只需键入提示，文本就会产生。然而，大型语言模型（LLM）经过的培训越多，即它摄取的语言碎片越多，随着时间的推移，它在生成、更改和分析文本方面就会变得更加准确。&lt;/p&gt;
&lt;h2 id=&#34;3-学习提示工程&#34;&gt;3. 学习提示工程&lt;/h2&gt;
&lt;p&gt;部署 GenAI 的良好方法之一是掌握编写提示的技巧，这既是一门艺术又是一门科学。虽然提示工程师是一个实际的职位描述，但它也是任何希望提高他们使用 AI 的人的好绰号。优秀的提示工程师知道如何开发、完善和优化文本提示，以获得最佳结果并提高整个 AI 系统的性能。&lt;/p&gt;
&lt;p&gt;提示工程不需要特定的学位或背景，但从事这项工作的人需要擅长清晰解释事物。这是重要的，因为所有可用的 LLM 都是无状态的，这意味着没有长期记忆，每次交互只存在于小会话中。&lt;/p&gt;
&lt;p&gt;在提示工程中，以下三个因素变得重要：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;上下文&lt;/strong&gt;：你提出的问题、聊天历史记录和你设置的参数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;知识&lt;/strong&gt;：LLM 已经接受的培训内容以及你通过提示提供的新信息的结合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;形式&lt;/strong&gt;：你期望以何种方式生成信息的语气。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上下文、知识和形式的结合塑造了 GenAI 的大量知识存储成为你希望获得的响应类型。&lt;/p&gt;
&lt;h2 id=&#34;4-探索其他-genai-提示方法&#34;&gt;4. 探索其他 GenAI 提示方法&lt;/h2&gt;
&lt;p&gt;到目前为止，我们一直在谈论零-shot 提示，这基本上意味着提出一个带有一些上下文的问题。如果你从这种方法中没有得到期望的结果，还有四种提示 GenAI 的方法。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;单次提示&lt;/strong&gt;：提供你正在寻找的输出类型的示例。如果你想要特定类型的格式，例如[标题]和[4 个要点]，这将特别有用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;少量提示&lt;/strong&gt;：这类似于单次提示，但你会提供三到五个示例而不仅仅是一个。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“让我们一步一步地思考”&lt;/strong&gt;：这种技巧对 LLM 和对人都同样有效。如果你有一个包含多个部分的复杂问题，请在末尾输入此短语，等待 LLM 分解问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;思路链提示&lt;/strong&gt;：对于涉及复杂算术或其他推理任务的问题，思路链提示会指示工具“展示其工作方式”并解释其如何得出答案。以下是可能的示例：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu5da5e28c94c7cdc0c2775914665ea64e_96992_f342279dba4bba02769e3e4d9529d7a4.webp 400w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu5da5e28c94c7cdc0c2775914665ea64e_96992_511b5bc0b6bb9a3414bd9dbb1ed0046c.webp 760w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu5da5e28c94c7cdc0c2775914665ea64e_96992_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu5da5e28c94c7cdc0c2775914665ea64e_96992_f342279dba4bba02769e3e4d9529d7a4.webp&#34;
               width=&#34;760&#34;
               height=&#34;402&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;5-查看其他-genai-工作示例&#34;&gt;5. 查看其他 GenAI 工作示例&lt;/h2&gt;
&lt;p&gt;一旦你熟悉了 GenAI 工具并了解如何编写出色的提示，&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main/examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;请查看 OpenAI 发布的一些示例&lt;/a&gt;，了解其他人正在做什么以及可能的其他可能性。随着你的实验，你将更加熟悉聊天界面，并学会如何对其进行微调，以便熟练地缩小响应范围，甚至将响应转换为 CSV 文件或其他类型的表格。&lt;/p&gt;
&lt;p&gt;考虑如何将你的 GenAI 知识应用于你的业务，以简化困难或重复性任务，生成创意并使信息易于让更广泛的受众访问。你可以想象出哪些新的用例？以前不可能的东西现在成为可能了吗？&lt;/p&gt;
&lt;h2 id=&#34;6-集成第三方-genai-工具和-api&#34;&gt;6. 集成第三方 GenAI 工具和 API&lt;/h2&gt;
&lt;p&gt;考虑使用 ChatGPT、Bard 和 Claude 2 等 API 通过 API 使用 LLMs 的角色。这些工具都提供了强大的 API，并有支持文档，因此入门门槛很低。大多数这些 API 是基于使用量的，因此更容易玩弄。&lt;/p&gt;
&lt;p&gt;通常情况下，通过语义搜索和由向量数据库支持的嵌入来将自定义或私有数据集成到 LLM 提示中，你还可以集成自定义或私有数据。通常称为 RAG（检索增强生成）。&lt;/p&gt;
&lt;p&gt;分解这两个术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义搜索&lt;/strong&gt;：使用词嵌入比较查询的含义与其索引中文档的含义，即使没有完全匹配的单词也能获得更相关的结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;嵌入&lt;/strong&gt;：将对象（如单词、句子或整个文档）的数值表示转化为多维空间。这使得评估不同实体之间的关系成为可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下是这可能看起来的一个示例：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu7405699fb1f5ef241729ad19d0f40079_80167_7a9ac0597d4aee2cb8a39b43f765632d.webp 400w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu7405699fb1f5ef241729ad19d0f40079_80167_3a040cfa2182d854fef72600ce15298f.webp 760w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu7405699fb1f5ef241729ad19d0f40079_80167_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu7405699fb1f5ef241729ad19d0f40079_80167_7a9ac0597d4aee2cb8a39b43f765632d.webp&#34;
               width=&#34;760&#34;
               height=&#34;608&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这幅图展示了“猫”和“狗”的概念比它们与“人”或“蜘蛛”的概念更接近，“车辆汽车”则是最远的，是概念中最不相关的。（&lt;a href=&#34;https://www.confluent.io/blog/chatgpt-and-streaming-data-for-real-time-generative-ai/#connecting-knowledge-base-to-gpt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里有更多关于如何使用语义搜索和嵌入的信息&lt;/a&gt;。）&lt;/p&gt;
&lt;h2 id=&#34;7-从头开始训练自己的模型&#34;&gt;7. 从头开始训练自己的模型&lt;/h2&gt;
&lt;p&gt;这最后的建议实际上不太像建议，更像是一个“可选的下一步”。训练自己的 GenAI 模型并不适合每个人，但如果你：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有独特而有价值的知识库。&lt;/li&gt;
&lt;li&gt;想要执行商业 LLM 无法完成的某些任务。&lt;/li&gt;
&lt;li&gt;发现商业 LLM 的推理成本在商业上没有意义。&lt;/li&gt;
&lt;li&gt;有特定的安全要求，需要托管自己的 LLM 数据，并且不愿通过第三方 API 传递数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练自己的模型的一种方法是使用开源模型，例如 Llama 2、Mosaic MPT-7B、Falcon 或 Vicuna，其中许多还提供了商业使用许可证。这些通常根据它们具有的参数数量进行标记：7B、13B、40B 等。 “B”代表模型的参数数目，以及它可以处理和存储的信息量。数字越高，模型就越复杂和复杂，但训练和运行成本也越高。如果你的用例不复杂，并且如果你计划在性能相当强大的现代笔记本电脑上运行模型，那么具有较低参数的模型是开始的最佳且最经济的方法。&lt;/p&gt;
&lt;p&gt;中大型组织可能会选择从头开始构建和训练一个 LLM 模型。这是一条非常昂贵、资源密集且耗时的 AI 之路。你需要难以招聘的技术人才，并具备长时间迭代的机会，因此对大多数组织来说，这条路线不现实。&lt;/p&gt;
&lt;h3 id=&#34;微调-llm&#34;&gt;微调 LLM&lt;/h3&gt;
&lt;p&gt;一些组织选择中间路径：微调基本开源 LLM 以实现模型预训练能力之外的特定功能。如果你希望以你品牌独特的声音创建虚拟助手或基于真实客户购买构建的推荐系统，那么这是一个很好的选择。这些模型会随着你纳入排名靠前的用户交互而不断地训练自己。事实上，&lt;a href=&#34;https://voicebot.ai/2023/08/23/openai-brings-fine-tuning-to-gpt-3-5-turbo-and-gpt-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open AI 报告&lt;/a&gt;，使用此模型，可以将提示长度缩短多达 90%，同时保持性能不变。此外，Open AI 的商业 API 的最新增强功能使其与驱动 ChatGPT 和 Bing AI 的模型一样强大和易于访问。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes 故障排除智慧的演变</title>
      <link>https://cloudnative.to/blog/can-chatgpt-save-collective-kubernetes-troubleshooting/</link>
      <pubDate>Sun, 10 Sep 2023 19:03:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/can-chatgpt-save-collective-kubernetes-troubleshooting/</guid>
      <description>&lt;p&gt;摘要：本文讨论了在 Kubernetes 故障排除中的两种路径：一种是增强操作员的分析工作，通过自动化和简化对故障排除知识的访问来提供帮助；另一种是将操作员从故障排除中排除，通过使用 AI/ML 模型和可观察性数据来自动化故障修复。同时强调了数据的重要性，以及继续共享故障排除经验和建立对可观察性的一致认识的必要性。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本文译自：https://thenewstack.io/can-chatgpt-save-collective-kubernetes-troubleshooting/&lt;/p&gt;
&lt;p&gt;数十年前，系统管理员们开始在互联网上分享他们每天面临的技术问题。他们进行了长时间、充满活力且富有价值的讨论，探讨如何调查和解决问题的根本原因，然后详细说明最终对他们有效的解决方案。&lt;/p&gt;
&lt;p&gt;这股洪流从未停歇，只是改变了流向。如今，这些讨论仍在 Stack Overflow、Reddit 以及企业工程博客上进行。每一次讨论都是对全球 IT 系统故障排除经验的宝贵贡献。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://roadmap.sh/kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes&lt;/a&gt;也从根本上改变了这种流向。与几十年来困扰系统管理员和 IT 人员的虚拟机（VM）和单体应用程序相比，&lt;a href=&#34;https://thenewstack.io/microservices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;微服务架构&lt;/a&gt;要复杂得多。由于 Kubernetes 缺乏数据持久性，往往无法对规模化的 K8s 错误进行本地重现。即使能够捕获，观测数据也会在多个平台上分散，而资源和依赖关系的相互关联关系也难以捕捉。&lt;/p&gt;
&lt;p&gt;现在，凭直觉并不一定足够。您需要知道如何调试集群以获得下一步的线索。&lt;/p&gt;
&lt;p&gt;这种复杂性意味着公开的故障排除讨论比以往任何时候都更为重要，但现在我们开始看到这股宝贵的洪流不是被重定向，而是完全被堵住了。你在谷歌上看到了这一点。任何与 Kubernetes 相关问题的搜索都会出现一半以上的付费广告和至少一页 SEO 驱动的文章，这些文章缺乏技术深度。&lt;a href=&#34;https://thenewstack.io/stack-overflow-adds-ai-will-the-community-respond/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stack Overflow&lt;/a&gt; 正在失去其作为技术人员首选问答资源的主导地位，Reddit 在过去几年中也陷入了争议。&lt;/p&gt;
&lt;p&gt;现在，每个 Kubernetes 的 DevOps 平台都在建立最后一个堤坝：将您的故障排除知识集中在其平台上，并用&lt;a href=&#34;https://thenewstack.io/70-percent-of-developers-using-or-will-use-ai-says-stack-overflow-survey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;人工智能（AI）和机器学习（ML）&lt;/a&gt;取而代之，直到整个堆栈对于甚至是最有经验的云原生工程师来说都成为一个黑盒。当发生这种情况时，您失去了逐个探测、排除故障和修复系统的能力。这种趋势将曾经是众包故障排除技能洪流变成了过去所能提供的仅仅是一滴水。&lt;/p&gt;
&lt;p&gt;当我们依赖于平台时，故障排除技术的集体智慧就会消失。&lt;/p&gt;
&lt;h2 id=&#34;故障排除智慧的传承&#34;&gt;故障排除智慧的传承&lt;/h2&gt;
&lt;p&gt;起初，系统管理员依靠实体书籍进行技术文档和整体最佳实践的实施。随着互联网在 80 年代和 90 年代的普及，这些人通常通过&lt;a href=&#34;https://today.duke.edu/2010/05/usenet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Usenet&lt;/a&gt;与同行进行交流，并在像 comp.lang.* 这样的新闻组中提出工作中的技术问题，这类新闻组类似于我们今天所知的论坛的简化版本。&lt;/p&gt;
&lt;p&gt;随着互联网的普及迅速，并几乎完全改变了故障排除智慧的洪流。工程师和管理员们不再聚集在新闻组中，而是涌向包括 Experts Exchange 在内的数千个论坛，该论坛于 1996 年上线。在积累了大量的问题和答案之后，Experts Exchange 团队将所有答案都放在了每年 250 美元的付费墙后面，这使得无数宝贵的讨论无法公开获取，最终导致了该网站的影响力下降。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.joelonsoftware.com/2018/04/06/the-stack-overflow-age/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stack Overflow 随后出现&lt;/a&gt;，再次向公众开放了这些讨论，并通过声望点数对讨论进行游戏化，这些声望点数可以通过提供见解和解决方案来获得。其他用户随后对“最佳”解决方案进行投票和验证，这有助于其他搜索者快速找到答案。Stack Overflow 的游戏化、自我管理和社区使其成为了洪流式故障排除知识的唯一渠道。&lt;/p&gt;
&lt;p&gt;但是，就像其他时代一样，没有什么好事能永远持续下去。近 10 年来，人们一直在预测&lt;a href=&#34;https://johnslegers.medium.com/the-decline-of-stack-overflow-7cb69faa575d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Stack Overflow 的衰落”&lt;/a&gt;，并指出由于其具有攻击性的性质和由拥有最多声望点数的人进行管理的结构，它“讨厌新用户”。虽然 Stack Overflow 的影响力和流行度确实下降了，但 Reddit 的开发/工程专注的 subreddit 填补了这个空白，它仍然是公开可访问的故障排除知识的最大存储库。&lt;/p&gt;
&lt;p&gt;特别是对于 Kubernetes 和云原生社区来说，这仍然是一个重要的资源，因为它们仍然在经历重大的增长阵痛。而这是一种宝贵的资源，因为如果您认为现在的 Kubernetes 已经很复杂了&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-的复杂性问题&#34;&gt;Kubernetes 的复杂性问题&lt;/h2&gt;
&lt;p&gt;在一篇关于“直观调试”失败的精彩文章中，软件交付顾问 Pete Hodgson 认为，构建和交付软件的现代架构（如 Kubernetes 和微服务）比以往任何时候都更加复杂。他写道：“对于我们大多数人来说，为服务器命名为希腊神话角色，并通过 ssh 进入服务器运行&lt;code&gt;tail&lt;/code&gt;和&lt;code&gt;top&lt;/code&gt;的日子已经一去不复返了。”但是，“这种转变是有代价的……传统的理解和故障排除生产环境的方法在这个新世界中已经行不通了。”&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-cynefin-模型&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cynefin 模型&#34; srcset=&#34;
               /blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hua353e1f3a668859fa4d7a161556969e4_94124_bc813e5da2f806022c76773eb056f8e6.webp 400w,
               /blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hua353e1f3a668859fa4d7a161556969e4_94124_6570ccc58d88a6ef37457e3c8bdedde7.webp 760w,
               /blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hua353e1f3a668859fa4d7a161556969e4_94124_1200x1200_fit_q75_h2_lanczos.webp 1200w&#34;
               src=&#34;https://cloudnative.to/blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hua353e1f3a668859fa4d7a161556969e4_94124_bc813e5da2f806022c76773eb056f8e6.webp&#34;
               width=&#34;760&#34;
               height=&#34;676&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cynefin 模型
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Cynefin 模型。来源：维基百科&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hodgson 使用&lt;a href=&#34;https://en.wikipedia.org/wiki/Cynefin_framework&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cynefin 模型&lt;/a&gt;来说明软件架构过去是复杂的，因为有足够的经验，人们可以理解故障排除和解决方案之间的因果关系。&lt;/p&gt;
&lt;p&gt;他认为，分布式微服务架构是复杂的，即使经验丰富的人对根本原因以及如何进行故障排除也只有“有限的直觉”。他们必须花更多时间通过可观察性数据提出问题和回答问题，最终假设可能出错的原因。&lt;/p&gt;
&lt;p&gt;如果我们同意 Hodgson 的前提 - Kubernetes 本质上是复杂的，并且在响应之前需要花费更多的时间分析问题，那么与 Kubernetes 一起工作的工程师学会了哪些问题最重要，然后用可观察性数据回答，以进行最佳的下一步行动，似乎是至关重要的。&lt;/p&gt;
&lt;p&gt;这正是新一代以 AI 驱动的故障排除平台所提供的智慧。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-故障排除的两种路径&#34;&gt;Kubernetes 故障排除的两种路径&lt;/h2&gt;
&lt;p&gt;多年来，像 OpenAI 这样的公司一直在根据 Stack Overflow、Reddit 等公开数据进行抓取和训练模型，这意味着这些 AI 模型可以访问大量的系统和应用知识，包括 Kubernetes。还有一些人意识到组织的可观察性数据是训练 AI/ML 模型分析新场景的宝贵资源。&lt;/p&gt;
&lt;p&gt;他们都在问同一个问题：我们如何利用关于 Kubernetes 的现有数据来简化搜索最佳解决方案的过程？他们正在构建的产品采取非常不同的路径。&lt;/p&gt;
&lt;h3 id=&#34;第一种增强操作员的分析工作&#34;&gt;第一种：增强操作员的分析工作&lt;/h3&gt;
&lt;p&gt;这些工具自动化和简化对公开在线发布的大量故障排除知识的访问。它们不会取代进行适当故障排除或&lt;a href=&#34;https://aws.amazon.com/opensearch-service/resources/root-cause-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;根本原因分析&lt;/a&gt;（RCA）所需的人类直觉和创造力，而是有条不紊地自动化操作员查找相关信息的方式。&lt;/p&gt;
&lt;p&gt;例如，如果一个刚接触 Kubernetes 的开发人员在运行&lt;code&gt;kubectl get pods&lt;/code&gt;时发现&lt;code&gt;CrashLoopBackOff&lt;/code&gt;状态导致他们无法部署应用程序，他们可以查询一个 AI 驱动的工具以获得建议，比如运行&lt;code&gt;kubectl describe $POD&lt;/code&gt;或&lt;code&gt;kubectl logs $POD&lt;/code&gt;。这些步骤可能会进一步引导开发人员使用&lt;code&gt;kubectl describe $DEPLOYMENT&lt;/code&gt;来调查相关的部署情况。&lt;/p&gt;
&lt;p&gt;在&lt;a href=&#34;https://botkube.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Botkube&lt;/a&gt;，我们对使用 AI 在大量故障排除智慧的基础上自动化这个来回查询的概念非常感兴趣。用户应该能够直接在 Slack 中提问，如“我如何排除这个无法正常工作的服务？”并收到 ChatGPT 撰写的回答。在一次公司范围的黑客马拉松活动中，我们着手实施这一概念，为我们的协作故障排除平台构建了一个新的插件。&lt;/p&gt;
&lt;p&gt;通过&lt;a href=&#34;https://botkube.io/blog/use-chatgpt-to-troubleshoot-kubernetes-errors-with-botkubes-doctor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doctor&lt;/a&gt;，您可以利用大量的故障排除知识，通过 Botkube 作为您的 Kubernetes 集群和消息/协作平台之间的桥梁，无需在 Stack Overflow 或 Google 搜索广告中漫游，这对于新手 Kubernetes 开发人员和操作员特别有用。&lt;/p&gt;
&lt;p&gt;该插件还通过生成一个带有&lt;strong&gt;获取帮助&lt;/strong&gt;按钮的 Slack 消息进一步自动化，用于任何错误或异常，然后查询 ChatGPT 以获取可行的解决方案和下一步操作。您甚至可以将 Doctor 插件的结果导入其他操作或集成，以简化您主动使用现有广泛的 Kubernetes 故障排除知识来更直观地调试和感知问题的方式。&lt;/p&gt;
&lt;h3 id=&#34;第二种将操作员从故障排除中排除&#34;&gt;第二种：将操作员从故障排除中排除&lt;/h3&gt;
&lt;p&gt;这些工具不关心公开知识的泛滥。如果它们可以基于实际的可观察性数据训练通用的 AI/ML 模型，然后根据您的特定架构进行微调，它们可以试图完全剔除人为操作员在根本原因分析和故障修复中的作用。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.causely.io/platform/causely-for-kubernetes-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causely&lt;/a&gt;就是这样一家初创公司，他们并不回避使用 AI 来“消除人为故障排除”的愿景。该平台连接到您现有的可观察性数据，并处理它们以微调因果关系模型，理论上可直接进行修复步骤 - 无需探测或使用&lt;code&gt;kubectl&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;如果说有时候有一个 Kubernetes 神灵听起来很诱人，那我可能会撒谎，但我对像 Causely 这样的工具夺走运维工作并不担心。我担心的是在 Causely 引领的未来中，我们宝贵的故障排除知识会发生什么。&lt;/p&gt;
&lt;h3 id=&#34;这两种路径之间的差距数据&#34;&gt;这两种路径之间的差距：数据&lt;/h3&gt;
&lt;p&gt;我不是在为“人工智能将取代所有 DevOps 工作”发表言论。我们已经读过太多这样的末日场景，适用于每个小众和行业。我更关心这两种路径之间的差距：用于训练和回答问题或呈现结果的数据是什么？&lt;/p&gt;
&lt;p&gt;第一种路径通常使用现有的公开数据。尽管有关 AI 公司爬取这些站点进行训练数据的担忧-Reddit 和 Twitter，但这些数据的开放性仍然提供了一个激励循环，以保持开发人员和工程师继续在 Reddit、Stack Overflow 和其他平台上共享知识的持续泛滥。&lt;/p&gt;
&lt;p&gt;云原生社区通常也倾向于共享技术知识，认同共享技术知识和一个“涨潮（Kubernetes 故障排除技巧的涨潮）抬高所有船（压力巨大的 Kubernetes 工程师）”的想法。&lt;/p&gt;
&lt;p&gt;第二条路径看起来更为暗淡。随着以 AI 驱动的 DevOps 平台的兴起，越来越多的故障排除知识被锁定在这些仪表板和驱动平台的专有 AI 模型中。我们都同意，Kubernetes 基础架构将继续变得更加复杂，而不是更简单，这意味着随着时间的推移，我们对节点、Pod 和容器之间发生的情况的理解将变得更少。&lt;/p&gt;
&lt;p&gt;当我们停止互相分析问题和感知解决方案时，我们变得依赖于平台。这对每个人来说都是一条失败的道路，除了平台之外。&lt;/p&gt;
&lt;h3 id=&#34;我们如何不失去或失去得更少&#34;&gt;我们如何不失去（或失去得更少）？&lt;/h3&gt;
&lt;p&gt;我们能做的最好的事情是继续在线上发布关于我们在 Kubernetes 和其他领域的故障排除经验的惊人内容，比如“&lt;a href=&#34;https://learnk8s.io/troubleshooting-deployments&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;关于故障排除 Kubernetes 部署的视觉指南&lt;/a&gt;”；通过游戏化创造教育性应用程序，比如&lt;a href=&#34;https://sadservers.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SadServers&lt;/a&gt;；在故障排除系统时采取我们最喜欢的第一步，比如“&lt;a href=&#34;https://rachelbythebay.com/w/2018/03/26/w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;为什么在排除未知机器问题时我通常首先运行‘w’&lt;/a&gt;”；并进行详细的事后分析，详细描述了探测、感知和应对潜在灾难性情况的压力故事，比如&lt;a href=&#34;https://mail.tarsnap.com/tarsnap-announce/msg00050.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023 年 7 月的 Tarsnap 故障&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我们还可以超越技术解决方案，比如讨论我们如何在紧张的故障排除场景中管理和支持同事，或者在组织范围内建立对可观察性的一致认识。&lt;/p&gt;
&lt;p&gt;尽管它们目前面临困境，但 Stack Overflow 和 Reddit 将继续是讨论故障排除和寻求答案的可靠渠道。如果它们最终与 Usenet 和 Experts Exchange 齐名，它们可能会被其他可公开获得的替代品所取代。&lt;/p&gt;
&lt;p&gt;无论何时何地以何种方式发生，我希望您能加入我们在 Botkube 和全新的 Doctor 插件中，为在 Kubernetes 中协作解决复杂问题构建新的渠道。&lt;/p&gt;
&lt;p&gt;无论 AI 驱动的 DevOps 平台是否继续基于抓取的公共 Kubernetes 数据训练新模型，只要我们不自愿地将好奇心、冒险精神和解决问题的能力全部放入这些黑匣子中，就会始终有一条新路径，让宝贵的故障排除知识源源不断地流动。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>将 AI 应用于 WebAssembly 还为时过早吗？</title>
      <link>https://cloudnative.to/blog/is-it-too-early-to-leverage-ai-for-webassembly/</link>
      <pubDate>Thu, 07 Sep 2023 21:03:00 +0800</pubDate>
      <guid>https://cloudnative.to/blog/is-it-too-early-to-leverage-ai-for-webassembly/</guid>
      <description>&lt;p&gt;本文译自：&lt;a href=&#34;https://thenewstack.io/is-it-too-early-to-leverage-ai-for-webassembly/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thenewstack.io/is-it-too-early-to-leverage-ai-for-webassembly/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;摘要：Fermyon Technologies 认为，将 AI 应用于 WebAssembly 并不为时过早。WebAssembly 为在服务器上运行推理提供了坚实的基础，而且在许多不同的环境中，如浏览器和物联网设备等，通过将这些工作负载移动到终端用户设备上，可以消除延迟并避免将数据发送到集中式服务器，同时能够在边缘发现的多种异构设备上运行。Fermyon Serverless AI 通过提供超过 100 倍于其他按需 AI 基础设施服务的亚秒冷启动时间来解决了企业级 AI 应用程序成本高的问题。这是一种共生关系。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;人工智能及其在 IT、软件开发和运营方面的应用刚开始发挥作用，预示着人类角色将如何在近期和长期内演变，特别是在较小的规模上，WebAssembly 代表着一种正在引起重大关注的技术，同时证明了其可行性，但成功的商业模型尚未实现，主要是由于最终端点的缺乏标准化。与此同时，至少有一家供应商 Fermyon 认为，在这个阶段应用 AI 于 WebAssembly 并不为时过早。&lt;/p&gt;
&lt;p&gt;那么，AI 如何潜在地帮助 Wasm 的开发和采用，这是否为时过早？正如 VMware CTO 办公室的高级工程师 Angel M De Miguel Meana 所指出的那样，自从 ChatGPT 推出以来，AI 生态系统已经发生了巨大的变化，WebAssembly 为在服务器上运行推理提供了坚实的基础，而且在许多不同的环境中，如浏览器和物联网设备等，通过将这些工作负载移动到终端用户设备上，可以消除延迟并避免将数据发送到集中式服务器，同时能够在边缘发现的多种异构设备上运行。由于 Wasm 生态系统仍在兴起，因此在早期阶段集成 AI 将有助于推动新的和现有的与 AI 相关的标准。这是一种共生关系。&lt;/p&gt;
&lt;h2 id=&#34;完美的匹配&#34;&gt;完美的匹配&lt;/h2&gt;
&lt;p&gt;Fermyon Technologies 的联合创始人兼首席执行官 Matt Butcher 告诉 The New Stack：“我们成立 Fermyon 的目标是打造下一代无服务器平台。AI 显然是这一下一代的一部分。在我们的行业中，我们经常看到革命性的技术一起成长：Java 和 Web、云和微服务、Docker 和 Kubernetes。WebAssembly 和 AI 是一对完美的组合。我看到它们一起成长（并变老）。”&lt;/p&gt;
&lt;p&gt;“烘焙”AI 模型，如 LLM（大型语言模型）或转换器，到 WebAssembly 运行时中，是加速采用 WebAssembly 的逻辑下一步，Enterprise Management Associates (EMA) 的分析师 Torsten Volk 告诉 The New Stack。与调用诸如通过 API 的数据库服务类似，编译 WebAssembly 应用程序（二进制文件）可以将其 API 请求发送到 WebAssembly 运行时，该运行时将该调用中继到 AI 模型并将模型响应返回给发起者，Volk 说。&lt;/p&gt;
&lt;p&gt;“一旦我们有一个提供开发人员一个标准 API 的通用组件模型（CCM），访问数据库、AI 模型、GPU、消息传递、身份验证等，这些 API 请求将变得非常强大。CCM 将让开发人员编写相同的代码，在数据中心、云甚至边缘位置的任何类型的服务器上与 AI 模型（例如 GPT 或 Llama）进行通信，只要该服务器拥有足够的硬件资源可用，”Volk 说。“这一切都归结为关键问题，即产业参与者何时会就 CCM 达成一致。同时，WebAssembly 云（如 Fermyon）可以利用 WebAssembly 使 AI 模型在其自己的云基础设施中具有可移植性和可扩展性，无需 CCM，并将一些节省成本传递给客户。”&lt;/p&gt;
&lt;h2 id=&#34;解决问题&#34;&gt;解决问题&lt;/h2&gt;
&lt;p&gt;同时，Fermyon 认为，在这个阶段应用 AI 于 WebAssembly 并不为时过早。正如 Butcher 所指出的那样，负责在 LLM（如 LLaMA2）上构建和运行企业 AI 应用程序的开发人员面临着 100 倍计算成本的挑战，即每小时 32 美元及以上的 GPU 访问费用。或者，他们可以使用按需服务，但是启动时间却非常慢。这使得以实惠的方式提供企业级 AI 应用程序变得不切实际。&lt;/p&gt;
&lt;p&gt;Fermyon Serverless AI 通过提供超过 100 倍于其他按需 AI 基础设施服务的亚秒冷启动时间来解决了这个问题，Butcher 说。这一“突破”得益于驱动 Fermyon Cloud 的服务器 WebAssembly 技术，该技术被架构为亚毫秒冷启动和高容量时间分片的计算实例，已被证明可以将计算密度提高 30 倍。“将此运行时配置文件扩展到 GPU 将使 Fermyon Cloud 成为最快的 AI 推理基础设施服务，”Butcher 说。&lt;/p&gt;
&lt;p&gt;Volk 说，这样的推理服务“非常有趣”，因为典型的 WebAssembly 应用程序仅包含几兆字节，而 AI 模型的大小要大得多。这意味着它们不会像传统的 WebAssembly 应用程序那样启动得那么快。“我认为 Fermyon 已经想出了如何使用时间分片为 WebAssembly 应用程序提供 GPU 访问的方法，以便所有这些应用程序都可以通过其 WebAssembly 运行时保留一些时间片来获取所需的 GPU 资源”，Volk 说。“这意味着很多应用程序可以共享一小部分昂贵的 GPU，以按需为其用户提供服务。这有点像分时共享，但不需要强制参加午餐时间的演示。”&lt;/p&gt;
&lt;p&gt;使用 Spin 入门。&lt;/p&gt;
&lt;p&gt;!https://prod-files-secure.s3.us-west-2.amazonaws.com/86575c70-5cc9-4b3e-bee7-d1bb14ba20e3/6bf78916-e34c-4051-86a7-52145cdc372a/4a27b287-capture-decran-2023-09-05-192118.png&lt;/p&gt;
&lt;p&gt;那么，用户如何与 Serverless AI 交互？Fermyon 的 Serverless AI 没有 REST API 或外部服务，它仅构建在 Fermyon 的 Spin 本地和 Fermyon Cloud 中，Butcher 解释说。“在您的代码的任何位置，您都可以将提示传递到 Serverless AI 并获得响应。在这个第一个测试版中，我们包括 LLaMa2 的聊天模型和最近宣布的 Code Llama 代码生成模型，”Butcher 说。“因此，无论您是在总结文本、实现自己的聊天机器人还是编写后端代码生成器，Serverless AI 都可以满足您的需求。我们的目标是使 AI 变得简单，使开发人员可以立即开始利用它来构建新的令人瞩目的无服务器应用程序。”&lt;/p&gt;
&lt;h2 id=&#34;重要意义&#34;&gt;重要意义&lt;/h2&gt;
&lt;p&gt;使用 WebAssembly 来运行工作负载，可以使用 Fermyon Serverless AI 将“GPU 的一小部分”分配给用户应用程序，以“及时”执行 AI 操作，Fermyon CTO 和联合创始人 Radu Matei 在一篇博客文章中写道。 “当操作完成时，我们将该 GPU 的一小部分分配给队列中的另一个应用程序，”Matei 写道。“由于 Fermyon Cloud 中的启动时间为毫秒级，因此我们可以在分配给 GPU 的用户应用程序之间快速切换。如果所有 GPU 分数都在忙于计算数据，我们将在下一个可用的应用程序之前将传入的应用程序排队。”&lt;/p&gt;
&lt;p&gt;这有两个重大的影响，Matei 写道。首先，用户不必等待虚拟机或容器启动并附加到 GPU 上。此外，“我们可以实现更高的资源利用率和效率，”Matei 写道。&lt;/p&gt;
&lt;p&gt;Fermyon 传达的 Serverless AI 的具体特点包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这是一款开发人员工具和托管服务，专为使用开源 LLM 进行 AI 推理的无服务器应用程序而设计。&lt;/li&gt;
&lt;li&gt;由于我们的核心 WebAssembly 技术，我们的冷启动时间比竞争对手快 100 倍，从几分钟缩短到不到一秒。这使我们能够在相同的时间内（并且使用相同的硬件）执行数百个应用程序（二进制文件），而今天的服务用于运行一个。&lt;/li&gt;
&lt;li&gt;我们为使用 Spin 构建和运行 AI 应用程序提供了本地开发体验，然后将其部署到 Fermyon Cloud 中，以高性能的方式以其他解决方案的一小部分成本提供服务。&lt;/li&gt;
&lt;li&gt;Fermyon Cloud 使用 AI 级别的 GPU 处理每个请求。由于我们的快速启动和高效的时间共享，我们可以在数百个应用程序之间共享单个 GPU。&lt;/li&gt;
&lt;li&gt;我们正在推出免费的私人测试版。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;大希望&#34;&gt;大希望&lt;/h2&gt;
&lt;p&gt;然而，在 Wasm 和 AI 同时达到潜力之前，还有很长的路要走。在 WasmCon 2023 上，Second State 的 CEO 兼联合创始人 Michael Yuan 和 Wasm 的运行时项目以及 WasmEdge 的讨论了一些正在进行的工作。他在与 De Miguel Meana 的谈话中涵盖了这个话题，“开始使用 AI 和 WebAssembly”在 WasmCon 2023 上。&lt;/p&gt;
&lt;p&gt;“在这个领域（AI 和 Wasm）需要做很多生态系统工作。例如，仅拥有推理是不够的，”Yuan 说。“现在的百万美元问题是，当您拥有图像和文本时，如何将其转换为一系列数字，然后在推理之后如何将这些数字转换回可用的格式？”&lt;/p&gt;
&lt;p&gt;预处理和后处理是 Python 今天最大的优势之一，这得益于为这些任务提供的众多库，Yuan 说。将这些预处理和后处理函数合并到 Rust 函数中将是有益的，但需要社区更多的努力来支持其他模块。“这个生态系统有很大的增长潜力，”Yuan 说。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
