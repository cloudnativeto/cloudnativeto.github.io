<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI | 云原生社区（中国）</title>
    <link>https://cloudnativecn.com/tag/ai/</link>
      <atom:link href="https://cloudnativecn.com/tag/ai/index.xml" rel="self" type="application/rss+xml" />
    <description>AI</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>zh</language><lastBuildDate>Thu, 18 Jul 2024 19:23:50 +0800</lastBuildDate>
    <image>
      <url>https://cloudnativecn.com/media/sharing.png</url>
      <title>AI</title>
      <link>https://cloudnativecn.com/tag/ai/</link>
    </image>
    
    <item>
      <title>SAPwned：SAP AI 漏洞暴露客户云环境和私有 AI 工件</title>
      <link>https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/</link>
      <pubDate>Thu, 18 Jul 2024 19:23:50 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/</guid>
      <description>&lt;h2 id=&#34;ai-是否存在隔离问题&#34;&gt;AI 是否存在隔离问题？&lt;/h2&gt;
&lt;p&gt;在过去的几个月里，我们 Wiz 研究团队对多个 AI 服务提供商进行了广泛的租户隔离研究。我们认为这些服务更容易受到租户隔离漏洞的影响，因为它们允许用户运行 AI 模型和应用程序，这等同于执行任意代码。随着 AI 基础设施越来越成为许多商业环境的标配，这些攻击的影响正变得越来越重要。&lt;/p&gt;
&lt;p&gt;我们将在即将举行的 Black Hat 会议上展示这个研究项目的发现，在我们的会议“隔离还是幻觉？为乐趣和权重黑客攻击 AI 基础设施提供商”。&lt;/p&gt;
&lt;p&gt;在这个项目的最新一期中，我们研究了 SAP 的 AI 产品，恰当地命名为“SAP AI Core”。这是我们系列中的第三份报告，继我们对 Hugging Face 和 Replicate 平台的研究之后。本博客将探索漏洞链并详细介绍我们的发现，称为“SAPwned”，同时也将观察到确保管理 AI 平台安全的潜在影响和更广泛的启示。&lt;/p&gt;
&lt;h2 id=&#34;执行摘要&#34;&gt;执行摘要&lt;/h2&gt;
&lt;p&gt;AI 训练过程需要访问大量敏感客户数据，这使 AI 训练服务成为攻击者的诱人目标。SAP AI Core 提供与 HANA 及其他云服务的集成，通过云访问密钥访问客户的内部数据。这些凭据非常敏感，我们的研究目标是确定潜在的恶意行为者是否能够访问这些客户秘密。&lt;/p&gt;
&lt;p&gt;我们对 SAP AI Core 的研究始于使用 SAP 的基础设施执行合法的 AI 训练程序。通过执行任意代码，我们能够横向移动并接管服务——获取客户的私有文件以及客户云环境的凭据：AWS、Azure、SAP HANA Cloud 等。我们发现的漏洞可能允许攻击者访问客户数据并污染内部工件——蔓延到相关服务和其他客户环境。&lt;/p&gt;
&lt;p&gt;具体来说，我们获得的访问权限允许我们：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在 SAP 的内部容器注册表上读取和修改 Docker 镜像&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 Google 容器注册表上读取和修改 SAP 的 Docker 镜像&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;在 SAP 的内部 Artifactory 服务器上读取和修改工件&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;获得 SAP AI Core 的 Kubernetes 集群的集群管理员权限&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;访问客户的云凭证和私有 AI 工件&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-我们研究发现的逐步插图&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;我们研究发现的逐步插图&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f1_hu18243618901166088127.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f1_hu714737452478308898.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f1_hu7898719421972672962.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f1_hu18243618901166088127.webp&#34;
               width=&#34;760&#34;
               height=&#34;428&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      我们研究发现的逐步插图
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;我们发现这些问题的根本原因是攻击者可以运行恶意 AI 模型和训练程序，这本质上是代码。在审查了几个主要 AI 服务之后，我们认为行业必须改进其在运行 AI 模型时的隔离和沙箱标准。&lt;/p&gt;
&lt;p&gt;所有漏洞已报告给 SAP 的安全团队，并由 SAP 修复，如其网站所确认。我们感谢他们的合作。没有客户数据受到泄露。&lt;/p&gt;
&lt;h2 id=&#34;介绍研究开始&#34;&gt;介绍：研究开始&lt;/h2&gt;
&lt;p&gt;SAP AI Core 是一项服务，允许用户以可扩展和管理的方式在 SAP 的庞大云资源上开发、训练和运行 AI 服务。类似于其他云提供商（和 AI 基础设施提供商），客户的代码在 SAP 的共享环境中运行——构成跨租户访问的风险。&lt;/p&gt;
&lt;p&gt;我们的研究始于作为 SAP 客户，基本权限允许我们创建 AI 项目。因此，我们首先在 SAP AI Core 上创建了一个常规 AI 应用程序。SAP 的平台允许我们提供一个 Argo Workflow 文件，该文件反过来生成了一个根据我们的配置的新 Kubernetes Pod。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-sap-ai-core-上的-argo-工作流配置示例&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;SAP AI Core 上的 Argo 工作流配置示例&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f2_hu12895675789476928815.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f2_hu6596618348738739739.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f2_hu6071594457489936310.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f2_hu12895675789476928815.webp&#34;
               width=&#34;760&#34;
               height=&#34;516&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      SAP AI Core 上的 Argo 工作流配置示例
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这允许我们在 Pod 中按设计运行我们自己的任意代码——不需要任何漏洞。然而，我们的环境受到了相当大的限制。我们很快意识到，我们的 Pod 的网络访问非常有限，这是由 Istio 代理 sidecar 强制执行的——因此，扫描内部网络对我们来说不是一个选项。至少现在不是。&lt;/p&gt;
&lt;h2 id=&#34;bug-1-通过-1337-的力量绕过网络限制&#34;&gt;Bug #1: 通过 1337 的力量绕过网络限制&lt;/h2&gt;
&lt;p&gt;我们首先尝试的是为我们的 Pod 配置“有趣”的权限。然而，SAP 的准入控制器阻止了我们尝试的所有危险安全选项——例如，以&lt;code&gt;root&lt;/code&gt;身份运行我们的容器。&lt;/p&gt;
&lt;p&gt;尽管如此，我们发现准入控制器未能阻止两种有趣的配置。&lt;/p&gt;
&lt;p&gt;第一个是&lt;code&gt;shareProcessNamespace&lt;/code&gt;，它允许我们与我们的 sidecar 容器共享进程命名空间。由于我们的 sidecar 是 Istio 代理，我们获得了对 Istio 的配置的访问权限，包括对集群的集中式 Istiod 服务器的访问令牌。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-通过我们的-sidecar-容器访问-istio-令牌&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;通过我们的 sidecar 容器访问 Istio 令牌&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f3_hu14609963526827508083.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f3_hu18061715644222923455.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f3_hu9569186271581430545.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f3_hu14609963526827508083.webp&#34;
               width=&#34;760&#34;
               height=&#34;279&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      通过我们的 sidecar 容器访问 Istio 令牌
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;另一个是&lt;code&gt;runAsUser&lt;/code&gt;（和&lt;code&gt;runAsGroup&lt;/code&gt;）。虽然我们不能成为 root，但所有其他 UID 都是允许的——包括 Istio 的 UID，讽刺的是，这个 UID 是&lt;code&gt;1337&lt;/code&gt;（是的，真的）。我们将我们的 UID 设置为 1337，并成功地以 Istio 用户的身份运行。由于 Istio 本身是&lt;a href=&#34;https://istio.io/latest/docs/reference/config/analysis/ist0144/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;从 Istio 的 iptables 规则中排除的&lt;/a&gt;——我们现在运行时没有任何流量限制！&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-发送请求到内部网络在-uid-1337-之前和之后&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;发送请求到内部网络——在 UID 1337 之前和之后&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f4_hu7730611824788218361.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f4_hu265332745971002851.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f4_hu4363213494532570107.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f4_hu7730611824788218361.webp&#34;
               width=&#34;760&#34;
               height=&#34;241&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      发送请求到内部网络——在 UID 1337 之前和之后
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;我们摆脱了流量束缚，开始扫描我们 Pod 的内部网络。使用我们的 Istio 令牌，我们能够从 Istiod 服务器读取配置并了解内部环境——这引导我们进行了以下发现。&lt;/p&gt;
&lt;h2 id=&#34;bug-2-loki-泄露-aws-令牌&#34;&gt;Bug #2: Loki 泄露 AWS 令牌&lt;/h2&gt;
&lt;p&gt;我们在集群中找到了一个 Grafana Loki 的实例，因此我们请求了&lt;code&gt;/config&lt;/code&gt;端点以查看 Loki 的配置。API 响应了完整的配置，包括 Loki 用来访问 S3 的 AWS 密钥：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-来自-sap-的-loki-服务器的配置摘录&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;来自 SAP 的 Loki 服务器的配置摘录&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f5_hu10908587585710315639.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f5_hu5210851232726765505.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f5_hu15252043344179883139.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f5_hu10908587585710315639.webp&#34;
               width=&#34;760&#34;
               height=&#34;312&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      来自 SAP 的 Loki 服务器的配置摘录
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这些密钥授予访问 Loki 的 S3 存储桶的权限，其中包含 AI Core 服务（SAP 称其不敏感）和客户 Pods 的大量日志。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-loki-的-s3-存储桶中的部分文件列表&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Loki 的 S3 存储桶中的部分文件列表&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f6_hu18007388991397704631.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f6_hu16248088741001940480.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f6_hu10462996416810849322.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f6_hu18007388991397704631.webp&#34;
               width=&#34;760&#34;
               height=&#34;379&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Loki 的 S3 存储桶中的部分文件列表
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;bug-3-未经身份验证的-efs-共享暴露用户文件&#34;&gt;Bug #3: 未经身份验证的 EFS 共享暴露用户文件&lt;/h2&gt;
&lt;p&gt;在内部网络中，我们发现了 6 个 AWS Elastic File System（EFS）实例，监听端口 2049。EFS 实例的一个&lt;a href=&#34;https://youtu.be/HcNmkCRXFdE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;常见问题&lt;/a&gt;是它们默认配置为公共的——这意味着只要您可以访问其 NFS 端口，就不需要凭据即可查看或编辑文件。这些实例也不例外，我们使用简单的开源 NFS 工具，可以自由访问共享的内容。&lt;/p&gt;
&lt;p&gt;列出这些 EFS 实例上存储的文件，揭示了大量 AI 数据，包括代码和训练数据集，按客户 ID 分类：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f7_hu13828022362543296357.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f7_hu8636987027368623978.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f7_hu15015156208339228283.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f7_hu13828022362543296357.webp&#34;
               width=&#34;760&#34;
               height=&#34;385&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-两个-efs-共享的部分文件列表每个文件夹代表一个不同的客户-id&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;两个 EFS 共享的部分文件列表；每个文件夹代表一个不同的客户 ID&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f8_hu1009129100692678847.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f8_hu2301761535920762428.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f8_hu17196893872299708137.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f8_hu1009129100692678847.webp&#34;
               width=&#34;760&#34;
               height=&#34;466&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      两个 EFS 共享的部分文件列表；每个文件夹代表一个不同的客户 ID
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;bug-4-未经身份验证的-helm-服务器危及内部-docker-注册表和-artifactory&#34;&gt;Bug #4: 未经身份验证的 Helm 服务器危及内部 Docker 注册表和 Artifactory&lt;/h2&gt;
&lt;p&gt;我们在网络上最有趣的发现是一个名为 Tiller 的服务，这是 Helm 包管理器的服务器组件（版本 2）。&lt;/p&gt;
&lt;p&gt;与 Tiller 的通信是通过其 gRPC 接口在端口 44134 进行的，该端口默认是未经身份验证的。&lt;/p&gt;
&lt;p&gt;在我们的内部网络上查询这个服务器，揭示了对 SAP 的 Docker 注册表以及其 Artifactory 服务器的高权限密钥：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-通过-helm-服务器查询暴露的容器注册表和-artifactory-凭据&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;通过 Helm 服务器查询暴露的容器注册表和 Artifactory 凭据&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f9_hu15154392964960714063.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f9_hu9271754789093555752.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f9_hu10695779122234910877.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f9_hu15154392964960714063.webp&#34;
               width=&#34;760&#34;
               height=&#34;659&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      通过 Helm 服务器查询暴露的容器注册表和 Artifactory 凭据
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;使用这些密钥的读取权限，潜在的攻击者可以读取内部图像和构建，提取商业秘密，可能还包括客户数据。&lt;/p&gt;
&lt;p&gt;使用这些密钥的写权限，攻击者可以篡改图像和构建，对 SAP AI Core 服务进行供应链攻击。&lt;/p&gt;
&lt;h2 id=&#34;bug-5-未经身份验证的-helm-服务器危及-k8s-集群暴露-google-访问令牌和客户秘密&#34;&gt;Bug #5: 未经身份验证的 Helm 服务器危及 K8s 集群，暴露 Google 访问令牌和客户秘密&lt;/h2&gt;
&lt;p&gt;Helm 服务器暴露了读写操作。尽管读取权限暴露了敏感的秘密（如上所示），但服务器的写权限允许完全接管集群。&lt;/p&gt;
&lt;p&gt;Tiller 的&lt;code&gt;install&lt;/code&gt;命令接受一个 Helm 包并将其部署到 K8s 集群。我们创建了一个恶意 Helm 包，生成了一个具有&lt;code&gt;cluster-admin&lt;/code&gt;权限的新 Pod，并运行了安装命令。&lt;/p&gt;
&lt;p&gt;现在我们在集群上运行具有完全权限！&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-通过-helm-获得的-k8s-权限的部分列表&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;通过 Helm 获得的 K8s 权限的部分列表&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f10_hu1898260239118641762.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f10_hu2419965957431158519.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f10_hu9324779240786252293.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f10_hu1898260239118641762.webp&#34;
               width=&#34;760&#34;
               height=&#34;429&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      通过 Helm 获得的 K8s 权限的部分列表
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;使用这种访问级别，攻击者可以直接访问其他客户的 Pods 并窃取敏感数据，如模型、数据集和代码。这种访问还允许攻击者干扰客户的 Pods，污染 AI 数据并操纵模型的推理。&lt;/p&gt;
&lt;p&gt;此外，这种访问级别还将允许我们查看客户自己的秘密——甚至超出 SAP AI Core 范围的秘密。例如，我们的 AI Core 账户包含了我们的 AWS 账户（用于 S3 数据访问）、我们的 SAP HANA 账户（用于 Data Lake 访问）和我们的 Docker Hub 账户（用于拉取我们的镜像）的秘密。使用我们新获得的访问级别，我们查询了这些秘密，并设法以纯文本形式访问它们所有：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-使用我们的-k8s-权限访问客户秘密&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;使用我们的 K8s 权限访问客户秘密&#34; srcset=&#34;
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f11_hu12750870683359847843.webp 400w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f11_hu6128828811066188935.webp 760w,
               /blog/sapwned-sap-ai-vulnerabilities-ai-security/f11_hu7872809955157554974.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/sapwned-sap-ai-vulnerabilities-ai-security/f11_hu12750870683359847843.webp&#34;
               width=&#34;760&#34;
               height=&#34;513&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      使用我们的 K8s 权限访问客户秘密
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;同样的查询还揭示了一个名为&lt;code&gt;sap-docker-registry-secret&lt;/code&gt;的 SAP 访问 Google 容器注册表的密钥。我们已经确认这个密钥授予了读写权限——进一步扩大了潜在供应链攻击的范围。&lt;/p&gt;
&lt;h2 id=&#34;启示&#34;&gt;启示&lt;/h2&gt;
&lt;p&gt;我们对 SAP AI Core 的研究表明，深度防御的重要性。我们面临的主要安全障碍是 Istio 阻止我们的流量到达内部网络。一旦我们能够绕过这个障碍，我们就获得了对几个内部资产的访问权限，这些资产不需要任何其他身份验证——这意味着内部网络被视为可信的。加固这些内部服务本可以将这次攻击的影响降至最低，将其从完全服务接管降级为轻微的安全事件。&lt;/p&gt;
&lt;p&gt;符合我们之前与 K8s 相关的漏洞，这项研究还展示了在管理服务中使用 K8s 的租户隔离陷阱。控制平面（服务逻辑）和数据平面（客户计算）之间的关键分离受到了 K8s 架构的影响，该架构通过 API、身份、共享计算和软件分段网络允许它们之间的逻辑连接。&lt;/p&gt;
&lt;p&gt;此外，这项研究表明，AI R&amp;amp;D 过程引入的独特挑战。AI 培训本质上需要运行任意代码；因此，应该有适当的保护措施，确保不受信任的代码与内部资产和其他租户正确分离。&lt;/p&gt;
&lt;h2 id=&#34;披露时间线&#34;&gt;披露时间线&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2024 年 1 月 25 日&lt;/strong&gt; – Wiz 研究报告安全发现给 SAP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2024 年 1 月 27 日&lt;/strong&gt; – SAP 回复并分配了一个案件编号&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2024 年 2 月 16 日&lt;/strong&gt; – SAP 修复了第一个漏洞并旋转了相关的秘密&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2024 年 2 月 28 日&lt;/strong&gt; – Wiz 研究绕过补丁使用 2 个新漏洞，报告给 SAP&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2024 年 5 月 15 日&lt;/strong&gt; – SAP 部署修复了所有报告的漏洞&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;2024 年 7 月 17 日&lt;/strong&gt; – 公开披露&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;保持联系&#34;&gt;保持联系！&lt;/h2&gt;
&lt;p&gt;嗨，我们是 Wiz 研究团队的 Hillai Ben-Sasson（&lt;a href=&#34;https://twitter.com/hillai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@hillai&lt;/a&gt;），Shir Tamari（&lt;a href=&#34;https://twitter.com/shirtamari&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@shirtamari&lt;/a&gt;），Nir Ohfeld（&lt;a href=&#34;https://twitter.com/nirohfeld&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@nirohfeld&lt;/a&gt;），Sagi Tzadik（&lt;a href=&#34;https://twitter.com/sagitz_&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@sagitz_&lt;/a&gt;) 和 Ronen Shustin（&lt;a href=&#34;https://twitter.com/ronenshh&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;@ronenshh&lt;/a&gt;）。我们是一群资深白帽黑客，我们的目标是让云成为每个人更安全的地方。我们主要关注在云中找到新的攻击向量并揭露云供应商的隔离问题。&lt;/p&gt;
&lt;p&gt;我们很想听听您的意见！欢迎通过 Twitter 或电子邮件与我们联系：&lt;a href=&#34;mailto:research@wiz.io&#34;&gt;research@wiz.io&lt;/a&gt;。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>了解 Llama 3：迄今最强大的免费开源大模型从概念到使用</title>
      <link>https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/</link>
      <pubDate>Mon, 01 Jul 2024 09:33:23 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/</guid>
      <description>&lt;p&gt;Meta 公司最近发布了 &lt;a href=&#34;https://ai.meta.com/blog/meta-llama-3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llama 3&lt;/a&gt;，这是其最新一代尖端开源大型语言模型（LLM）。基于其前身的基础之上，Llama 3 旨在提升 Llama 2 作为与 ChatGPT 竞争的重要开源产品的能力，如文章 &lt;a href=&#34;https://www.unite.ai/llama-2-a-deep-dive-into-the-open-source-challenger-to-chatgpt/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llama 2: 深入探索开源挑战者 ChatGPT&lt;/a&gt; 中全面回顾的那样。&lt;/p&gt;
&lt;p&gt;在本文中，我们将讨论 Llama 3 背后的核心概念，探索其创新架构和训练过程，并提供关于如何负责任地访问、使用和部署这一开创性模型的实际指导。无论你是研究人员、开发者还是 AI 爱好者，这篇文章都将为你提供利用 Llama 3 为你的项目和应用赋能的知识和资源。&lt;/p&gt;
&lt;h2 id=&#34;llama-的演变从-llama-2-到-llama-3&#34;&gt;Llama 的演变：从 Llama 2 到 Llama 3&lt;/h2&gt;
&lt;p&gt;Meta 的 CEO，Mark Zuckerberg，在 &lt;a href=&#34;https://www.threads.net/@zuck/post/C56MFEKxl-x&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Threads.net&lt;/a&gt; 上宣布了 Llama 3 的首次亮相，这是 Meta AI 开发的最新 AI 模型。这个尖端模型现在已开源，旨在提升 Meta 的各种产品，包括 Messenger 和 Instagram。Zuckerberg 强调，Llama 3 使 Meta AI 成为最先进的&lt;a href=&#34;https://about.fb.com/news/2024/04/meta-ai-assistant-built-with-llama-3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;免费可用的 AI 助手&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;在我们讨论 Llama 3 的具体细节之前，让我们简要回顾一下它的前身，Llama 2。Llama 2 于 2022 年推出，是开源 LLM 领域的一个重要里程碑，提供了一个强大而高效的模型，可以在消费者硬件上运行。&lt;/p&gt;
&lt;p&gt;然而，尽管 Llama 2 取得了显著的成就，但它也有其局限性。用户报告了一些问题，如错误拒绝（模型拒绝回答无害的提示）、有限的帮助性，以及在推理和代码生成等领域的改进空间。&lt;/p&gt;
&lt;p&gt;进入 Llama 3：Meta 对这些挑战和社区的反馈做出了回应。通过 Llama 3，Meta 设定了与当今市场上顶级专有模型相媲美的最佳开源模型的目标，同时也优先考虑了负责任的开发和部署实践。&lt;/p&gt;
&lt;h2 id=&#34;llama-3架构和训练&#34;&gt;Llama 3：架构和训练&lt;/h2&gt;
&lt;p&gt;Llama 3 的一项关键创新是其分词器，特点是显著扩展的词汇表，&lt;strong&gt;128,256 个 token&lt;/strong&gt;（从 Llama 2 的 32,000 个增加）。这更大的词汇表允许更有效的文本编码，无论是输入还是输出，可能导致更强的多语言能力和整体性能的提升。&lt;/p&gt;
&lt;p&gt;Llama 3 还采用了&lt;strong&gt;分组查询注意力&lt;/strong&gt;（GQA），这是一种提高可扩展性的有效表示技术，有助于模型更有效地处理更长的上下文。&lt;strong&gt;8B&lt;/strong&gt; 版本的 Llama 3 使用了 GQA，而&lt;strong&gt;8B&lt;/strong&gt; 和 &lt;strong&gt;70B&lt;/strong&gt; 模型都可以处理长达 &lt;strong&gt;8,192 个 token&lt;/strong&gt;的序列。&lt;/p&gt;
&lt;h3 id=&#34;训练数据和扩展&#34;&gt;训练数据和扩展&lt;/h3&gt;
&lt;p&gt;用于 Llama 3 的训练数据是其性能提升的关键因素。Meta 精心策划了一个包含超过 &lt;strong&gt;15 万亿&lt;/strong&gt; token 的庞大数据集，来自公开可获得的在线来源，是用于 Llama 2 的数据集的七倍。这个数据集还包括了超过 5% 的高质量非英语数据，涵盖了 &lt;strong&gt;30 多种语言&lt;/strong&gt;，为未来的多语言应用做准备。&lt;/p&gt;
&lt;p&gt;为了确保数据质量，Meta 采用了先进的过滤技术，包括启发式过滤器、NSFW 过滤器、语义去重和训练在 Llama 2 上预测数据质量的文本分类器。团队还进行了广泛的实验，以确定预训练的最佳数据来源组合，确保 Llama 3 在广泛的用例上表现良好，包括琐事、STEM、编码和历史知识。&lt;/p&gt;
&lt;p&gt;放大预训练是 Llama 3 开发的另一个关键方面。Meta 开发了缩放法则，使他们能够在实际训练之前预测其最大模型在关键任务上的性能，如代码生成。这些信息指导了关于数据组合和计算分配的决策，最终导致了更有效和有效的培训。&lt;/p&gt;
&lt;p&gt;Llama 3 最大的模型是在两个定制构建的 24,000 GPU 集群上训练的，利用数据并行、模型并行和流水线并行技术的组合。Meta 的先进训练堆栈自动化了错误检测、处理和维护，最大化了 GPU 的运行时间，使训练效率比 Llama 2 提高了大约三倍。&lt;/p&gt;
&lt;h3 id=&#34;指令微调和性能&#34;&gt;指令微调和性能&lt;/h3&gt;
&lt;p&gt;为了充分发挥 Llama 3 在聊天和对话应用中的潜力，Meta 创新了其指令微调方法。其方法结合了&lt;strong&gt;监督微调&lt;/strong&gt;（SFT）、拒绝抽样、&lt;strong&gt;近端政策优化&lt;/strong&gt;（PPO）和&lt;strong&gt;直接偏好优化&lt;/strong&gt;（DPO）。&lt;/p&gt;
&lt;p&gt;SFT 中使用的提示质量和在 PPO 和 DPO 中使用的偏好排名在对齐模型的性能中起着关键作用。Meta 的团队精心策划了这些数据，并对由人类注释者提供的注释进行了多轮质量保证。&lt;/p&gt;
&lt;p&gt;通过 PPO 和 DPO 对偏好排名进行训练还显著提高了 Llama 3 在推理和编码任务上的性能。Meta 发现，即使模型在直接回答推理问题时遇到困难，它仍然可能产生正确的推理迹象。通过偏好排名的训练，模型学会了如何从这些迹象中选择正确的答案。&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-对比结果&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;对比结果&#34; srcset=&#34;
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu1078991893979778142.webp 400w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu5794619480392154546.webp 760w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu13233647470042596874.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f2_hu1078991893979778142.webp&#34;
               width=&#34;760&#34;
               height=&#34;617&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      对比结果
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;成果显而易见：Llama 3 在常见的行业基准测试中表现优于许多可用的开源聊天模型，为 LLM 的 8B 和 70B 参数级别建立了新的最佳性能。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu13002368666571729442.webp 400w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu10874792885244218593.webp 760w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu623210716393878442.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f3_hu13002368666571729442.webp&#34;
               width=&#34;760&#34;
               height=&#34;467&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;负责任的开发和安全考虑&#34;&gt;负责任的开发和安全考虑&lt;/h2&gt;
&lt;p&gt;在追求尖端性能的同时，Meta 也优先考虑了负责任的开发和部署实践，用于 Llama 3。该公司采用了系统级方法，将 Llama 3 模型视为更广泛生态系统的一部分，使开发者能够设计和定制模型以满足其特定用例和安全要求。&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu7484193510083323431.webp 400w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu15430403253064485765.webp 760w,
               /blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu13946329183677072181.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/everything-you-need-to-know-about-llama-3-most-powerful-open-source-model-yet-concepts-to-usage/f4_hu7484193510083323431.webp&#34;
               width=&#34;760&#34;
               height=&#34;300&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Meta 进行了广泛的红队演习，执行了对抗评估，并实施了安全缓解技术，以降低其指令调优模型中的残余风险。然而，该公司承认可能仍会存在残余风险，并建议开发者在其特定用例的背景下评估这些风险。&lt;/p&gt;
&lt;p&gt;为支持负责任的部署，Meta 更新了其负责任使用指南，为开发者提供了一个全面的资源，以实施模型和系统级安全最佳实践，用于他们的应用。该指南涵盖了内容审查、风险评估和使用安全工具（如 Llama Guard 2 和 Code Shield）等主题。&lt;/p&gt;
&lt;p&gt;Llama Guard 2，基于 MLCommons 分类法构建，旨在对 LLM 输入（提示）和响应进行分类，检测可能被视为不安全或有害的内容。CyberSecEval 2 在其前身的基础上增加了措施，以防止模型的代码解释器被滥用、攻击性网络安全能力和对提示注入攻击的易感性。&lt;/p&gt;
&lt;p&gt;Code Shield 是 Llama 3 新推出的一个介绍，增加了推断时间的不安全代码过滤，减轻了不安全代码建议、代码解释器滥用和安全命令执行等风险。&lt;/p&gt;
&lt;h2 id=&#34;访问和使用-llama-3&#34;&gt;访问和使用 Llama 3&lt;/h2&gt;
&lt;p&gt;随着 Meta AI 的 Llama 3 发布，已推出了几种开源工具，可在各种操作系统上进行本地部署，包括 Mac、Windows 和 Linux。本节详细介绍了三个值得注意的工具：Ollama、Open WebUI 和 LM Studio，每个工具都提供了利用 Llama 3 功能的独特功能。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Ollama&lt;/strong&gt;：适用于 Mac、Linux 和 Windows，&lt;a href=&#34;https://ollama.com/download&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ollama&lt;/a&gt; 简化了在个人计算机上操作 Llama 3 和其他大型语言模型的过程，即使是那些硬件较弱的设备也是如此。它包括一个包管理器，便于模型管理，并支持跨平台的下载和运行模型的命令。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open WebUI with Docker&lt;/strong&gt;：这个工具提供了一个用户友好的、基于 &lt;a href=&#34;https://docs.docker.com/desktop/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Docker&lt;/a&gt; 的界面，兼容 Mac、Linux 和 Windows。它与 Ollama 注册表中的模型无缝集成，允许用户在本地 Web 界面内部署和交互，例如 Llama 3。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;：面向 Mac、Linux 和 Windows 的用户，&lt;a href=&#34;https://lmstudio.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LM Studio&lt;/a&gt; 支持一系列模型，基于 llama.cpp 项目构建。它提供了一个聊天界面，便于直接与各种模型进行交互，包括 Llama 3 8B Instruct 模型。&lt;/p&gt;
&lt;p&gt;这些工具确保用户可以在个人设备上高效利用 Llama 3，满足技术技能和需求的不同范围。每个平台都提供了设置和模型交互的分步过程，使先进的人工智能更加易于开发者和爱好者接触。&lt;/p&gt;
&lt;h2 id=&#34;大规模部署-llama-3&#34;&gt;大规模部署 Llama 3&lt;/h2&gt;
&lt;p&gt;除了直接提供模型权重外，Meta 还与各种云提供商、模型 API 服务和硬件平台合作，实现 Llama 3 的无缝部署。&lt;/p&gt;
&lt;p&gt;Llama 3 的一大优势是其改进的 token 效率，这要归功于新的分词器。基准测试显示，与 Llama 2 相比，Llama 3 需要的 token 减少了 &lt;strong&gt;15%&lt;/strong&gt;，从而实现了更快、更经济的推断。&lt;/p&gt;
&lt;p&gt;Grouped Query Attention (GQA) 的整合在 Llama 3 的 8B 版本中有助于保持与 Llama 2 的 7B 版本相当的推断效率，尽管参数数量增加了。&lt;/p&gt;
&lt;p&gt;为简化部署流程，Meta 提供了 Llama Recipes 代码库，其中包含开源代码和微调、部署、模型评估等示例。这个代码库为开发者提供了一个宝贵的资源，帮助他们利用 Llama 3 的能力来应用到他们的应用中。&lt;/p&gt;
&lt;p&gt;对于那些有兴趣探索 Llama 3 性能的人来说，Meta 已经将其最新模型整合到 Meta AI 中，这是一个以 Llama 3 技术构建的领先人工智能助手。用户可以通过各种 Meta 应用程序，如 Facebook、Instagram、WhatsApp、Messenger 和 Web 与 Meta AI 进行交互，以完成任务、学习、创造和与他们关心的事物建立联系。&lt;/p&gt;
&lt;h2 id=&#34;llama-3-接下来会怎样&#34;&gt;Llama 3 接下来会怎样？&lt;/h2&gt;
&lt;p&gt;尽管 8B 和 70B 模型标志着 Llama 3 发布的开始，但 Meta 对这款开创性 LLM 的未来有雄心勃勃的计划。&lt;/p&gt;
&lt;p&gt;在未来几个月，我们可以期待看到新功能的引入，包括多模态（能够处理和生成不同数据模态，如图像和视频）、多语言支持（支持多种语言）和更长的上下文窗口，以提高在需要广泛上下文的任务上的性能。&lt;/p&gt;
&lt;p&gt;此外，Meta 计划发布更大的模型大小，包括目前正在训练中的超过 4000 亿参数的模型，这些模型在性能和能力方面展现出了有前途的趋势。&lt;/p&gt;
&lt;p&gt;为了进一步推进该领域的发展，Meta 还将发布关于 Llama 3 的详细研究论文，与广泛的 AI 社区分享其发现和见解。&lt;/p&gt;
&lt;p&gt;作为即将推出的内容的预览，Meta 分享了一些其最大 LLM 模型在各种基准上的早期性能快照。尽管这些结果是基于早期检查点的，并且可能会发生变化，但它们提供了一个激动人心的展望，展示了 Llama 3 的未来潜力。&lt;/p&gt;
&lt;h2 id=&#34;结论&#34;&gt;结论&lt;/h2&gt;
&lt;p&gt;Llama 3 代表了开源大型语言模型演变的一个重要里程碑，推动了性能、能力和负责任开发实践的边界。凭借其创新架构、庞大的训练数据集和尖端的微调技术，Llama 3 为 LLM 的 8B 和 70B 参数级别建立了新的最佳性能基准。&lt;/p&gt;
&lt;p&gt;然而，Llama 3 不仅仅是一个强大的语言模型；它还体现了 Meta 致力于培养一个开放和负责任的 AI 生态系统的承诺。通过提供全面的资源、安全工具和最佳实践，Meta 授权开发者充分利用 Llama 3 的潜力，同时确保根据其特定用例和受众的需求实现负责任的部署。&lt;/p&gt;
&lt;p&gt;随着 Llama 3 之旅的继续，随着新的能力、模型大小和研究发现的出现，AI 社区热切期待从这款开创性 LLM 中涌现出的创新应用和突破。&lt;/p&gt;
&lt;p&gt;无论你是一名推动自然语言处理边界的研究人员、一名构建下一代智能应用的开发者还是对最新进展感到好奇的 AI 爱好者，Llama 3 都承诺成为你工具箱中的强大工具，开启新的大门并解锁一系列可能性。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>给初学生成式 AI（GenAI）的开发人员的 7 条最佳实践</title>
      <link>https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/</link>
      <pubDate>Wed, 20 Dec 2023 11:30:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/</guid>
      <description>&lt;h2 id=&#34;编者按&#34;&gt;编者按&lt;/h2&gt;
&lt;p&gt;本文译自：&lt;a href=&#34;https://thenewstack.io/7-best-practices-for-developers-getting-started-with-genai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thenewstack.io/7-best-practices-for-developers-getting-started-with-genai/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;编辑评论：这是一篇非常有价值的文章，向开发者展示了生成式 AI 的潜力和应用。生成式 AI 是一种利用大型语言模型来生成和转换文本的技术，它可以帮助开发者解决一些复杂的问题，如代码生成，文档编写，内容创作等。生成式 AI 也是一种云原生的技术，它需要大量的计算资源和数据，以及高效的部署和管理方式。文章提供了一些实用的工具和平台，如 GitHub Copilot，Bard，ChatGPT 等，让开发者可以轻松地尝试和使用生成式 AI。文章还给出了一些注意事项和建议，如保护数据隐私，验证输出质量，避免滥用等，让开发者可以负责任地使用生成式 AI。我认为这篇文章是一个很好的入门指南，让开发者可以了解和利用生成式 AI 来打造创新的云原生应用。&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;正文&#34;&gt;正文&lt;/h2&gt;
&lt;p&gt;通过一点经验，你可以使用 GenAI 解决一些相当困难的问题，就像学习任何新技术一样，最好的方法就是动手实践。&lt;/p&gt;
&lt;p&gt;随着可访问的生成式人工智能进入主流，以及由此产生的通过简单语言转化整个人类知识的能力，每个企业都在竭力将人工智能整合到其技术体系中。对于开发人员来说，压力很大，但也有着令人兴奋的无限可能性。&lt;/p&gt;
&lt;p&gt;如果你有一些经验，你可以使用 GenAI 解决一些相当困难的问题，就像学习自 HTML 诞生以来的每一项新技术一样。让我们看看你可以采取的七个步骤，以开始建立 GenAI 的基础，并最终逐步发展成一个完全运作、可扩展的应用程序。&lt;/p&gt;
&lt;h2 id=&#34;1-玩转现有的-genai-工具&#34;&gt;1. 玩转现有的 GenAI 工具&lt;/h2&gt;
&lt;p&gt;入门 GenAI 的最佳方法是实践，而且门槛非常低。市场上现在有许多免费选项，比如 Bard、ChatGPT、Bing 和 Anthropic，有很多可以学习的选择。&lt;/p&gt;
&lt;p&gt;尝试使用 GenAI 工具和代码生成解决方案进行实验（并鼓励你的团队进行实验），例如&lt;a href=&#34;https://github.com/features/copilot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub Copilot&lt;/a&gt;，它集成到每个流行的 IDE 中，充当一对程序员。Copilot 提供程序员建议，帮助解决代码问题，并生成整个函数，使学习和适应&lt;a href=&#34;https://us.resources.cio.com/resources/the-data-streaming-platform-key-to-ai-initiatives-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GenAI&lt;/a&gt;变得更快更容易。&lt;/p&gt;
&lt;p&gt;当你首次使用这些现成的工具时，要小心使用专有或敏感的公司数据，即使只是提供给工具一个提示也要小心。Gen AI 供应商可能会存储并使用你的数据用于将来的训练运行，这是公司数据政策和信息安全协议的重大违规行为。确保你及时直接地向你的团队传达这一黄金规则。&lt;/p&gt;
&lt;h2 id=&#34;2-了解从-genai-中可以获得什么&#34;&gt;2. 了解从 GenAI 中可以获得什么&lt;/h2&gt;
&lt;p&gt;一旦你开始尝试 GenAI，你将很快了解到不同提示会产生什么类型的输出。大多数 GenAI 工具可以生成各种格式的文本，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;生成&lt;/strong&gt;新的故事、想法、文章或任意长度的文本。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;转换&lt;/strong&gt;现有文本为不同格式，如 JSON、Markdown 或 CSV。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;翻译&lt;/strong&gt;文本成不同语言。&lt;/li&gt;
&lt;li&gt;以聊天的方式&lt;strong&gt;对话&lt;/strong&gt;来回。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;审查&lt;/strong&gt;文本以展示特定元素。&lt;/li&gt;
&lt;li&gt;将长篇内容&lt;strong&gt;汇总&lt;/strong&gt;以获取洞察。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分析&lt;/strong&gt;文本的情感。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;任何人都可以生成这些类型的生成文本结果，无需编程技能。只需键入提示，文本就会产生。然而，大型语言模型（LLM）经过的培训越多，即它摄取的语言碎片越多，随着时间的推移，它在生成、更改和分析文本方面就会变得更加准确。&lt;/p&gt;
&lt;h2 id=&#34;3-学习提示工程&#34;&gt;3. 学习提示工程&lt;/h2&gt;
&lt;p&gt;部署 GenAI 的良好方法之一是掌握编写提示的技巧，这既是一门艺术又是一门科学。虽然提示工程师是一个实际的职位描述，但它也是任何希望提高他们使用 AI 的人的好绰号。优秀的提示工程师知道如何开发、完善和优化文本提示，以获得最佳结果并提高整个 AI 系统的性能。&lt;/p&gt;
&lt;p&gt;提示工程不需要特定的学位或背景，但从事这项工作的人需要擅长清晰解释事物。这是重要的，因为所有可用的 LLM 都是无状态的，这意味着没有长期记忆，每次交互只存在于小会话中。&lt;/p&gt;
&lt;p&gt;在提示工程中，以下三个因素变得重要：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;上下文&lt;/strong&gt;：你提出的问题、聊天历史记录和你设置的参数。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;知识&lt;/strong&gt;：LLM 已经接受的培训内容以及你通过提示提供的新信息的结合。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;形式&lt;/strong&gt;：你期望以何种方式生成信息的语气。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上下文、知识和形式的结合塑造了 GenAI 的大量知识存储成为你希望获得的响应类型。&lt;/p&gt;
&lt;h2 id=&#34;4-探索其他-genai-提示方法&#34;&gt;4. 探索其他 GenAI 提示方法&lt;/h2&gt;
&lt;p&gt;到目前为止，我们一直在谈论零-shot 提示，这基本上意味着提出一个带有一些上下文的问题。如果你从这种方法中没有得到期望的结果，还有四种提示 GenAI 的方法。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;单次提示&lt;/strong&gt;：提供你正在寻找的输出类型的示例。如果你想要特定类型的格式，例如[标题]和[4 个要点]，这将特别有用。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;少量提示&lt;/strong&gt;：这类似于单次提示，但你会提供三到五个示例而不仅仅是一个。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“让我们一步一步地思考”&lt;/strong&gt;：这种技巧对 LLM 和对人都同样有效。如果你有一个包含多个部分的复杂问题，请在末尾输入此短语，等待 LLM 分解问题。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;思路链提示&lt;/strong&gt;：对于涉及复杂算术或其他推理任务的问题，思路链提示会指示工具“展示其工作方式”并解释其如何得出答案。以下是可能的示例：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu14801736640312961138.webp 400w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu14731880502347286634.webp 760w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu5759154962722220918.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/f1_hu14801736640312961138.webp&#34;
               width=&#34;760&#34;
               height=&#34;402&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;5-查看其他-genai-工作示例&#34;&gt;5. 查看其他 GenAI 工作示例&lt;/h2&gt;
&lt;p&gt;一旦你熟悉了 GenAI 工具并了解如何编写出色的提示，&lt;a href=&#34;https://github.com/openai/openai-cookbook/tree/main/examples&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;请查看 OpenAI 发布的一些示例&lt;/a&gt;，了解其他人正在做什么以及可能的其他可能性。随着你的实验，你将更加熟悉聊天界面，并学会如何对其进行微调，以便熟练地缩小响应范围，甚至将响应转换为 CSV 文件或其他类型的表格。&lt;/p&gt;
&lt;p&gt;考虑如何将你的 GenAI 知识应用于你的业务，以简化困难或重复性任务，生成创意并使信息易于让更广泛的受众访问。你可以想象出哪些新的用例？以前不可能的东西现在成为可能了吗？&lt;/p&gt;
&lt;h2 id=&#34;6-集成第三方-genai-工具和-api&#34;&gt;6. 集成第三方 GenAI 工具和 API&lt;/h2&gt;
&lt;p&gt;考虑使用 ChatGPT、Bard 和 Claude 2 等 API 通过 API 使用 LLMs 的角色。这些工具都提供了强大的 API，并有支持文档，因此入门门槛很低。大多数这些 API 是基于使用量的，因此更容易玩弄。&lt;/p&gt;
&lt;p&gt;通常情况下，通过语义搜索和由向量数据库支持的嵌入来将自定义或私有数据集成到 LLM 提示中，你还可以集成自定义或私有数据。通常称为 RAG（检索增强生成）。&lt;/p&gt;
&lt;p&gt;分解这两个术语：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;语义搜索&lt;/strong&gt;：使用词嵌入比较查询的含义与其索引中文档的含义，即使没有完全匹配的单词也能获得更相关的结果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;嵌入&lt;/strong&gt;：将对象（如单词、句子或整个文档）的数值表示转化为多维空间。这使得评估不同实体之间的关系成为可能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下是这可能看起来的一个示例：&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu1009306822241985111.webp 400w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu11470018464599431523.webp 760w,
               /blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu17865044914941909841.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/7-best-practices-for-developers-getting-started-with-genai/f2_hu1009306822241985111.webp&#34;
               width=&#34;760&#34;
               height=&#34;608&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;这幅图展示了“猫”和“狗”的概念比它们与“人”或“蜘蛛”的概念更接近，“车辆汽车”则是最远的，是概念中最不相关的。（&lt;a href=&#34;https://www.confluent.io/blog/chatgpt-and-streaming-data-for-real-time-generative-ai/#connecting-knowledge-base-to-gpt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里有更多关于如何使用语义搜索和嵌入的信息&lt;/a&gt;。）&lt;/p&gt;
&lt;h2 id=&#34;7-从头开始训练自己的模型&#34;&gt;7. 从头开始训练自己的模型&lt;/h2&gt;
&lt;p&gt;这最后的建议实际上不太像建议，更像是一个“可选的下一步”。训练自己的 GenAI 模型并不适合每个人，但如果你：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拥有独特而有价值的知识库。&lt;/li&gt;
&lt;li&gt;想要执行商业 LLM 无法完成的某些任务。&lt;/li&gt;
&lt;li&gt;发现商业 LLM 的推理成本在商业上没有意义。&lt;/li&gt;
&lt;li&gt;有特定的安全要求，需要托管自己的 LLM 数据，并且不愿通过第三方 API 传递数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;训练自己的模型的一种方法是使用开源模型，例如 Llama 2、Mosaic MPT-7B、Falcon 或 Vicuna，其中许多还提供了商业使用许可证。这些通常根据它们具有的参数数量进行标记：7B、13B、40B 等。 “B”代表模型的参数数目，以及它可以处理和存储的信息量。数字越高，模型就越复杂和复杂，但训练和运行成本也越高。如果你的用例不复杂，并且如果你计划在性能相当强大的现代笔记本电脑上运行模型，那么具有较低参数的模型是开始的最佳且最经济的方法。&lt;/p&gt;
&lt;p&gt;中大型组织可能会选择从头开始构建和训练一个 LLM 模型。这是一条非常昂贵、资源密集且耗时的 AI 之路。你需要难以招聘的技术人才，并具备长时间迭代的机会，因此对大多数组织来说，这条路线不现实。&lt;/p&gt;
&lt;h3 id=&#34;微调-llm&#34;&gt;微调 LLM&lt;/h3&gt;
&lt;p&gt;一些组织选择中间路径：微调基本开源 LLM 以实现模型预训练能力之外的特定功能。如果你希望以你品牌独特的声音创建虚拟助手或基于真实客户购买构建的推荐系统，那么这是一个很好的选择。这些模型会随着你纳入排名靠前的用户交互而不断地训练自己。事实上，&lt;a href=&#34;https://voicebot.ai/2023/08/23/openai-brings-fine-tuning-to-gpt-3-5-turbo-and-gpt-4/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open AI 报告&lt;/a&gt;，使用此模型，可以将提示长度缩短多达 90%，同时保持性能不变。此外，Open AI 的商业 API 的最新增强功能使其与驱动 ChatGPT 和 Bing AI 的模型一样强大和易于访问。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>微软零成本收购 OpenAI？——OpenAI 的错位与微软的收益</title>
      <link>https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/</link>
      <pubDate>Tue, 21 Nov 2023 14:40:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/</guid>
      <description>&lt;p&gt;上个周末的 OpenAI“宫斗”大戏相信大家都知晓了，今天正巧看到 Ben Thompson 的这篇总结文章，感觉不错，分享给大家。&lt;/p&gt;
&lt;p&gt;关于作者：Ben Thompson（本·汤普森）是一位知名的科技和商业评论家，他是 Stratechery（《战略》）博客的创始人和作者。Stratechery 是一家专注于科技产业和商业策略分析的网站，汤普森在博客中深入剖析科技公司、产品、行业趋势和商业模式。他的文章通常涵盖了互联网、数字媒体、云计算、人工智能等领域的重要话题。&lt;/p&gt;
&lt;p&gt;Ben Thompson 因其深刻的洞察力和独特的分析方法而闻名，他的文章常常引发行业内外的广泛讨论。他的分析不仅关注技术本身，还关注技术如何塑造和影响商业和社会。由于其博客的高质量内容，他已经建立了一支忠实的读者群，并成为了科技产业和商业领域的重要声音之一。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;下文译自：&lt;a href=&#34;https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stratechery.com/2023/openais-misalignment-and-microsofts-gain/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;摘要：
这篇文章分析了 OpenAI 的使命与商业模式之间的不一致，以及 Microsoft 如何从中受益。文章认为，OpenAI 的 GPT-3 模型是一种强大的通用人工智能，但是它的许可协议限制了它的应用范围。Microsoft 作为 OpenAI 的合作伙伴和投资者，拥有 GPT-3 的独家许可权，可以将其用于自己的产品和服务，从而获得竞争优势。&lt;/p&gt;
&lt;p&gt;正如您所预期的那样，我已经在脑海中和页面上撰写了几个版本的这篇文章，因为&lt;a href=&#34;https://twitter.com/benthompson/status/1726514608234746003&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;我职业生涯中最不同寻常的周末&lt;/a&gt;已经开始。简要总结如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;周五，时任 CEO Sam Altman 被 OpenAI 的董事会解雇；接着，OpenAI 总裁 Greg Brockman 被免去董事会职务，并随后辞职。&lt;/li&gt;
&lt;li&gt;周末期间，有传闻称 Altman 正在谈判回归，然后 OpenAI 聘请了前 Twitch CEO Emmett Shear 担任 CEO。&lt;/li&gt;
&lt;li&gt;最后，在周日深夜，&lt;a href=&#34;https://twitter.com/satyanadella/status/1726509045803336122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Satya Nadella 通过推文宣布&lt;/a&gt;，Altman 和 Brockman 以及他们的“同事”将加入 Microsoft。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;很显然，对于 Microsoft 来说，这是一个非凡的成果。该公司已经获得了&lt;a href=&#34;https://www.wsj.com/articles/microsoft-and-openai-forge-awkward-partnership-as-techs-new-power-couple-3092de51&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;所有 OpenAI 知识产权的永久许可证&lt;/a&gt;（&lt;a href=&#34;https://openai.com/our-structure#:~:text=the%20board%20determines%20when%20we%27ve%20attained%20AGI.%20Again%2C%20by%20AGI%20we%20mean%20a%20highly%20autonomous%20system%20that%20outperforms%20humans%20at%20most%20economically%20valuable%20work.%20Such%20a%20system%20is%20excluded%20from%20IP%20licenses%20and%20other%20commercial%20terms%20with%20Microsoft%2C%20which%20only%20apply%20to%20pre%2DAGI%20technology.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;除通用人工智能之外&lt;/a&gt;)，包括源代码和模型权重；问题是，如果 OpenAI 遭受了人才流失的威胁，Altman 和 Brockman 被解雇，它是否有足够的才能来利用那些知识产权。实际上，他们将拥有足够多的人才，似乎很有可能流向 Microsoft；你可以这样说，Microsoft 刚刚以零风险和零成本收购了 OpenAI。&lt;/p&gt;
&lt;p&gt;注：微软与 OpenAI 的最初协议还禁止微软基于 OpenAI 技术单独开发通用人工智能（AGI）；据我了解，这一条款已在最近的协议中被删除&lt;/p&gt;
&lt;p&gt;与此同时，对于 OpenAI 来说，这是一个损失，因为它在金钱和计算资源上依赖于总部位于雷德蒙德的 Microsoft 公司：OpenAI 员工在人工智能方面的工作要么因为永久许可证属于 Microsoft，要么因为员工加入了 Altman 的团队而成为 Microsoft 的直接财产。OpenAI 的王牌是 ChatGPT，它正在朝着技术的至高境界迈进——一个大规模的消费者平台——但是如果周末的报道可信，OpenAI 的董事会可能已经对 ChapGPT 给公司带来的激励产生了疑虑（后文详述）。&lt;/p&gt;
&lt;p&gt;然而，最大的损失是一个必然的损失：即除了盈利公司，任何其他组织形式都不是正确的公司组织方式。&lt;/p&gt;
&lt;h2 id=&#34;openai-的非营利模式&#34;&gt;OpenAI 的非营利模式&lt;/h2&gt;
&lt;p&gt;OpenAI 成立于 2015 年，是一家被称为“非营利智能研究公司”的组织。从他们的&lt;a href=&#34;https://openai.com/blog/introducing-openai&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;最初博客文章&lt;/a&gt;中可以得知：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;OpenAI 是一家非营利人工智能研究公司。我们的目标是以最有可能造福整个人类的方式推进数字智能，不受需要产生财务回报的限制。由于我们的研究不受财务义务的制约，我们可以更好地专注于对人类产生积极影响。我们认为人工智能应该是个体人类意愿的延伸，在自由的精神中尽可能广泛和均匀地分布。这个冒险的结果是不确定的，工作是困难的，但我们认为目标和结构是正确的。我们希望这对领域最优秀的人来说是最重要的。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;对于 OpenAI 的创始人，特别是 Altman 和 Elon Musk 的动机，我曾经有过相当怀疑；我在一篇&lt;a href=&#34;https://stratechery.com/2015/openai-artificial-intelligence-and-data-data-and-recruiting/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;每日更新&lt;/a&gt;中写到：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Elon Musk 和 Sam Altman 分别领导着特斯拉和 YCombinator 等组织，这些组织看起来很像我刚刚描述的那两个受到 Google 和 Facebook 数据优势威胁的公司的例子，他们与 OpenAI 做到了这一点，还有使整个事情成为非营利性组织的额外动机；我说“动机”是因为成为非营利性组织几乎肯定更多地与我在开头强调的那句话有关：“我们希望这对领域中最优秀的人来说最重要。”换句话说，OpenAI 也许没有最好的数据，但至少它有一个可能帮助理想主义研究人员晚上睡得更香的使命结构。OpenAI 可能有助于平衡特斯拉和 YCombinator 的竞争局势，我猜我们应该相信这是一个愉快的巧合。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;无论 Altman 和 Musk 的动机如何，将 OpenAI 建立为非营利性组织的决定并不仅仅是口头上的；该公司是一个 501(c)(3) 组织；您可以在&lt;a href=&#34;https://projects.propublica.org/nonprofits/organizations/810861541&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;这里&lt;/a&gt;查看他们的年度 IRS 申报。Form 990 上的第一个问题要求组织“简要描述组织的使命或最重要的活动”；&lt;a href=&#34;https://projects.propublica.org/nonprofits/organizations/810861541/201703459349300445/full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2016 年的第一份申报&lt;/a&gt;中写道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;OpenAI 的目标是以最有可能使整个人类受益的方式推进数字智能，不受产生财务回报的需求限制。我们认为人工智能技术将有助于塑造 21 世纪，我们希望帮助世界构建安全的人工智能技术，并确保人工智能的利益尽可能广泛均衡地分布。我们试图作为更大社区的一部分构建人工智能，并希望在此过程中公开分享我们的计划和能力。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;两年后，“公开分享我们的计划和能力”的承诺消失了；三年后，“推进数字智能”的目标被替换为“构建通用人工智能”。&lt;/p&gt;
&lt;p&gt;2018 年，根据今年早些时候的一份 Semafor 报告，马斯克试图接管该公司，但被拒绝了；他离开了董事会，并且更为关键的是，停止了对 OpenAI 的运营支持。这导致了第二个关键背景信息：面对需要支付大量计算资源的需求，现在坚决掌控 OpenAI 的 Altman 创建了 OpenAI Global, LLC，这是一家有上限的营利性公司，微软是少数股东。这是 OpenAI 当前结构的图像来自&lt;a href=&#34;https://openai.com/our-structure&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;他们的网站&lt;/a&gt;：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-openai-的公司结构&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;OpenAI 的公司结构&#34; srcset=&#34;
               /blog/openais-misalignment-and-microsofts-gain/openai-2_hu5409524645142908902.webp 400w,
               /blog/openais-misalignment-and-microsofts-gain/openai-2_hu4922875475911230746.webp 760w,
               /blog/openais-misalignment-and-microsofts-gain/openai-2_hu10391124918716492126.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/openai-2_hu5409524645142908902.webp&#34;
               width=&#34;640&#34;
               height=&#34;464&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      OpenAI 的公司结构
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;OpenAI Global 可以筹集资金，而且对于其投资者来说至关重要的是，它可以盈利，但它仍然在非营利性组织和其使命的监督下运营；OpenAI Global 的经营协议规定：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该公司的存在是为了推进 OpenAI, Inc.确保开发安全的人工通用智能并使其造福全人类的使命。该公司对这一使命以及 OpenAI, Inc.宪章中提出的原则的责任优先于产生利润的任何义务。该公司可能永远不会盈利，也没有义务这样做。该公司可以自由重新投资公司的全部或部分现金流于研究和开发活动和/或相关费用，而无需向成员承担任何义务。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;chatgpt-族群&#34;&gt;ChatGPT 族群&lt;/h3&gt;
&lt;p&gt;第三个关键背景信息是最为人熟知的，也推动了这些雄心壮志达到了新的高度：ChatGPT 在 2022 年 11 月底发布，震撼了整个世界。如今，ChatGPT 拥有超过 1 亿的每周用户和超过 10 亿美元的收入；它还从根本上改变了几乎每个大公司和政府关于人工智能的讨论。&lt;/p&gt;
&lt;p&gt;但对我来说，最引人注目的是我上面提到的可能性，即 ChatGPT 成为一个新的重要消费者科技公司的基础，这是最有价值和最难建立的公司类型。我在今年早些时候在&lt;a href=&#34;https://stratechery.com/2023/the-accidental-consumer-tech-company-chatgpt-meta-and-product-market-fit-aggregation-and-apis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;《偶然的消费者科技公司》&lt;/a&gt;中写道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在涉及有意义的消费者科技公司时，实际上产品是最重要的。消费者产品的关键在于高效的客户获取，这意味着口碑和/或网络效应；ChatGPT 实际上没有后者（是的，它会收到反馈），但它拥有大量的前者。事实上，ChatGPT 的出现最让我想到的产品是谷歌：它简直比市场上任何其他产品都要好，这意味着它来自一对大学生并不重要（起源故事有些相似！）。此外，就像谷歌一样，与扎克伯格对硬件的痴迷相反，ChatGPT 非常出色，人们会想方设法使用它。甚至没有一个应用程序！然而，仅仅四个月过去，已经有一个平台了。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;我所指的平台是&lt;a href=&#34;https://stratechery.com/2023/chatgpt-learns-computing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGPT 插件&lt;/a&gt;；这是一个令人着迷的概念，其用户界面不太完善，而在八个月后的&lt;a href=&#34;https://stratechery.com/2023/the-openai-keynote/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI 首个开发者日&lt;/a&gt;上，公司宣布了 GPTs，这是他们试图成为一个平台的第二次尝试。与此同时，据报道，Altman 正在探索 OpenAI 监管范围之外的新公司，以构建芯片和硬件，显然没有向董事会报备。这些因素的某种组合，或者可能尚未报道的其他因素，是董事会的最后一根稻草，由首席科学家 Ilya Sutskever 领导，他们在周末罢免了 Altman。&lt;a href=&#34;https://www.theatlantic.com/technology/archive/2023/11/sam-altman-open-ai-chatgpt-chaos/676050/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Atlantic 报道&lt;/a&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Altman 在上周五被 OpenAI 董事会解雇，这是公司两派意识形态的权力斗争的顶点，一派来自硅谷的技术乐观主义，充满了快速商业化的活力；另一方则深受人工智能代表对人类构成存在威胁的担忧，认为必须极度谨慎地控制。多年来，这两派成功地共存，虽然也出现了一些波折。&lt;/p&gt;
&lt;p&gt;根据现任和前员工的说法，几乎一年前的今天，正是 ChatGPT 的发布导致了 OpenAI 陷入全球关注的关键时刻。从外部看，ChatGPT 看起来像有史以来最成功的产品推出之一。它比历史上任何其他消费者应用程序都增长得更快，似乎单凭一己之力重新定义了数百万人对自动化的威胁和机遇的理解。但它使 OpenAI 朝着截然相反的方向发展，加剧了已经存在的意识形态分歧。ChatGPT 为盈利而创造产品的竞争加速了，同时给公司的基础设施和专注于评估和减轻技术风险的员工带来了前所未有的压力。这加剧了 OpenAI 派系之间已经紧张的关系，Altman 在 2019 年的员工电子邮件中称之为“族群”。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Altman 的“族群”——将 OpenAI 变得更像传统科技公司的一方——对于科技界的人们，包括我自己，肯定更加熟悉。我甚至在我的文章中关于开发者日演讲的段落中谈到了 OpenAI 的过渡，但不幸的是，我编辑掉了。以下是我写的内容：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;大约在这个时候，我再次开始哀叹&lt;a href=&#34;https://stratechery.com/2022/dall-e-open-to-all-openai-and-openness-openai-opportunities-and-threats/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI 奇怪的公司结构&lt;/a&gt;。作为长期关注硅谷的观察者，看到 OpenAI 追随传统的创业公司道路是令人愉快的：公司显然处于迅速扩张阶段，产品经理突然被认为是有用的，因为他们处于找到并交付低悬果实的甜蜜点，而这对于一个尚未拥有时间或壕沟来容忍王国建设和功能蔓延的实体来说是有益的。&lt;/p&gt;
&lt;p&gt;让我犹豫不决的是，目标不是上市，然后退休到游艇上，并向那些在消除极度富有的内疚感方面做得更好的事业提供资金。赚钱并回应股东的要求会控制更多救世主的冲动；当我听说 Altman 不拥有 OpenAI 的任何股权时，这使我更加紧张而不是宽慰。或者也许是因为我不会有 S-1 或 10-K 要分析。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&#34;微软与董事会的对立&#34;&gt;微软与董事会的对立&lt;/h3&gt;
&lt;p&gt;在整个周末，科技界 Twitter 上的讨论大部分集中在董事会为何会烧掉如此多的价值上感到震惊。首先，Altman 是硅谷最有关联的高管之一，是一位多产的筹款人和交易谈判者；其次，一些 OpenAI 员工已经辞职，预计未来几天会有更多人辞职。OpenAI 以前可能有两个派系；可以合理地假设未来只会有一个，由新任 CEO Shear 领导，他将人工智能末日的概率定为&lt;a href=&#34;https://twitter.com/rowancheung/status/1726473420299534491&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;5% 至 50% 之间&lt;/a&gt;，并提倡&lt;a href=&#34;https://twitter.com/eshear/status/1703178063306203397&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;大幅减缓发展&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;然而，事实上是这样的：无论您是否同意 Sutskever/Shear 派的观点，董事会的宪章和责任不是为了赚钱。这不是一家受益人公司，其对股东具有信托责任；事实上，正如我上面所述，OpenAI 的宪章明确规定了它是“无需生成财务回报”。从这个角度来看，董事会实际上正在履行其职责，尽管这似乎有些违反直觉：在董事会认为 Altman 及其派别并没有“构建造福于人类的通用人工智能”的情况下，他们有权解雇他；他们这么做了。&lt;/p&gt;
&lt;p&gt;这涉及到了我对该公司非营利性地位的担忧的讽刺之处：我曾担心 Altman 没有受到赚钱的需要的限制，或者担心由某人负责，而他没有对结果有财务利益，而事实上正是这些因素让他失去了工作。更广泛地说，我的批评不够全面，因为在商业分析的案例中，对无约束权力的哲学关切相形见绌——至少在 OpenAI 成为与之进行交易的根本不稳定的实体方面如此。当然，这涉及到了微软，作为一直支持 Satya Nadella 领导的人，我不得不承认我对公司与 OpenAI 合作的分析存在不足之处。&lt;/p&gt;
&lt;p&gt;微软已经押注了其未来的大部分，这超越了金钱，微软拥有大量的金钱，其中很多尚未支付（或以 Azure 积分形式授予）；OpenAI 的技术已经内建到了微软的许多产品中，从 Windows 到 Office，甚至有些大多数人从未听说过的产品（我看到了你，Dynamics CRM 迷！）。微软还在为 OpenAI 量身定制的基础设施上进行大规模投资，纳德拉一直在宣传专业化的财务优势，而且刚刚发布了一款专门用于运行 OpenAI 模型的定制芯片。现在看来，微软将大量的承诺交给了一个非追求利润的实体，因此不受微软作为投资者和收入驱动因素的约束，这似乎是荒谬的。&lt;/p&gt;
&lt;p&gt;或者说，直到纳德拉在太平洋时间晚上 11:53 发推文如下：&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-satya-nadella-发文宣布-sam-altman-加入微软&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Satya Nadella 发文宣布 Sam Altman 加入微软&#34; srcset=&#34;
               /blog/openais-misalignment-and-microsofts-gain/openai-1_hu6400280948200541092.webp 400w,
               /blog/openais-misalignment-and-microsofts-gain/openai-1_hu6791968672014753677.webp 760w,
               /blog/openais-misalignment-and-microsofts-gain/openai-1_hu17602148903376625195.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/openais-misalignment-and-microsofts-gain/openai-1_hu6400280948200541092.webp&#34;
               width=&#34;640&#34;
               height=&#34;358&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Satya Nadella 发文宣布 Sam Altman 加入微软
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;对我刚才提出的关于微软错误决定与非营利组织合作的论点的反驳是关于人工智能开发的现实，特别是对大量计算资源的需求。正是对这种计算资源的需求导致 OpenAI，其自身禁止进行传统的风险投资交易，向微软交出了知识产权。换句话说，尽管董事会可能拥有非营利公司的宪章，以及一种令人钦佩的愿意行动并坚守信念，但他们最终没有筹码，因为他们不是一家拥有资本以真正独立的公司。最终的结果是，一个以安全开发人工智能为宗旨的实体基本上已经将其所有工作，以及可能很快的时间内的一大部分人才，移交给了地球上最大的盈利实体之一。或者用与人工智能相关的框架来说，OpenAI 的结构最终与实现其所述使命不符。试图通过命令组织激励措施根本无法考虑在动态情况下可能发挥作用的所有可能情况和变量；出于良好的原因，收获自身利益一直是对齐个人和公司的最佳方式。&lt;/p&gt;
&lt;h3 id=&#34;关于-altman-的疑问&#34;&gt;关于 Altman 的疑问&lt;/h3&gt;
&lt;p&gt;董事会的行动还有一个角度值得承认：这很可能是有正当理由的。我支持&lt;a href=&#34;https://www.newcomer.co/p/give-openais-board-some-time-the&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eric Newcomer 在他的同名 Substack 上发表的深思熟虑的专栏文章&lt;/a&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在其&lt;a href=&#34;https://openai.com/blog/openai-announces-leadership-transition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;声明&lt;/a&gt;中，董事会表示他们得出了结论，认为 Altman“在与董事会的沟通中没有始终坦诚”。我们不应该因为糟糕的公开信息传递让我们忽视一个事实，那就是 Altman 失去了董事会的信任，而董事会本应该证明 OpenAI 的诚信&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;据我了解，一些董事会成员真诚地认为 Altman 在与他们的沟通中不诚实，不可靠，消息来源告诉我。董事会的一些成员认为他们无法监督公司，因为他们无法相信 Altman 所说的话。然而，非营利董事会的存在是 OpenAI 的&lt;a href=&#34;https://x.com/martin_casado/status/1723112508234539270?s=20&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;信誉&lt;/a&gt;的一个关键理由。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Newcomer 指出了我上面提到的董事会的宪章，Anthropic 的创始人觉得有必要首先离开 OpenAI，Musk 对 Altman 的敌意，以及 Altman 在离开 YCombinator 时仍然&lt;a href=&#34;https://www.newcomer.co/p/odds-and-ends?nthPub=1251&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;有些模糊和未解释清楚的离职&lt;/a&gt;。Newcomer 总结道：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我相信写这封警示信不会让我在硅谷的许多角落受欢迎。但我认为我们应该放慢脚步，获取更多的事实。如果 OpenAI 将我们引向人工通用智能或接近的领域，我们将希望花更多的时间来思考我们想要由谁引领我们去那里&amp;hellip;&amp;hellip;&lt;/p&gt;
&lt;p&gt;Altman 拥有很大的权力，以及非营利组织的外衣，以及超过他更混杂的私人声誉的公开形象。他失去了董事会的信任。我们应该认真对待这一点。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;也许我因为上面提到的对微软分析的疏忽而感到有些谦卑，更不用说对周末深夜的命运逆转感到震惊了，但我要指出，&lt;a href=&#34;https://stratechery.com/2023/attenuating-innovation-ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;我已经明确了立场&lt;/a&gt;，反对 AI 末日论者和呼吁监管的呼声；为此，我对确认本周末事件背后驱动力的假设性叙述感到警惕。并且，我要指出，我仍然担心那些试图在没有自己切身利益的情况下控制令人难以置信的能力的高管的哲学问题。&lt;/p&gt;
&lt;p&gt;为此，像 Altman 这样的初创生态系统固定成员加入微软当然令人惊讶：微软是唯一保留对 OpenAI 知识产权的地方，可以将其与有效无限的资金和 GPU 访问相结合，这无疑增加了权力控制人工智能是 Altman 主要动机的叙述的可信度。&lt;/p&gt;
&lt;h3 id=&#34;改变后的格局&#34;&gt;改变后的格局&lt;/h3&gt;
&lt;p&gt;显然，Altman 和 Microsoft 掌握了人工智能的主导权。微软拥有知识产权，并很快将拥有足够的团队，可以将其与资金和基础设施相结合，同时摆脱了他们与 OpenAI 之前合作中固有的协调问题（当然，他们仍然是 OpenAI 的合作伙伴！）。&lt;/p&gt;
&lt;p&gt;我也辩论了一段时间，外部公司建立在 Azure 的 API 上比建立在 OpenAI 的 API 上更有意义；微软天生就是一个开发平台，而 OpenAI 虽然有趣和令人兴奋，但很可能会克隆您的功能或淘汰旧的 API。现在，选择更加明显了。从微软的角度来看，这消除了企业客户避免使用 Azure 的主要原因之一，因为他们依赖 OpenAI；微软现在拥有完整的技术栈。&lt;/p&gt;
&lt;p&gt;与此同时，谷歌可能需要进行一些重大变革；公司的最新模型 Gemini 已经延迟推出，其云业务因支出转向人工智能而放缓，这正是公司所希望的完全相反的结果。公司的创始人和股东还能容忍公司进展太慢的看法多久，特别是与微软展示的灵活性和愿意承担风险的态度相比？&lt;/p&gt;
&lt;p&gt;这留给了 Anthropic，12 小时前看起来像是大赢家，现在作为一个独立实体感觉越来越脆弱。该公司已经与谷歌和亚马逊达成了合作协议，但现在面临着一个竞争对手微软，其拥有实际上无限的资金和 GPU 访问权限；很难摆脱这样一种感觉，即将其作为 AWS 的一部分是有道理的（是的，B 公司可以被收购，比非营利组织更容易）。&lt;/p&gt;
&lt;p&gt;然而，最终，可以提出这样的论点，即实际上并没有发生太大变化：很长一段时间以来，已经明显&lt;a href=&#34;https://stratechery.com/2023/ai-and-the-big-five/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI 至少在短期和中期内&lt;/a&gt;是一种持续的创新，而不是一种颠覆性的创新，也就是说，它主要会使最大的公司受益并部署。成本如此之高，以至于其他人很难获得资金，甚至在考虑渠道和客户获取问题之前。如果有一家公司有望加入 Big Five 的行列，那就是 OpenAI，多亏了 ChatGPT，但现在似乎不太可能了（但不是不可能）。最终，这是纳德拉的洞察力：如果您很大，赢得胜利的关键不是像初创公司那样发明，而是利用自己的规模来收购或快速跟随它们；如果您可以以 0 美元的低价格做到这一点，那就更好了。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kubernetes 故障排除智慧的演变</title>
      <link>https://cloudnativecn.com/blog/can-chatgpt-save-collective-kubernetes-troubleshooting/</link>
      <pubDate>Sun, 10 Sep 2023 19:03:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/can-chatgpt-save-collective-kubernetes-troubleshooting/</guid>
      <description>&lt;p&gt;摘要：本文讨论了在 Kubernetes 故障排除中的两种路径：一种是增强操作员的分析工作，通过自动化和简化对故障排除知识的访问来提供帮助；另一种是将操作员从故障排除中排除，通过使用 AI/ML 模型和可观察性数据来自动化故障修复。同时强调了数据的重要性，以及继续共享故障排除经验和建立对可观察性的一致认识的必要性。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本文译自：https://thenewstack.io/can-chatgpt-save-collective-kubernetes-troubleshooting/&lt;/p&gt;
&lt;p&gt;数十年前，系统管理员们开始在互联网上分享他们每天面临的技术问题。他们进行了长时间、充满活力且富有价值的讨论，探讨如何调查和解决问题的根本原因，然后详细说明最终对他们有效的解决方案。&lt;/p&gt;
&lt;p&gt;这股洪流从未停歇，只是改变了流向。如今，这些讨论仍在 Stack Overflow、Reddit 以及企业工程博客上进行。每一次讨论都是对全球 IT 系统故障排除经验的宝贵贡献。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://roadmap.sh/kubernetes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kubernetes&lt;/a&gt;也从根本上改变了这种流向。与几十年来困扰系统管理员和 IT 人员的虚拟机（VM）和单体应用程序相比，&lt;a href=&#34;https://thenewstack.io/microservices/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;微服务架构&lt;/a&gt;要复杂得多。由于 Kubernetes 缺乏数据持久性，往往无法对规模化的 K8s 错误进行本地重现。即使能够捕获，观测数据也会在多个平台上分散，而资源和依赖关系的相互关联关系也难以捕捉。&lt;/p&gt;
&lt;p&gt;现在，凭直觉并不一定足够。您需要知道如何调试集群以获得下一步的线索。&lt;/p&gt;
&lt;p&gt;这种复杂性意味着公开的故障排除讨论比以往任何时候都更为重要，但现在我们开始看到这股宝贵的洪流不是被重定向，而是完全被堵住了。你在谷歌上看到了这一点。任何与 Kubernetes 相关问题的搜索都会出现一半以上的付费广告和至少一页 SEO 驱动的文章，这些文章缺乏技术深度。&lt;a href=&#34;https://thenewstack.io/stack-overflow-adds-ai-will-the-community-respond/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stack Overflow&lt;/a&gt; 正在失去其作为技术人员首选问答资源的主导地位，Reddit 在过去几年中也陷入了争议。&lt;/p&gt;
&lt;p&gt;现在，每个 Kubernetes 的 DevOps 平台都在建立最后一个堤坝：将您的故障排除知识集中在其平台上，并用&lt;a href=&#34;https://thenewstack.io/70-percent-of-developers-using-or-will-use-ai-says-stack-overflow-survey/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;人工智能（AI）和机器学习（ML）&lt;/a&gt;取而代之，直到整个堆栈对于甚至是最有经验的云原生工程师来说都成为一个黑盒。当发生这种情况时，您失去了逐个探测、排除故障和修复系统的能力。这种趋势将曾经是众包故障排除技能洪流变成了过去所能提供的仅仅是一滴水。&lt;/p&gt;
&lt;p&gt;当我们依赖于平台时，故障排除技术的集体智慧就会消失。&lt;/p&gt;
&lt;h2 id=&#34;故障排除智慧的传承&#34;&gt;故障排除智慧的传承&lt;/h2&gt;
&lt;p&gt;起初，系统管理员依靠实体书籍进行技术文档和整体最佳实践的实施。随着互联网在 80 年代和 90 年代的普及，这些人通常通过&lt;a href=&#34;https://today.duke.edu/2010/05/usenet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Usenet&lt;/a&gt;与同行进行交流，并在像 comp.lang.* 这样的新闻组中提出工作中的技术问题，这类新闻组类似于我们今天所知的论坛的简化版本。&lt;/p&gt;
&lt;p&gt;随着互联网的普及迅速，并几乎完全改变了故障排除智慧的洪流。工程师和管理员们不再聚集在新闻组中，而是涌向包括 Experts Exchange 在内的数千个论坛，该论坛于 1996 年上线。在积累了大量的问题和答案之后，Experts Exchange 团队将所有答案都放在了每年 250 美元的付费墙后面，这使得无数宝贵的讨论无法公开获取，最终导致了该网站的影响力下降。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.joelonsoftware.com/2018/04/06/the-stack-overflow-age/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stack Overflow 随后出现&lt;/a&gt;，再次向公众开放了这些讨论，并通过声望点数对讨论进行游戏化，这些声望点数可以通过提供见解和解决方案来获得。其他用户随后对“最佳”解决方案进行投票和验证，这有助于其他搜索者快速找到答案。Stack Overflow 的游戏化、自我管理和社区使其成为了洪流式故障排除知识的唯一渠道。&lt;/p&gt;
&lt;p&gt;但是，就像其他时代一样，没有什么好事能永远持续下去。近 10 年来，人们一直在预测&lt;a href=&#34;https://johnslegers.medium.com/the-decline-of-stack-overflow-7cb69faa575d&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Stack Overflow 的衰落”&lt;/a&gt;，并指出由于其具有攻击性的性质和由拥有最多声望点数的人进行管理的结构，它“讨厌新用户”。虽然 Stack Overflow 的影响力和流行度确实下降了，但 Reddit 的开发/工程专注的 subreddit 填补了这个空白，它仍然是公开可访问的故障排除知识的最大存储库。&lt;/p&gt;
&lt;p&gt;特别是对于 Kubernetes 和云原生社区来说，这仍然是一个重要的资源，因为它们仍然在经历重大的增长阵痛。而这是一种宝贵的资源，因为如果您认为现在的 Kubernetes 已经很复杂了&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-的复杂性问题&#34;&gt;Kubernetes 的复杂性问题&lt;/h2&gt;
&lt;p&gt;在一篇关于“直观调试”失败的精彩文章中，软件交付顾问 Pete Hodgson 认为，构建和交付软件的现代架构（如 Kubernetes 和微服务）比以往任何时候都更加复杂。他写道：“对于我们大多数人来说，为服务器命名为希腊神话角色，并通过 ssh 进入服务器运行&lt;code&gt;tail&lt;/code&gt;和&lt;code&gt;top&lt;/code&gt;的日子已经一去不复返了。”但是，“这种转变是有代价的……传统的理解和故障排除生产环境的方法在这个新世界中已经行不通了。”&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-cynefin-模型&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Cynefin 模型&#34; srcset=&#34;
               /blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hu7678611328878471593.webp 400w,
               /blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hu931055034156845125.webp 760w,
               /blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hu10537070282739028136.webp 1200w&#34;
               src=&#34;https://cloudnativecn.com/blog/can-chatgpt-save-collective-kubernetes-troubleshooting/cynfin_hu7678611328878471593.webp&#34;
               width=&#34;760&#34;
               height=&#34;676&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      Cynefin 模型
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Cynefin 模型。来源：维基百科&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Hodgson 使用&lt;a href=&#34;https://en.wikipedia.org/wiki/Cynefin_framework&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cynefin 模型&lt;/a&gt;来说明软件架构过去是复杂的，因为有足够的经验，人们可以理解故障排除和解决方案之间的因果关系。&lt;/p&gt;
&lt;p&gt;他认为，分布式微服务架构是复杂的，即使经验丰富的人对根本原因以及如何进行故障排除也只有“有限的直觉”。他们必须花更多时间通过可观察性数据提出问题和回答问题，最终假设可能出错的原因。&lt;/p&gt;
&lt;p&gt;如果我们同意 Hodgson 的前提 - Kubernetes 本质上是复杂的，并且在响应之前需要花费更多的时间分析问题，那么与 Kubernetes 一起工作的工程师学会了哪些问题最重要，然后用可观察性数据回答，以进行最佳的下一步行动，似乎是至关重要的。&lt;/p&gt;
&lt;p&gt;这正是新一代以 AI 驱动的故障排除平台所提供的智慧。&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-故障排除的两种路径&#34;&gt;Kubernetes 故障排除的两种路径&lt;/h2&gt;
&lt;p&gt;多年来，像 OpenAI 这样的公司一直在根据 Stack Overflow、Reddit 等公开数据进行抓取和训练模型，这意味着这些 AI 模型可以访问大量的系统和应用知识，包括 Kubernetes。还有一些人意识到组织的可观察性数据是训练 AI/ML 模型分析新场景的宝贵资源。&lt;/p&gt;
&lt;p&gt;他们都在问同一个问题：我们如何利用关于 Kubernetes 的现有数据来简化搜索最佳解决方案的过程？他们正在构建的产品采取非常不同的路径。&lt;/p&gt;
&lt;h3 id=&#34;第一种增强操作员的分析工作&#34;&gt;第一种：增强操作员的分析工作&lt;/h3&gt;
&lt;p&gt;这些工具自动化和简化对公开在线发布的大量故障排除知识的访问。它们不会取代进行适当故障排除或&lt;a href=&#34;https://aws.amazon.com/opensearch-service/resources/root-cause-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;根本原因分析&lt;/a&gt;（RCA）所需的人类直觉和创造力，而是有条不紊地自动化操作员查找相关信息的方式。&lt;/p&gt;
&lt;p&gt;例如，如果一个刚接触 Kubernetes 的开发人员在运行&lt;code&gt;kubectl get pods&lt;/code&gt;时发现&lt;code&gt;CrashLoopBackOff&lt;/code&gt;状态导致他们无法部署应用程序，他们可以查询一个 AI 驱动的工具以获得建议，比如运行&lt;code&gt;kubectl describe $POD&lt;/code&gt;或&lt;code&gt;kubectl logs $POD&lt;/code&gt;。这些步骤可能会进一步引导开发人员使用&lt;code&gt;kubectl describe $DEPLOYMENT&lt;/code&gt;来调查相关的部署情况。&lt;/p&gt;
&lt;p&gt;在&lt;a href=&#34;https://botkube.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Botkube&lt;/a&gt;，我们对使用 AI 在大量故障排除智慧的基础上自动化这个来回查询的概念非常感兴趣。用户应该能够直接在 Slack 中提问，如“我如何排除这个无法正常工作的服务？”并收到 ChatGPT 撰写的回答。在一次公司范围的黑客马拉松活动中，我们着手实施这一概念，为我们的协作故障排除平台构建了一个新的插件。&lt;/p&gt;
&lt;p&gt;通过&lt;a href=&#34;https://botkube.io/blog/use-chatgpt-to-troubleshoot-kubernetes-errors-with-botkubes-doctor&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Doctor&lt;/a&gt;，您可以利用大量的故障排除知识，通过 Botkube 作为您的 Kubernetes 集群和消息/协作平台之间的桥梁，无需在 Stack Overflow 或 Google 搜索广告中漫游，这对于新手 Kubernetes 开发人员和操作员特别有用。&lt;/p&gt;
&lt;p&gt;该插件还通过生成一个带有&lt;strong&gt;获取帮助&lt;/strong&gt;按钮的 Slack 消息进一步自动化，用于任何错误或异常，然后查询 ChatGPT 以获取可行的解决方案和下一步操作。您甚至可以将 Doctor 插件的结果导入其他操作或集成，以简化您主动使用现有广泛的 Kubernetes 故障排除知识来更直观地调试和感知问题的方式。&lt;/p&gt;
&lt;h3 id=&#34;第二种将操作员从故障排除中排除&#34;&gt;第二种：将操作员从故障排除中排除&lt;/h3&gt;
&lt;p&gt;这些工具不关心公开知识的泛滥。如果它们可以基于实际的可观察性数据训练通用的 AI/ML 模型，然后根据您的特定架构进行微调，它们可以试图完全剔除人为操作员在根本原因分析和故障修复中的作用。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.causely.io/platform/causely-for-kubernetes-applications/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Causely&lt;/a&gt;就是这样一家初创公司，他们并不回避使用 AI 来“消除人为故障排除”的愿景。该平台连接到您现有的可观察性数据，并处理它们以微调因果关系模型，理论上可直接进行修复步骤 - 无需探测或使用&lt;code&gt;kubectl&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;如果说有时候有一个 Kubernetes 神灵听起来很诱人，那我可能会撒谎，但我对像 Causely 这样的工具夺走运维工作并不担心。我担心的是在 Causely 引领的未来中，我们宝贵的故障排除知识会发生什么。&lt;/p&gt;
&lt;h3 id=&#34;这两种路径之间的差距数据&#34;&gt;这两种路径之间的差距：数据&lt;/h3&gt;
&lt;p&gt;我不是在为“人工智能将取代所有 DevOps 工作”发表言论。我们已经读过太多这样的末日场景，适用于每个小众和行业。我更关心这两种路径之间的差距：用于训练和回答问题或呈现结果的数据是什么？&lt;/p&gt;
&lt;p&gt;第一种路径通常使用现有的公开数据。尽管有关 AI 公司爬取这些站点进行训练数据的担忧-Reddit 和 Twitter，但这些数据的开放性仍然提供了一个激励循环，以保持开发人员和工程师继续在 Reddit、Stack Overflow 和其他平台上共享知识的持续泛滥。&lt;/p&gt;
&lt;p&gt;云原生社区通常也倾向于共享技术知识，认同共享技术知识和一个“涨潮（Kubernetes 故障排除技巧的涨潮）抬高所有船（压力巨大的 Kubernetes 工程师）”的想法。&lt;/p&gt;
&lt;p&gt;第二条路径看起来更为暗淡。随着以 AI 驱动的 DevOps 平台的兴起，越来越多的故障排除知识被锁定在这些仪表板和驱动平台的专有 AI 模型中。我们都同意，Kubernetes 基础架构将继续变得更加复杂，而不是更简单，这意味着随着时间的推移，我们对节点、Pod 和容器之间发生的情况的理解将变得更少。&lt;/p&gt;
&lt;p&gt;当我们停止互相分析问题和感知解决方案时，我们变得依赖于平台。这对每个人来说都是一条失败的道路，除了平台之外。&lt;/p&gt;
&lt;h3 id=&#34;我们如何不失去或失去得更少&#34;&gt;我们如何不失去（或失去得更少）？&lt;/h3&gt;
&lt;p&gt;我们能做的最好的事情是继续在线上发布关于我们在 Kubernetes 和其他领域的故障排除经验的惊人内容，比如“&lt;a href=&#34;https://learnk8s.io/troubleshooting-deployments&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;关于故障排除 Kubernetes 部署的视觉指南&lt;/a&gt;”；通过游戏化创造教育性应用程序，比如&lt;a href=&#34;https://sadservers.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SadServers&lt;/a&gt;；在故障排除系统时采取我们最喜欢的第一步，比如“&lt;a href=&#34;https://rachelbythebay.com/w/2018/03/26/w/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;为什么在排除未知机器问题时我通常首先运行‘w’&lt;/a&gt;”；并进行详细的事后分析，详细描述了探测、感知和应对潜在灾难性情况的压力故事，比如&lt;a href=&#34;https://mail.tarsnap.com/tarsnap-announce/msg00050.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023 年 7 月的 Tarsnap 故障&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;我们还可以超越技术解决方案，比如讨论我们如何在紧张的故障排除场景中管理和支持同事，或者在组织范围内建立对可观察性的一致认识。&lt;/p&gt;
&lt;p&gt;尽管它们目前面临困境，但 Stack Overflow 和 Reddit 将继续是讨论故障排除和寻求答案的可靠渠道。如果它们最终与 Usenet 和 Experts Exchange 齐名，它们可能会被其他可公开获得的替代品所取代。&lt;/p&gt;
&lt;p&gt;无论何时何地以何种方式发生，我希望您能加入我们在 Botkube 和全新的 Doctor 插件中，为在 Kubernetes 中协作解决复杂问题构建新的渠道。&lt;/p&gt;
&lt;p&gt;无论 AI 驱动的 DevOps 平台是否继续基于抓取的公共 Kubernetes 数据训练新模型，只要我们不自愿地将好奇心、冒险精神和解决问题的能力全部放入这些黑匣子中，就会始终有一条新路径，让宝贵的故障排除知识源源不断地流动。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>将 AI 应用于 WebAssembly 还为时过早吗？</title>
      <link>https://cloudnativecn.com/blog/is-it-too-early-to-leverage-ai-for-webassembly/</link>
      <pubDate>Thu, 07 Sep 2023 21:03:00 +0800</pubDate>
      <guid>https://cloudnativecn.com/blog/is-it-too-early-to-leverage-ai-for-webassembly/</guid>
      <description>&lt;p&gt;本文译自：&lt;a href=&#34;https://thenewstack.io/is-it-too-early-to-leverage-ai-for-webassembly/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://thenewstack.io/is-it-too-early-to-leverage-ai-for-webassembly/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;摘要：Fermyon Technologies 认为，将 AI 应用于 WebAssembly 并不为时过早。WebAssembly 为在服务器上运行推理提供了坚实的基础，而且在许多不同的环境中，如浏览器和物联网设备等，通过将这些工作负载移动到终端用户设备上，可以消除延迟并避免将数据发送到集中式服务器，同时能够在边缘发现的多种异构设备上运行。Fermyon Serverless AI 通过提供超过 100 倍于其他按需 AI 基础设施服务的亚秒冷启动时间来解决了企业级 AI 应用程序成本高的问题。这是一种共生关系。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;人工智能及其在 IT、软件开发和运营方面的应用刚开始发挥作用，预示着人类角色将如何在近期和长期内演变，特别是在较小的规模上，WebAssembly 代表着一种正在引起重大关注的技术，同时证明了其可行性，但成功的商业模型尚未实现，主要是由于最终端点的缺乏标准化。与此同时，至少有一家供应商 Fermyon 认为，在这个阶段应用 AI 于 WebAssembly 并不为时过早。&lt;/p&gt;
&lt;p&gt;那么，AI 如何潜在地帮助 Wasm 的开发和采用，这是否为时过早？正如 VMware CTO 办公室的高级工程师 Angel M De Miguel Meana 所指出的那样，自从 ChatGPT 推出以来，AI 生态系统已经发生了巨大的变化，WebAssembly 为在服务器上运行推理提供了坚实的基础，而且在许多不同的环境中，如浏览器和物联网设备等，通过将这些工作负载移动到终端用户设备上，可以消除延迟并避免将数据发送到集中式服务器，同时能够在边缘发现的多种异构设备上运行。由于 Wasm 生态系统仍在兴起，因此在早期阶段集成 AI 将有助于推动新的和现有的与 AI 相关的标准。这是一种共生关系。&lt;/p&gt;
&lt;h2 id=&#34;完美的匹配&#34;&gt;完美的匹配&lt;/h2&gt;
&lt;p&gt;Fermyon Technologies 的联合创始人兼首席执行官 Matt Butcher 告诉 The New Stack：“我们成立 Fermyon 的目标是打造下一代无服务器平台。AI 显然是这一下一代的一部分。在我们的行业中，我们经常看到革命性的技术一起成长：Java 和 Web、云和微服务、Docker 和 Kubernetes。WebAssembly 和 AI 是一对完美的组合。我看到它们一起成长（并变老）。”&lt;/p&gt;
&lt;p&gt;“烘焙”AI 模型，如 LLM（大型语言模型）或转换器，到 WebAssembly 运行时中，是加速采用 WebAssembly 的逻辑下一步，Enterprise Management Associates (EMA) 的分析师 Torsten Volk 告诉 The New Stack。与调用诸如通过 API 的数据库服务类似，编译 WebAssembly 应用程序（二进制文件）可以将其 API 请求发送到 WebAssembly 运行时，该运行时将该调用中继到 AI 模型并将模型响应返回给发起者，Volk 说。&lt;/p&gt;
&lt;p&gt;“一旦我们有一个提供开发人员一个标准 API 的通用组件模型（CCM），访问数据库、AI 模型、GPU、消息传递、身份验证等，这些 API 请求将变得非常强大。CCM 将让开发人员编写相同的代码，在数据中心、云甚至边缘位置的任何类型的服务器上与 AI 模型（例如 GPT 或 Llama）进行通信，只要该服务器拥有足够的硬件资源可用，”Volk 说。“这一切都归结为关键问题，即产业参与者何时会就 CCM 达成一致。同时，WebAssembly 云（如 Fermyon）可以利用 WebAssembly 使 AI 模型在其自己的云基础设施中具有可移植性和可扩展性，无需 CCM，并将一些节省成本传递给客户。”&lt;/p&gt;
&lt;h2 id=&#34;解决问题&#34;&gt;解决问题&lt;/h2&gt;
&lt;p&gt;同时，Fermyon 认为，在这个阶段应用 AI 于 WebAssembly 并不为时过早。正如 Butcher 所指出的那样，负责在 LLM（如 LLaMA2）上构建和运行企业 AI 应用程序的开发人员面临着 100 倍计算成本的挑战，即每小时 32 美元及以上的 GPU 访问费用。或者，他们可以使用按需服务，但是启动时间却非常慢。这使得以实惠的方式提供企业级 AI 应用程序变得不切实际。&lt;/p&gt;
&lt;p&gt;Fermyon Serverless AI 通过提供超过 100 倍于其他按需 AI 基础设施服务的亚秒冷启动时间来解决了这个问题，Butcher 说。这一“突破”得益于驱动 Fermyon Cloud 的服务器 WebAssembly 技术，该技术被架构为亚毫秒冷启动和高容量时间分片的计算实例，已被证明可以将计算密度提高 30 倍。“将此运行时配置文件扩展到 GPU 将使 Fermyon Cloud 成为最快的 AI 推理基础设施服务，”Butcher 说。&lt;/p&gt;
&lt;p&gt;Volk 说，这样的推理服务“非常有趣”，因为典型的 WebAssembly 应用程序仅包含几兆字节，而 AI 模型的大小要大得多。这意味着它们不会像传统的 WebAssembly 应用程序那样启动得那么快。“我认为 Fermyon 已经想出了如何使用时间分片为 WebAssembly 应用程序提供 GPU 访问的方法，以便所有这些应用程序都可以通过其 WebAssembly 运行时保留一些时间片来获取所需的 GPU 资源”，Volk 说。“这意味着很多应用程序可以共享一小部分昂贵的 GPU，以按需为其用户提供服务。这有点像分时共享，但不需要强制参加午餐时间的演示。”&lt;/p&gt;
&lt;p&gt;使用 Spin 入门。&lt;/p&gt;
&lt;p&gt;!https://prod-files-secure.s3.us-west-2.amazonaws.com/86575c70-5cc9-4b3e-bee7-d1bb14ba20e3/6bf78916-e34c-4051-86a7-52145cdc372a/4a27b287-capture-decran-2023-09-05-192118.png&lt;/p&gt;
&lt;p&gt;那么，用户如何与 Serverless AI 交互？Fermyon 的 Serverless AI 没有 REST API 或外部服务，它仅构建在 Fermyon 的 Spin 本地和 Fermyon Cloud 中，Butcher 解释说。“在您的代码的任何位置，您都可以将提示传递到 Serverless AI 并获得响应。在这个第一个测试版中，我们包括 LLaMa2 的聊天模型和最近宣布的 Code Llama 代码生成模型，”Butcher 说。“因此，无论您是在总结文本、实现自己的聊天机器人还是编写后端代码生成器，Serverless AI 都可以满足您的需求。我们的目标是使 AI 变得简单，使开发人员可以立即开始利用它来构建新的令人瞩目的无服务器应用程序。”&lt;/p&gt;
&lt;h2 id=&#34;重要意义&#34;&gt;重要意义&lt;/h2&gt;
&lt;p&gt;使用 WebAssembly 来运行工作负载，可以使用 Fermyon Serverless AI 将“GPU 的一小部分”分配给用户应用程序，以“及时”执行 AI 操作，Fermyon CTO 和联合创始人 Radu Matei 在一篇博客文章中写道。 “当操作完成时，我们将该 GPU 的一小部分分配给队列中的另一个应用程序，”Matei 写道。“由于 Fermyon Cloud 中的启动时间为毫秒级，因此我们可以在分配给 GPU 的用户应用程序之间快速切换。如果所有 GPU 分数都在忙于计算数据，我们将在下一个可用的应用程序之前将传入的应用程序排队。”&lt;/p&gt;
&lt;p&gt;这有两个重大的影响，Matei 写道。首先，用户不必等待虚拟机或容器启动并附加到 GPU 上。此外，“我们可以实现更高的资源利用率和效率，”Matei 写道。&lt;/p&gt;
&lt;p&gt;Fermyon 传达的 Serverless AI 的具体特点包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;这是一款开发人员工具和托管服务，专为使用开源 LLM 进行 AI 推理的无服务器应用程序而设计。&lt;/li&gt;
&lt;li&gt;由于我们的核心 WebAssembly 技术，我们的冷启动时间比竞争对手快 100 倍，从几分钟缩短到不到一秒。这使我们能够在相同的时间内（并且使用相同的硬件）执行数百个应用程序（二进制文件），而今天的服务用于运行一个。&lt;/li&gt;
&lt;li&gt;我们为使用 Spin 构建和运行 AI 应用程序提供了本地开发体验，然后将其部署到 Fermyon Cloud 中，以高性能的方式以其他解决方案的一小部分成本提供服务。&lt;/li&gt;
&lt;li&gt;Fermyon Cloud 使用 AI 级别的 GPU 处理每个请求。由于我们的快速启动和高效的时间共享，我们可以在数百个应用程序之间共享单个 GPU。&lt;/li&gt;
&lt;li&gt;我们正在推出免费的私人测试版。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;大希望&#34;&gt;大希望&lt;/h2&gt;
&lt;p&gt;然而，在 Wasm 和 AI 同时达到潜力之前，还有很长的路要走。在 WasmCon 2023 上，Second State 的 CEO 兼联合创始人 Michael Yuan 和 Wasm 的运行时项目以及 WasmEdge 的讨论了一些正在进行的工作。他在与 De Miguel Meana 的谈话中涵盖了这个话题，“开始使用 AI 和 WebAssembly”在 WasmCon 2023 上。&lt;/p&gt;
&lt;p&gt;“在这个领域（AI 和 Wasm）需要做很多生态系统工作。例如，仅拥有推理是不够的，”Yuan 说。“现在的百万美元问题是，当您拥有图像和文本时，如何将其转换为一系列数字，然后在推理之后如何将这些数字转换回可用的格式？”&lt;/p&gt;
&lt;p&gt;预处理和后处理是 Python 今天最大的优势之一，这得益于为这些任务提供的众多库，Yuan 说。将这些预处理和后处理函数合并到 Rust 函数中将是有益的，但需要社区更多的努力来支持其他模块。“这个生态系统有很大的增长潜力，”Yuan 说。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
